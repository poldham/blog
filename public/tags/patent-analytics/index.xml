<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Patent Analytics on Paul Oldham&#39;s Analytics Blog</title>
    <link>https://www.pauloldham.net/tags/patent-analytics/</link>
    <description>Recent content in Patent Analytics on Paul Oldham&#39;s Analytics Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright @ Paul Oldham 2017-2018</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://www.pauloldham.net/tags/patent-analytics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Creating an Infographic with infogram</title>
      <link>https://www.pauloldham.net/infographics/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.pauloldham.net/infographics/</guid>
      <description>&lt;p&gt;In this article we will use RStudio to prepare patent data for visualisation in an infographic using the online software tool &lt;a href=&#34;https://infogram.com/?rc=paid0sem0branded0search0&amp;amp;gclid=EAIaIQobChMIw6KgvMiq2AIViLvtCh2fpgxhEAAYASAAEgKR2PD_BwE&#34;&gt;infogram&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Infographics are a popular way of presenting data in a way that is easy for a reader to understand without reading a long report. Infographics are well suited to presenting summaries of data with simple messages about key findings. A good infographic can encourage the audience to read a detailed report and is a tool for engagement with audiences during presentations of the findings of patent research.&lt;/p&gt;
&lt;p&gt;Some patent offices have already been creating infographics as part of their reports to policy makers and other clients. The Instituto Nacional de Propiedade Industrial (INPI) in Brazil produces regular two page &lt;a href=&#34;http://www.inpi.gov.br/menu-servicos/informacao/radares-tecnologicos&#34;&gt;Technology Radar&lt;/a&gt; (Radar Tecnologico) consisting of charts and maps that briefly summarise more detailed research on subjects such as &lt;a href=&#34;http://www.inpi.gov.br/menu-servicos/arquivos-cedin/n08_radar_tecnologico_nano_residuos_versao_resumida_ingles_atualizada_20160122.pdf&#34;&gt;Nanotechnology in Waste Management&lt;/a&gt;. &lt;a href=&#34;http://www.wipo.int/patentscope/en/programs/patent_landscapes/&#34;&gt;WIPO Patent Landscape Reports&lt;/a&gt;, which go into depth on patent activity for a particular area, are accompanied by one page infographics that have proved very popular such as the infographic accompanying a recent report on &lt;a href=&#34;http://www.wipo.int/export/sites/www/patentscope/en/programs/patent_landscapes/reports/documents/assistivedevices_infographic.pdf&#34;&gt;assistive devices&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A growing number of companies are offering online infographic software services such as &lt;a href=&#34;https://infogr.am/app/#/library&#34;&gt;infogr.am&lt;/a&gt;,&lt;a href=&#34;http://www.easel.ly&#34;&gt;easel.ly&lt;/a&gt; &lt;a href=&#34;https://magic.piktochart.com/templates&#34;&gt;piktochart.com&lt;/a&gt;, &lt;a href=&#34;https://www.canva.com/create/infographics/&#34;&gt;canva.com&lt;/a&gt; or &lt;a href=&#34;https://venngage.com&#34;&gt;venngage.com&lt;/a&gt; to mention only a selection of the offerings out there. The &lt;a href=&#34;http://www.coolinfographics.com/tools/&#34;&gt;Cool Infographics website&lt;/a&gt; provides a useful overview of available tools.&lt;/p&gt;
&lt;p&gt;One feature of many of these services is that they are based on a freemium model. Creating graphics is free but the ability to export files and the available formats for export of your masterpiece (e.g. high resolution or .pdf) often depend on upgrading to a monthly account at varying prices. In this chapter we test drive &lt;a href=&#34;https://infogr.am/app/#/library&#34;&gt;infogr.am&lt;/a&gt; as a chart friendly service, albeit with export options that depend on a paid account.&lt;/p&gt;
&lt;p&gt;This article is divided into two sections.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In part 1 we focus on using RStudio to prepare patent data for visualisation in infographics software using the &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;tidyr&lt;/code&gt; and &lt;code&gt;stringr&lt;/code&gt; packages. This involves dealing with common problems with patent data such as concatenated fields, white space and creating counts of data fields. Part 1 is intended for those starting out using R and assumes no prior knowledge of R.&lt;/li&gt;
&lt;li&gt;In part 2 we produce an infographic from the data using &lt;a href=&#34;https://infogr.am/app/#/library&#34;&gt;infogr.am&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This article is now a chapter in the &lt;a href=&#34;https://wipo-analytics.github.io/&#34;&gt;WIPO Manual on Open Source Patent Analytics&lt;/a&gt;. You can read the chapter in electronic book format &lt;a href=&#34;https://wipo-analytics.github.io/patent-infographics-with-r.html&#34;&gt;here&lt;/a&gt; and find all the materials including presentations at the &lt;a href=&#34;https://github.com/wipo-analytics&#34;&gt;WIPO Analytics Github homepage&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;getting-started&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;To start with we need to ensure that RStudio and R for your operating system are installed by following the instructions on the RStudio website &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/&#34;&gt;here&lt;/a&gt;. Do not forget to follow the link to also &lt;a href=&#34;https://cran.rstudio.com&#34;&gt;install R for your operating system&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When working in RStudio it is good practice to work with projects. This will keep all of the files for a project in the same folder. To create a project go to File, New Project and create a project. Call the project something like infographic. Any file you create and save for the project will now be listed under the Files tab in RStudio.&lt;/p&gt;
&lt;p&gt;R works using packages (libraries) and there are around 7,490 of them for a whole range of purposes. We will use just a few of them. To install a package we use the following. Copy and paste the code into the Console and press enter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;tidyverse&amp;quot;)  # the group of packages you will need&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Packages can also be installed by selecting the Packages tab and typing the name of the package.&lt;/p&gt;
&lt;p&gt;To load the packages (libraries) use the following or check the tick box in the Packages pane.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now ready to go.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;load-a-.csv-file-using-readr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Load a .csv file using &lt;code&gt;readr&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;We will work with the &lt;code&gt;pizza_medium_clean&lt;/code&gt; dataset in the online &lt;a href=&#34;https://github.com/wipo-analytics/opensource-patent-analytics/tree/master/2_datasets&#34;&gt;Github Manual repository&lt;/a&gt;. If manually downloading a file remember to click on the file name and select &lt;code&gt;Raw&lt;/code&gt; to download the actual file.&lt;/p&gt;
&lt;p&gt;We can use the easy to use &lt;code&gt;read_csv()&lt;/code&gt; function from the &lt;code&gt;readr&lt;/code&gt; package to quickly read in our pizza data directly from the Github repository. Note the &lt;code&gt;raw&lt;/code&gt; at the beginning of the filename.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
pizza &amp;lt;- read_csv(&amp;quot;https://github.com/wipo-analytics/opensource-patent-analytics/blob/master/2_datasets/pizza_medium_clean/pizza.csv?raw=true&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;readr&lt;/code&gt; will display a warning for the file arising from its efforts to parse publication dates on import. We will ignore this as we will not be using this field.&lt;/p&gt;
&lt;p&gt;As an alternative to importing directly from Github download the file and in RStudio use &lt;code&gt;File &amp;gt; Import Dataset &amp;gt; From .csv&lt;/code&gt;. If you experience problems with direct import of a file the File &amp;gt; Import Dataset approach will give you a range of easy to use controls for figuring this out (e.g. where .csv is actually a tab separated file).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;viewing-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Viewing Data&lt;/h2&gt;
&lt;p&gt;We can view data in a variety of ways.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In the console:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pizza&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9,996 x 31
##    applicants_cleaned    applicants_clean… applicants_orga… applicants_original
##    &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;              
##  1 &amp;lt;NA&amp;gt;                  People            &amp;lt;NA&amp;gt;             &amp;lt;NA&amp;gt;               
##  2 Ventimeglia Jamie Jo… People            &amp;lt;NA&amp;gt;             Ventimeglia Jamie …
##  3 Cordova Robert; Mart… People            &amp;lt;NA&amp;gt;             Cordova Robert;Mar…
##  4 Lazarillo De Tormes … Corporate         Lazarillo De To… LAZARILLO DE TORME…
##  5 &amp;lt;NA&amp;gt;                  People            &amp;lt;NA&amp;gt;             &amp;lt;NA&amp;gt;               
##  6 Depoortere, Thomas    People            &amp;lt;NA&amp;gt;             DEPOORTERE, Thomas 
##  7 Frisco Findus Ag      Corporate         Frisco Findus Ag FRISCO-FINDUS AG   
##  8 Bicycle Tools Incorp… Corporate         Bicycle Tools I… Bicycle Tools Inco…
##  9 Castiglioni, Carlo    People            &amp;lt;NA&amp;gt;             CASTIGLIONI, CARLO 
## 10 &amp;lt;NA&amp;gt;                  People            &amp;lt;NA&amp;gt;             &amp;lt;NA&amp;gt;               
## # ... with 9,986 more rows, and 27 more variables: inventors_cleaned &amp;lt;chr&amp;gt;,
## #   inventors_original &amp;lt;chr&amp;gt;, ipc_class &amp;lt;chr&amp;gt;, ipc_codes &amp;lt;chr&amp;gt;,
## #   ipc_names &amp;lt;chr&amp;gt;, ipc_original &amp;lt;chr&amp;gt;, ipc_subclass_codes &amp;lt;chr&amp;gt;,
## #   ipc_subclass_detail &amp;lt;chr&amp;gt;, ipc_subclass_names &amp;lt;chr&amp;gt;,
## #   priority_country_code &amp;lt;chr&amp;gt;, priority_country_code_names &amp;lt;chr&amp;gt;,
## #   priority_data_original &amp;lt;chr&amp;gt;, priority_date &amp;lt;chr&amp;gt;,
## #   publication_country_code &amp;lt;chr&amp;gt;, publication_country_name &amp;lt;chr&amp;gt;,
## #   publication_date &amp;lt;chr&amp;gt;, publication_date_original &amp;lt;chr&amp;gt;,
## #   publication_day &amp;lt;int&amp;gt;, publication_month &amp;lt;int&amp;gt;, publication_number &amp;lt;chr&amp;gt;,
## #   publication_number_espacenet_links &amp;lt;chr&amp;gt;, publication_year &amp;lt;int&amp;gt;,
## #   title_cleaned &amp;lt;chr&amp;gt;, title_nlp_cleaned &amp;lt;chr&amp;gt;,
## #   title_nlp_multiword_phrases &amp;lt;chr&amp;gt;, title_nlp_raw &amp;lt;chr&amp;gt;,
## #   title_original &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;In Environment click on the blue arrow to see in the environment. Keep clicking to open a new window with the data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;View()&lt;/code&gt; command (for data.frames and tables)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;View(pizza)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If possible use the View() command or environment. The difficulty with the console is that large amounts of data will simply stream past.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;identifying-types-of-object&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Identifying Types of Object&lt;/h2&gt;
&lt;p&gt;We often want to know what type of object we are working with and more details about the object so we know what to do later. Here are some of the most common commands for obtaining information about objects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(pizza)  ## type of object
names(pizza)  ## names of variables
str(pizza)  ## structure of object
dim(pizza)  ## dimensions of the object&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most useful command in this list is &lt;code&gt;str()&lt;/code&gt; because this allows us to access the structure of the object and see its type.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(pizza, max.level = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;str()&lt;/code&gt; is particularly useful because we can see the names of the fields (vectors) and their type. Most patent data is a character vector with dates forming integers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;working-with-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Working with Data&lt;/h2&gt;
&lt;p&gt;We will often want to select aspects of our data to focus on a specific set of columns or to create a graph. We might also want to add information, notably numeric counts.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;dplyr&lt;/code&gt; package provides a set of very handy functions for selecting, adding and counting data. The &lt;code&gt;tidyr&lt;/code&gt; and &lt;code&gt;stringr&lt;/code&gt; packages are sister packages that contain a range of other useful functions for working with our data. We have covered some of these in other chapters on graphing using R but will go through them quickly and then pull them together into a function that we can use across our dataset.&lt;/p&gt;
&lt;div id=&#34;select&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Select&lt;/h3&gt;
&lt;p&gt;In this case we will start by using the &lt;code&gt;select()&lt;/code&gt; function to limit the data to specific columns. We can do this using their names or their numeric position (best for large number of columns e.g. 1:31). In &lt;code&gt;dplyr&lt;/code&gt;, unlike most R packages, existing character columns do not require &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
pizza_number &amp;lt;- select(pizza, publication_number, publication_year)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a new data.frame that contains two columns. One with the year and one with the publication number. Note that we have created a new object called pizza_number using &lt;code&gt;&amp;lt;-&lt;/code&gt; and that after &lt;code&gt;select()&lt;/code&gt; we have named our original data and the columns we want. A fundamental feature of select is that it will drop columns that we do not name. So it is best to create a new object using &lt;code&gt;&amp;lt;-&lt;/code&gt; if you want to keep your original data for later work.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-data-with-mutate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding data with &lt;code&gt;mutate()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;mutate()&lt;/code&gt; is a &lt;code&gt;dplyr&lt;/code&gt; function that allows us to add data based on existing data in our data frame, for example to perform a calculation. In the case of patent data we normally lack a numeric field to use for counts. We can however assign a value to our publication field by using sum() and the number 1 as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
pizza_number &amp;lt;- mutate(pizza_number, n = sum(publication_number = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we view &lt;code&gt;pizza_number&lt;/code&gt; we now have a value of 1 in the column &lt;code&gt;n&lt;/code&gt; for each publication number.&lt;/p&gt;
&lt;p&gt;Note that in patent data a priority, application, publication or family number may occur multiple times and we would want to reduce the dataset to distinct records. For that we would use &lt;code&gt;n_distinct(pizza_number$publication_number)&lt;/code&gt; from &lt;code&gt;dplyr&lt;/code&gt; or &lt;code&gt;unique(pizza_number$publication_number)&lt;/code&gt; from base R. Because the publication numbers are unique we can proceed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;counting-data-using-count&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Counting data using &lt;code&gt;count()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;At the moment, we have multiple instances of the same year (where a patent publication occurs in that year). We now want to calculate how many of our documents were published in each year. To do that we will use the &lt;code&gt;dplyr&lt;/code&gt; function &lt;code&gt;count()&lt;/code&gt;. We will use the publication_year and add &lt;code&gt;wt =&lt;/code&gt; (for weight) with &lt;code&gt;n&lt;/code&gt; as the value to count.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
pizza_total &amp;lt;- count(pizza_number, publication_year, wt = n)
pizza_total&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 58 x 2
##    publication_year    nn
##               &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
##  1             1940    1.
##  2             1954    1.
##  3             1956    1.
##  4             1957    1.
##  5             1959    1.
##  6             1962    1.
##  7             1964    2.
##  8             1966    1.
##  9             1967    1.
## 10             1968    8.
## # ... with 48 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we now examine pizza_total, we will see the publication year and a summed value for the records in that year.&lt;/p&gt;
&lt;p&gt;This raises the question of how we know that R has calculated the count correctly. We already know that there are 9996 records in the pizza dataset. To check our count is correct we can simply use sum and select the column we want to sum using &lt;code&gt;$&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
sum(pizza_total$nn)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 9996&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, all is good and we can move on. The &lt;code&gt;$&lt;/code&gt; sign is one of the main ways of subsetting to tell R that we want to work with a specific column (the others are “[” and “[[”).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rename-a-field-with-rename&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Rename a field with &lt;code&gt;rename()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Next we will use &lt;code&gt;rename()&lt;/code&gt; from &lt;code&gt;dplyr&lt;/code&gt; to rename the fields. Note that understanding which field require quote marks can take some effort. In this case renaming the character vector publication_year as “pubyear” requires quotes while renaming the numeric vector “n” does not.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
pizza_total &amp;lt;- rename(pizza_total, pubyear = publication_year, publications = nn) %&amp;gt;% 
    print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 58 x 2
##    pubyear publications
##      &amp;lt;int&amp;gt;        &amp;lt;dbl&amp;gt;
##  1    1940           1.
##  2    1954           1.
##  3    1956           1.
##  4    1957           1.
##  5    1959           1.
##  6    1962           1.
##  7    1964           2.
##  8    1966           1.
##  9    1967           1.
## 10    1968           8.
## # ... with 48 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;make-a-quickplot-with-qplot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Make a quickplot with &lt;code&gt;qplot()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Using the &lt;code&gt;qplot()&lt;/code&gt; function in &lt;code&gt;ggplot2&lt;/code&gt; we can now draw a quick line graph. Note that qplot() is unusual in R because the data (pizza_total) appears after the coordinates. We will specify that we want a line using &lt;code&gt;geom =&lt;/code&gt; (if geom is left out it will be a scatter plot). This will give us an idea of what our plot might look like in our infographic and actions we might want to take on the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
qplot(x = pubyear, y = publications, data = pizza_total, geom = &amp;quot;line&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/post/2016-04-20-infographics_files/figure-html/qplot-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![](images_foot/infogram/fig1_infographic.png)---&gt;
&lt;p&gt;The plot reveals a data cliff in recent years. This normally reflects a lack of data for the last 2-3 years as recent documents feed through the system en route to publication.&lt;/p&gt;
&lt;p&gt;It is a good idea to remove the data cliff by cutting the data 2-3 years prior to the present. In some cases two years is sufficient, but 3 years is a good rule of thumb.&lt;/p&gt;
&lt;p&gt;We also have long tail of data with limited data from 1940 until the late 1970s. Depending on our purposes with the analysis we might want to keep this data (for historical analysis) or to focus in on a more recent period.&lt;/p&gt;
&lt;p&gt;We will limit our data to specific values using the &lt;code&gt;dplyr&lt;/code&gt; function &lt;code&gt;filter()&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;filter-data-using-filter&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Filter data using &lt;code&gt;filter()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;In contrast with &lt;code&gt;select()&lt;/code&gt; which works with columns, &lt;code&gt;filter()&lt;/code&gt; in &lt;code&gt;dplyr&lt;/code&gt; works with rows. In this case we need to filter on the values in the pubyear column. To remove the data prior to 1990 we will use the greater than or equal to operator &lt;code&gt;&amp;gt;=&lt;/code&gt; on the pubyear column and we will use the less than or equal to &lt;code&gt;&amp;lt;=&lt;/code&gt; operator on the values after 2012.&lt;/p&gt;
&lt;p&gt;One strength of &lt;code&gt;filter()&lt;/code&gt; in &lt;code&gt;dplyr&lt;/code&gt; is that it is easy to filter on multiple values in the same expression (unlike the very similar filter function in base R). The use of &lt;code&gt;filter()&lt;/code&gt; will also remove the 30 records where the year is recorded as NA (Not Available). We will write this file to disk using the simple &lt;code&gt;write_csv()&lt;/code&gt; from &lt;code&gt;readr&lt;/code&gt;. To use &lt;code&gt;write_csv()&lt;/code&gt; we first name our data (&lt;code&gt;pizza_total&lt;/code&gt;) and then provide a file name with a .csv extension. In this case and other examples below we have used a descriptive file name bearing in mind that Windows systems have limitations on the length and type of characters that can be used in file names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(readr)
pizza_total &amp;lt;- filter(pizza_total, pubyear &amp;gt;= 1990, pubyear &amp;lt;= 2012)
write_csv(pizza_total, &amp;quot;pizza_total_1990_2012.csv&amp;quot;)
pizza_total&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 23 x 2
##    pubyear publications
##      &amp;lt;int&amp;gt;        &amp;lt;dbl&amp;gt;
##  1    1990         139.
##  2    1991         154.
##  3    1992         212.
##  4    1993         201.
##  5    1994         162.
##  6    1995         173.
##  7    1996         180.
##  8    1997         186.
##  9    1998         212.
## 10    1999         290.
## # ... with 13 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we print pizza_total to the console we will see that the data now covers the period 1990-2012. When using &lt;code&gt;filter()&lt;/code&gt; on values in this way it is important to remember to apply this filter to any subsequent operations on the data (such as applicants) so that it matches the same data period.&lt;/p&gt;
&lt;p&gt;To see our .csv file we can head over to the Files tab and, assuming that we have created a project, the file will now appear in the list of project files. Clicking on the file name will display the raw unformatted data in RStudio.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;simplify-code-with-pipes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simplify code with pipes &lt;code&gt;%&amp;gt;%&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;So far we have handled the code one line at a time. But, one of the great strengths of using a programming language is that we can run multiple lines of code together. There are two basic ways that we can do this.&lt;/p&gt;
&lt;p&gt;We will create a new temporary object &lt;code&gt;df&lt;/code&gt; to demonstrate this.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The standard way&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)
df &amp;lt;- select(pizza, publication_number, publication_year)
df &amp;lt;- mutate(df, n = sum(publication_number = 1))
df &amp;lt;- count(df, publication_year, wt = n)
df &amp;lt;- rename(df, pubyear = publication_year, publications = nn)
df &amp;lt;- filter(df, pubyear &amp;gt;= 1990, pubyear &amp;lt;= 2012)
qplot(x = pubyear, y = publications, data = df, geom = &amp;quot;line&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code we have just created is six lines long. If we select all of this code and run it in one go it will produce our graph.&lt;/p&gt;
&lt;p&gt;One feature of this code is that each time we run a function on the object total we name it at the start of each function (e.g. mutate(df…)) and then we overwrite the object.&lt;/p&gt;
&lt;p&gt;We can save quite a lot of typing and reduce the complexity of the code using the pipe operator introduced by the the &lt;code&gt;magrittr&lt;/code&gt; package and then adopted in Hadley Wickham’s data wrangling and tidying packages.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Using pipes &lt;code&gt;%&amp;gt;%&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Pipes are now a very popular way of writing R code because they simplify writing R code and speed it up. The most popular pipe is &lt;code&gt;%&amp;gt;%&lt;/code&gt; which means “this” then “that”. In this case we are going to create a new temporary object &lt;code&gt;df1&lt;/code&gt; by first applying select to pizza, then mutate, count, rename and filter. Note that we only name our dataset once (in &lt;code&gt;select()&lt;/code&gt;) and we do not need to keep overwriting the object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)
df1 &amp;lt;- select(pizza, publication_number, publication_year) %&amp;gt;% mutate(n = sum(publication_number = 1)) %&amp;gt;% 
    count(publication_year, wt = n) %&amp;gt;% rename(pubyear = publication_year, publications = nn) %&amp;gt;% 
    filter(pubyear &amp;gt;= 1990, pubyear &amp;lt;= 2012) %&amp;gt;% qplot(x = pubyear, y = publications, 
    data = ., geom = &amp;quot;line&amp;quot;) %&amp;gt;% print()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/post/2016-04-20-infographics_files/figure-html/piped-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![](images_foot/infogram/fig2_infographic_qplot.png)---&gt;
&lt;p&gt;In the standard code we typed &lt;code&gt;df&lt;/code&gt; nine times to arrive at the same result. Using pipes we typed df1 once. Of greater importance is that the use of pipes simplifies the structure of R code by introducing a basic “this” then “that” logic which makes it easier to understand.&lt;/p&gt;
&lt;p&gt;One point to note about this code is that &lt;code&gt;qplot()&lt;/code&gt; requires us to name our data (in this case &lt;code&gt;df1&lt;/code&gt;). However, &lt;code&gt;df1&lt;/code&gt; is actually the final output of the code and does not exist as an input object before the final line is run. So, if we attempt to use &lt;code&gt;data = df1&lt;/code&gt; in &lt;code&gt;qplot()&lt;/code&gt; we will receive an error message. The way around this is to use &lt;code&gt;.&lt;/code&gt; in place of our data object. That way &lt;code&gt;qplot()&lt;/code&gt; will know we want to graph the outputs of the earlier code. Finally, we need to add an explicit call to &lt;code&gt;print()&lt;/code&gt; to display the graph (without this the code will work but we will not see the graph).&lt;/p&gt;
&lt;p&gt;If we now inspect the structure of the df1 object (using &lt;code&gt;str(df1)&lt;/code&gt;) in the console, it will be a list. The reason for this is that it is an object with mixed components, including a data.frame with our data plus additional data setting out the contents of the plot. As there is no direct link between R and our infographics software this will create problems for us later because the infographics software won’t know how to interpret the list object. So, it is generally a good idea to use a straight data.frame by excluding the call to &lt;code&gt;qplot&lt;/code&gt; and adding it later when needed as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)
df2 &amp;lt;- select(pizza, publication_number, publication_year) %&amp;gt;% mutate(n = sum(publication_number = 1)) %&amp;gt;% 
    count(publication_year, wt = n) %&amp;gt;% rename(pubyear = publication_year, publications = nn) %&amp;gt;% 
    filter(pubyear &amp;gt;= 1990, pubyear &amp;lt;= 2012) %&amp;gt;% print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 23 x 2
##    pubyear publications
##      &amp;lt;int&amp;gt;        &amp;lt;dbl&amp;gt;
##  1    1990         139.
##  2    1991         154.
##  3    1992         212.
##  4    1993         201.
##  5    1994         162.
##  6    1995         173.
##  7    1996         180.
##  8    1997         186.
##  9    1998         212.
## 10    1999         290.
## # ... with 13 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that in this case the only change is that we need to explicitly include the reference to the df2 data frame as the data argument in the call to &lt;code&gt;qplot()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
qplot(x = pubyear, y = publications, data = df2, geom = &amp;quot;line&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/post/2016-04-20-infographics_files/figure-html/df2_qplot-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;harmonising-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Harmonising data&lt;/h2&gt;
&lt;p&gt;One challenge with creating multiple tables from a baseline dataset is keeping track of subdatasets. At the moment we have two basic objects we will be working with:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;pizza&lt;/code&gt; - our raw dataset&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pizza_total&lt;/code&gt; - created via &lt;code&gt;pizza_number&lt;/code&gt; limited to 1990_2012.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the remainder of the chapter we will want to create some additional datasets from our pizza dataset. These are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Country trends&lt;/li&gt;
&lt;li&gt;Applicants&lt;/li&gt;
&lt;li&gt;International Patent Classification (IPC) Class&lt;/li&gt;
&lt;li&gt;Phrases&lt;/li&gt;
&lt;li&gt;Google&lt;/li&gt;
&lt;li&gt;Google IPC&lt;/li&gt;
&lt;li&gt;Google phrases&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We need to make sure that any data that we generate from our raw dataset matches the period for the &lt;code&gt;pizza_total&lt;/code&gt; dataset. If we do not do this there is a risk that we will generate subdatasets with counts for the raw pizza dataset.&lt;/p&gt;
&lt;p&gt;To handle this we will use &lt;code&gt;filter()&lt;/code&gt; to create a new baseline dataset with an unambiguous name.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
pizza_1990_2012 &amp;lt;- rename(pizza, pubyear = publication_year) %&amp;gt;% filter(pubyear &amp;gt;= 
    1990, pubyear &amp;lt;= 2012)
pizza_1990_2012&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8,262 x 31
##    applicants_cleaned  applicants_clean… applicants_organ… applicants_original 
##    &amp;lt;chr&amp;gt;               &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;               
##  1 &amp;lt;NA&amp;gt;                People            &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;                
##  2 Lazarillo De Torme… Corporate         Lazarillo De Tor… LAZARILLO DE TORMES…
##  3 &amp;lt;NA&amp;gt;                People            &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;                
##  4 Depoortere, Thomas  People            &amp;lt;NA&amp;gt;              DEPOORTERE, Thomas  
##  5 Frisco Findus Ag    Corporate         Frisco Findus Ag  FRISCO-FINDUS AG    
##  6 Bicycle Tools Inco… Corporate         Bicycle Tools In… Bicycle Tools Incor…
##  7 Castiglioni, Carlo  People            &amp;lt;NA&amp;gt;              CASTIGLIONI, CARLO  
##  8 &amp;lt;NA&amp;gt;                People            &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;                
##  9 Bujalski, Wlodzimi… People            &amp;lt;NA&amp;gt;              BUJALSKI, WLODZIMIE…
## 10 Ehrno Flexible A/S… Corporate; People Ehrno Flexible A… &amp;quot;EHRNO FLEXIBLE A/S…
## # ... with 8,252 more rows, and 27 more variables: inventors_cleaned &amp;lt;chr&amp;gt;,
## #   inventors_original &amp;lt;chr&amp;gt;, ipc_class &amp;lt;chr&amp;gt;, ipc_codes &amp;lt;chr&amp;gt;,
## #   ipc_names &amp;lt;chr&amp;gt;, ipc_original &amp;lt;chr&amp;gt;, ipc_subclass_codes &amp;lt;chr&amp;gt;,
## #   ipc_subclass_detail &amp;lt;chr&amp;gt;, ipc_subclass_names &amp;lt;chr&amp;gt;,
## #   priority_country_code &amp;lt;chr&amp;gt;, priority_country_code_names &amp;lt;chr&amp;gt;,
## #   priority_data_original &amp;lt;chr&amp;gt;, priority_date &amp;lt;chr&amp;gt;,
## #   publication_country_code &amp;lt;chr&amp;gt;, publication_country_name &amp;lt;chr&amp;gt;,
## #   publication_date &amp;lt;chr&amp;gt;, publication_date_original &amp;lt;chr&amp;gt;,
## #   publication_day &amp;lt;int&amp;gt;, publication_month &amp;lt;int&amp;gt;, publication_number &amp;lt;chr&amp;gt;,
## #   publication_number_espacenet_links &amp;lt;chr&amp;gt;, pubyear &amp;lt;int&amp;gt;,
## #   title_cleaned &amp;lt;chr&amp;gt;, title_nlp_cleaned &amp;lt;chr&amp;gt;,
## #   title_nlp_multiword_phrases &amp;lt;chr&amp;gt;, title_nlp_raw &amp;lt;chr&amp;gt;,
## #   title_original &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case we start with a call to &lt;code&gt;rename()&lt;/code&gt; to make this consistent with our pizza_total table and then use a pipe to filter the data on the year. Note here that when filtering raw data on a set of values it is important to inspect it first to check that the field is clean (e.g. not concatenated). If for some reason your data is concatenated (which happens quite a lot with patent data) then lookup &lt;code&gt;?tidyr::separate_rows&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We are now in a position to create our country trends table.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;country-trends-using-spread&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Country Trends using &lt;code&gt;spread()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;There are two basic data formats: long and wide. Our pizza dataset is in long format because each column is a variable (e.g. &lt;code&gt;publication_country&lt;/code&gt;) and each row in &lt;code&gt;publication_country&lt;/code&gt; contains a country name. This is the most common and useful data format.&lt;/p&gt;
&lt;p&gt;However, in some cases, such as &lt;code&gt;infogr.am&lt;/code&gt; our visualisation software will expect the data to be in wide format. In this case each country name would become a variable (column name) with the years forming the rows and the number of records per year the observations. The key to this is the &lt;code&gt;tidyr()&lt;/code&gt; function &lt;code&gt;spread()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As above we will start off by using &lt;code&gt;select()&lt;/code&gt; to create a table with the fields that we want. We will then use &lt;code&gt;mutate()&lt;/code&gt; to add a numeric field and then count up that data. To illustrate the process run this code (we will not create an object).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
select(pizza_1990_2012, publication_country_name, publication_number, pubyear) %&amp;gt;% 
    mutate(n = sum(publication_number = 1)) %&amp;gt;% count(publication_country_name, pubyear, 
    wt = n) %&amp;gt;% print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 223 x 3
##    publication_country_name pubyear    nn
##    &amp;lt;chr&amp;gt;                      &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
##  1 Canada                      1990   19.
##  2 Canada                      1991   49.
##  3 Canada                      1992   66.
##  4 Canada                      1993   59.
##  5 Canada                      1994   50.
##  6 Canada                      1995   39.
##  7 Canada                      1996   36.
##  8 Canada                      1997   45.
##  9 Canada                      1998   46.
## 10 Canada                      1999   47.
## # ... with 213 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we run this code we will see the results in long format. We now want to take our &lt;code&gt;publication_country_name&lt;/code&gt; column and spread it to form columns with &lt;code&gt;nn&lt;/code&gt; as the values.&lt;/p&gt;
&lt;p&gt;In using spread note that it takes a data argument (&lt;code&gt;pizza_1990_2012&lt;/code&gt;), a key (&lt;code&gt;publication_country_name&lt;/code&gt;), and value column (&lt;code&gt;nn&lt;/code&gt;) (created from &lt;code&gt;count()&lt;/code&gt;). We are using pipes so the data only needs to be named in the first line. For additional arguments see &lt;code&gt;?spread()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidyr)
country_totals &amp;lt;- select(pizza_1990_2012, publication_country_name, publication_number, 
    pubyear) %&amp;gt;% mutate(n = sum(publication_number = 1)) %&amp;gt;% count(publication_country_name, 
    pubyear, wt = n) %&amp;gt;% spread(publication_country_name, nn)
country_totals&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 23 x 17
##    pubyear Canada China `Eurasian Patent… `European Paten… Germany Israel Japan
##      &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1    1990    19.   NA                NA               22.      2.    NA    NA 
##  2    1991    49.   NA                NA               29.      2.    NA    NA 
##  3    1992    66.   NA                NA               36.      2.     1.   NA 
##  4    1993    59.   NA                NA               29.      2.    NA    NA 
##  5    1994    50.   NA                NA               26.      5.    NA    NA 
##  6    1995    39.   NA                NA               29.      2.     1.   NA 
##  7    1996    36.    1.               NA               27.      1.     1.   NA 
##  8    1997    45.   NA                NA               34.      1.    NA    NA 
##  9    1998    46.   NA                NA               36.      1.    NA    17.
## 10    1999    47.    2.                2.              60.      4.    NA    26.
## # ... with 13 more rows, and 9 more variables: `Korea, Republic of` &amp;lt;dbl&amp;gt;,
## #   Mexico &amp;lt;dbl&amp;gt;, `Patent Co-operation Treaty` &amp;lt;dbl&amp;gt;, Portugal &amp;lt;dbl&amp;gt;, `Russian
## #   Federation` &amp;lt;dbl&amp;gt;, Singapore &amp;lt;dbl&amp;gt;, `South Africa` &amp;lt;dbl&amp;gt;, Spain &amp;lt;dbl&amp;gt;,
## #   `United States of America` &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have data in wide format.&lt;/p&gt;
&lt;p&gt;In some cases, such as infogr.am, visualisation software may expect the country names to be the name of rows and the column names to be years . We can modify our call to &lt;code&gt;spread()&lt;/code&gt; by replacing the &lt;code&gt;publication_country_name&lt;/code&gt; with &lt;code&gt;pubyear&lt;/code&gt;. Then we will write the data to disk for use in our infographic.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(readr)
country_totals &amp;lt;- select(pizza_1990_2012, publication_country_name, publication_number, pubyear) %&amp;gt;%
  mutate(n = sum(publication_number = 1)) %&amp;gt;% 
  count(publication_country_name, pubyear, wt = n) %&amp;gt;% # note n
  spread(pubyear, nn) # note nn
country_totals&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 16 x 24
##    publication_country… `1990` `1991` `1992` `1993` `1994` `1995` `1996` `1997`
##    &amp;lt;chr&amp;gt;                 &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1 Canada                  19.    49.    66.    59.    50.    39.    36.    45.
##  2 China                   NA     NA     NA     NA     NA     NA      1.    NA 
##  3 Eurasian Patent Org…    NA     NA     NA     NA     NA     NA     NA     NA 
##  4 European Patent Off…    22.    29.    36.    29.    26.    29.    27.    34.
##  5 Germany                  2.     2.     2.     2.     5.     2.     1.     1.
##  6 Israel                  NA     NA      1.    NA     NA      1.     1.    NA 
##  7 Japan                   NA     NA     NA     NA     NA     NA     NA     NA 
##  8 Korea, Republic of      NA     NA     NA      1.    NA     NA      1.     1.
##  9 Mexico                  NA     NA     NA     NA     NA     NA     NA     NA 
## 10 Patent Co-operation…     8.    13.    31.    16.    20.    22.    23.    26.
## 11 Portugal                NA     NA     NA     NA     NA     NA     NA     NA 
## 12 Russian Federation      NA     NA     NA     NA     NA     NA     NA      5.
## 13 Singapore               NA     NA     NA     NA     NA     NA     NA     NA 
## 14 South Africa             2.     3.     3.     3.     3.     1.     9.     7.
## 15 Spain                   NA     NA     NA     NA     NA     NA     NA     NA 
## 16 United States of Am…    86.    58.    73.    91.    58.    79.    81.    67.
## # ... with 15 more variables: `1998` &amp;lt;dbl&amp;gt;, `1999` &amp;lt;dbl&amp;gt;, `2000` &amp;lt;dbl&amp;gt;,
## #   `2001` &amp;lt;dbl&amp;gt;, `2002` &amp;lt;dbl&amp;gt;, `2003` &amp;lt;dbl&amp;gt;, `2004` &amp;lt;dbl&amp;gt;, `2005` &amp;lt;dbl&amp;gt;,
## #   `2006` &amp;lt;dbl&amp;gt;, `2007` &amp;lt;dbl&amp;gt;, `2008` &amp;lt;dbl&amp;gt;, `2009` &amp;lt;dbl&amp;gt;, `2010` &amp;lt;dbl&amp;gt;,
## #   `2011` &amp;lt;dbl&amp;gt;, `2012` &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(country_totals, &amp;quot;pizza_country_1990_2012.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To restore the data to long format we would need to use &lt;code&gt;gather()&lt;/code&gt; as the counterpart to &lt;code&gt;spread()&lt;/code&gt;. &lt;code&gt;gather()&lt;/code&gt; takes a dataset, a key for the name of the column we want to gather the countries into, a value for the numeric count (in this case n), and finally the positions of the columns to gather in. Note here that we need to look up the column positions in &lt;code&gt;country_totals&lt;/code&gt; (e.g. using &lt;code&gt;View()&lt;/code&gt;) or count the columns using &lt;code&gt;ncol(country_totals)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
gather(country_totals, year, n, 2:24) %&amp;gt;% print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 368 x 3
##    publication_country_name     year      n
##    &amp;lt;chr&amp;gt;                        &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
##  1 Canada                       1990    19.
##  2 China                        1990    NA 
##  3 Eurasian Patent Organization 1990    NA 
##  4 European Patent Office       1990    22.
##  5 Germany                      1990     2.
##  6 Israel                       1990    NA 
##  7 Japan                        1990    NA 
##  8 Korea, Republic of           1990    NA 
##  9 Mexico                       1990    NA 
## 10 Patent Co-operation Treaty   1990     8.
## # ... with 358 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The combination of spread and gather work really well to prepare data in formats that are expected by other software. However, one of the main issues we encounter with patent data is that our data is not tidy because various fields are concatenated.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidying-data---separating-and-gathering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidying data - Separating and Gathering&lt;/h2&gt;
&lt;p&gt;In patent data we often see concatenated fields with a separator (normally a &lt;code&gt;;&lt;/code&gt;). These are typically applicant names, inventor names, International Patent Classification (IPC) codes, or document numbers (priority numbers, family numbers). We need to &lt;code&gt;tidy&lt;/code&gt; this data prior to data cleaning (such as cleaning names) or to prepare for analysis and visualisation. For more on the concept of tidy data read &lt;a href=&#34;http://vita.had.co.nz/papers/tidy-data.pdf&#34;&gt;Hadley Wickham’s Tidy Data article&lt;/a&gt;. The new &lt;a href=&#34;http://r4ds.had.co.nz/tidy-data.html&#34;&gt;R for Data Science book&lt;/a&gt; by Garrett Grolemund and Hadley Wickham (see Chapter 12) is also strongly recommended.&lt;/p&gt;
&lt;p&gt;To tidy patent data we will typically need to do two things.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Separate the data so that each cell contains a unique data point (e.g. a name, code or publication number). This normally involves separating data into columns.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Gathering the data back in. This involves transforming the data in the columns we have created into rows.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Separating data into columns is very easy in tools such as Excel. However, gathering the data back into separate rows is remarkably difficult. Happily, this is very easy to do in R with the &lt;code&gt;tidyr&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;tidyr&lt;/code&gt; package contains three functions that are very useful when working with patent data. When dealing with concatenated fields in columns the key function is &lt;code&gt;separate_rows&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here we will work with the &lt;code&gt;applicants_cleaned&lt;/code&gt; field in the pizza dataset. This field contains concatenated names with a &lt;code&gt;;&lt;/code&gt; as the separator. For example, on lines 1_9 there are single applicant names or NA values. However, on lines 10 and line 59 we see:&lt;/p&gt;
&lt;p&gt;Ehrno Flexible A/S; Stergaard, Ole Farrell Brian; Mcnulty John; Vishoot Lisa&lt;/p&gt;
&lt;p&gt;The problem here is that when we are dealing with thousands of lines of applicant names we don’t know how many names might be concatenated into each cell as a basis for separating the data into columns. Once we had split the columns (for example using Text to Columns in Excel) we would then need to work out how to gather the columns into rows. The &lt;code&gt;separate_rows()&lt;/code&gt; function from &lt;code&gt;tidyr&lt;/code&gt; makes light work of this problem. To use the function we name the dataset, the column we want to separate into rows and the separator (sep).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidyr)
pizza1 &amp;lt;- separate_rows(pizza_1990_2012, applicants_cleaned, sep = &amp;quot;;&amp;quot;)
pizza1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12,729 x 31
##    applicants_cleaned applicants_cleane… applicants_organ… applicants_original 
##    &amp;lt;chr&amp;gt;              &amp;lt;chr&amp;gt;              &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;               
##  1 &amp;lt;NA&amp;gt;               People             &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;                
##  2 Lazarillo De Torm… Corporate          Lazarillo De Tor… LAZARILLO DE TORMES…
##  3 &amp;lt;NA&amp;gt;               People             &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;                
##  4 Depoortere, Thomas People             &amp;lt;NA&amp;gt;              DEPOORTERE, Thomas  
##  5 Frisco Findus Ag   Corporate          Frisco Findus Ag  FRISCO-FINDUS AG    
##  6 Bicycle Tools Inc… Corporate          Bicycle Tools In… Bicycle Tools Incor…
##  7 Castiglioni, Carlo People             &amp;lt;NA&amp;gt;              CASTIGLIONI, CARLO  
##  8 &amp;lt;NA&amp;gt;               People             &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;                
##  9 Bujalski, Wlodzim… People             &amp;lt;NA&amp;gt;              BUJALSKI, WLODZIMIE…
## 10 Ehrno Flexible A/S Corporate; People  Ehrno Flexible A… &amp;quot;EHRNO FLEXIBLE A/S…
## # ... with 12,719 more rows, and 27 more variables: inventors_cleaned &amp;lt;chr&amp;gt;,
## #   inventors_original &amp;lt;chr&amp;gt;, ipc_class &amp;lt;chr&amp;gt;, ipc_codes &amp;lt;chr&amp;gt;,
## #   ipc_names &amp;lt;chr&amp;gt;, ipc_original &amp;lt;chr&amp;gt;, ipc_subclass_codes &amp;lt;chr&amp;gt;,
## #   ipc_subclass_detail &amp;lt;chr&amp;gt;, ipc_subclass_names &amp;lt;chr&amp;gt;,
## #   priority_country_code &amp;lt;chr&amp;gt;, priority_country_code_names &amp;lt;chr&amp;gt;,
## #   priority_data_original &amp;lt;chr&amp;gt;, priority_date &amp;lt;chr&amp;gt;,
## #   publication_country_code &amp;lt;chr&amp;gt;, publication_country_name &amp;lt;chr&amp;gt;,
## #   publication_date &amp;lt;chr&amp;gt;, publication_date_original &amp;lt;chr&amp;gt;,
## #   publication_day &amp;lt;int&amp;gt;, publication_month &amp;lt;int&amp;gt;, publication_number &amp;lt;chr&amp;gt;,
## #   publication_number_espacenet_links &amp;lt;chr&amp;gt;, pubyear &amp;lt;int&amp;gt;,
## #   title_cleaned &amp;lt;chr&amp;gt;, title_nlp_cleaned &amp;lt;chr&amp;gt;,
## #   title_nlp_multiword_phrases &amp;lt;chr&amp;gt;, title_nlp_raw &amp;lt;chr&amp;gt;,
## #   title_original &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our original dataset contained 8,262 rows. Our new dataset split on applicant names contains 12,729 rows. The function has moved our target column from column 1 to column 31 in the data frame. We can easily move it back to inspect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
pizza1 &amp;lt;- select(pizza1, 31, 1:30)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;separate_rows()&lt;/code&gt; has done a great job but one of the problems with concatenated names is extra white space around the separator. We will deal with this next.&lt;/p&gt;
&lt;div id=&#34;trimming-with-stringr&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Trimming with &lt;code&gt;stringr&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;If we inspect the bottom of the column by subsetting into it using &lt;code&gt;$&lt;/code&gt; we will see that a lot of the names have a leading whitespace space. This results from the separate exercise where the &lt;code&gt;;&lt;/code&gt; is actually &lt;code&gt;;space&lt;/code&gt;. Take a look at the last few rows of the data using &lt;code&gt;tail()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tail(pizza1$applicants_cleaned, 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Yahoo! Inc&amp;quot;                        &amp;quot;Clarcor Inc&amp;quot;                      
##  [3] &amp;quot;Holden Jeffrey A&amp;quot;                  &amp;quot; Vengroff Darren E&amp;quot;               
##  [5] &amp;quot;Casper Jeffrey L&amp;quot;                  &amp;quot; Erickson Braden J&amp;quot;               
##  [7] &amp;quot; Oppenheimer Alan A&amp;quot;               &amp;quot; Ray Madonna M&amp;quot;                   
##  [9] &amp;quot; Weber Jean L&amp;quot;                     &amp;quot;Pandey Neena&amp;quot;                     
## [11] &amp;quot; Sharma Sudhanshu&amp;quot;                 &amp;quot; Verizon Patent And Licensing Inc&amp;quot;
## [13] &amp;quot;Pandey Neena&amp;quot;                      &amp;quot; Sharma Sudhanshu&amp;quot;                
## [15] &amp;quot;Brown Michael&amp;quot;                     &amp;quot; Urban Scott&amp;quot;                     
## [17] &amp;quot;Brown Michael&amp;quot;                     &amp;quot; Urban Scott&amp;quot;                     
## [19] &amp;quot;Cole Lorin R&amp;quot;                      &amp;quot; Middleton Scott W&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a big issue because any counts that we make later on using the applicants_cleaned field will treat “Oppenheimer Alan A” and &amp;quot; Oppenheimer Alan A&amp;quot; as separate names when they should be grouped together.&lt;/p&gt;
&lt;p&gt;We can address this in a couple of ways. One approach is to recognise that actually our separator is not a simple &lt;code&gt;&amp;quot;;&amp;quot;&lt;/code&gt; but &lt;code&gt;&amp;quot;;space&amp;quot;&lt;/code&gt; in our call to &lt;code&gt;separate_rows()&lt;/code&gt;. In that case the call to &lt;code&gt;separate_rows()&lt;/code&gt; would actually be &lt;code&gt;sep = &amp;quot;; &amp;quot;&lt;/code&gt;. We will add a line of code to illustrate the impact of this change.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmp &amp;lt;- separate_rows(pizza_1990_2012, applicants_cleaned, sep = &amp;quot;; &amp;quot;)
tail(tmp$applicants_cleaned, 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Yahoo! Inc&amp;quot;                       &amp;quot;Clarcor Inc&amp;quot;                     
##  [3] &amp;quot;Holden Jeffrey A&amp;quot;                 &amp;quot;Vengroff Darren E&amp;quot;               
##  [5] &amp;quot;Casper Jeffrey L&amp;quot;                 &amp;quot;Erickson Braden J&amp;quot;               
##  [7] &amp;quot;Oppenheimer Alan A&amp;quot;               &amp;quot;Ray Madonna M&amp;quot;                   
##  [9] &amp;quot;Weber Jean L&amp;quot;                     &amp;quot;Pandey Neena&amp;quot;                    
## [11] &amp;quot;Sharma Sudhanshu&amp;quot;                 &amp;quot;Verizon Patent And Licensing Inc&amp;quot;
## [13] &amp;quot;Pandey Neena&amp;quot;                     &amp;quot;Sharma Sudhanshu&amp;quot;                
## [15] &amp;quot;Brown Michael&amp;quot;                    &amp;quot;Urban Scott&amp;quot;                     
## [17] &amp;quot;Brown Michael&amp;quot;                    &amp;quot;Urban Scott&amp;quot;                     
## [19] &amp;quot;Cole Lorin R&amp;quot;                     &amp;quot;Middleton Scott W&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to address this, is to use the &lt;code&gt;str_trim()&lt;/code&gt; function from the &lt;code&gt;stringr&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;We can address this problem using a function from the &lt;code&gt;stringr&lt;/code&gt; package &lt;code&gt;str_trim()&lt;/code&gt;. We have a choice with &lt;code&gt;str_trim()&lt;/code&gt; on whether to trim the whitespace on the right, left or both. Here we will choose both.&lt;/p&gt;
&lt;p&gt;Because we are seeking to modify an existing column (not to create a new vector or data.frame) we will use &lt;code&gt;$&lt;/code&gt; to select the column and as the data for the &lt;code&gt;str_trim()&lt;/code&gt; function. That will apply the function to the applicants column in pizza1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stringr)
pizza1$applicants_cleaned &amp;lt;- str_trim(pizza1$applicants_cleaned, side = &amp;quot;both&amp;quot;)
tail(pizza1$applicants_cleaned, 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Yahoo! Inc&amp;quot;                       &amp;quot;Clarcor Inc&amp;quot;                     
##  [3] &amp;quot;Holden Jeffrey A&amp;quot;                 &amp;quot;Vengroff Darren E&amp;quot;               
##  [5] &amp;quot;Casper Jeffrey L&amp;quot;                 &amp;quot;Erickson Braden J&amp;quot;               
##  [7] &amp;quot;Oppenheimer Alan A&amp;quot;               &amp;quot;Ray Madonna M&amp;quot;                   
##  [9] &amp;quot;Weber Jean L&amp;quot;                     &amp;quot;Pandey Neena&amp;quot;                    
## [11] &amp;quot;Sharma Sudhanshu&amp;quot;                 &amp;quot;Verizon Patent And Licensing Inc&amp;quot;
## [13] &amp;quot;Pandey Neena&amp;quot;                     &amp;quot;Sharma Sudhanshu&amp;quot;                
## [15] &amp;quot;Brown Michael&amp;quot;                    &amp;quot;Urban Scott&amp;quot;                     
## [17] &amp;quot;Brown Michael&amp;quot;                    &amp;quot;Urban Scott&amp;quot;                     
## [19] &amp;quot;Cole Lorin R&amp;quot;                     &amp;quot;Middleton Scott W&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that when using &lt;code&gt;str_trim()&lt;/code&gt; we use subsetting to modify the applicants column in place. There is possibly a more efficient way of doing this with pipes but this appears difficult because the data.frame needs to exist for &lt;code&gt;str_trim()&lt;/code&gt; to act on in place or we end up with a vector of applicant names rather than a data.frame. A solution to this problem is provided on Stack Overflow&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In practice, the most efficient solution in this case is to recognise that the separator for &lt;code&gt;separate_rows&lt;/code&gt; is &lt;code&gt;&amp;quot;;space&amp;quot;&lt;/code&gt;. However, that will not always be true making the tools in &lt;code&gt;stringr&lt;/code&gt; invaluable. To learn more about string manipulation in R try &lt;a href=&#34;http://r4ds.had.co.nz/strings.html&#34;&gt;Chapter 14 of R for Data Science by Garrett Grolemund and Hadley Wickham&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can tie the steps so far together using pipes into the following simpler code that we will become the applicants table for use in the infographic. We will add a call to rename and rename applicants_cleaned to tidy up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidyr)
library(stringr)
applicants &amp;lt;- rename(pizza, pubyear = publication_year) %&amp;gt;% filter(pubyear &amp;gt;= 1990, 
    pubyear &amp;lt;= 2012) %&amp;gt;% separate_rows(applicants_cleaned, sep = &amp;quot;; &amp;quot;) %&amp;gt;% rename(applicants = applicants_cleaned) %&amp;gt;% 
    select(31, 1:30)  # moves separated column to the beginning
applicants&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12,729 x 31
##    title_original applicants applicants_clea… applicants_orga… applicants_orig…
##    &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;           
##  1 PIZZA          &amp;lt;NA&amp;gt;       People           &amp;lt;NA&amp;gt;             &amp;lt;NA&amp;gt;            
##  2 IMPROVED PIZZA Lazarillo… Corporate        Lazarillo De To… LAZARILLO DE TO…
##  3 Pizza separat… &amp;lt;NA&amp;gt;       People           &amp;lt;NA&amp;gt;             &amp;lt;NA&amp;gt;            
##  4 Pizza separat… Depoorter… People           &amp;lt;NA&amp;gt;             DEPOORTERE, Tho…
##  5 PIZZA PREPARA… Frisco Fi… Corporate        Frisco Findus Ag FRISCO-FINDUS AG
##  6 Pizza Cutter   Bicycle T… Corporate        Bicycle Tools I… Bicycle Tools I…
##  7 PIZZA BOX      Castiglio… People           &amp;lt;NA&amp;gt;             CASTIGLIONI, CA…
##  8 PIZZA BOX      &amp;lt;NA&amp;gt;       People           &amp;lt;NA&amp;gt;             &amp;lt;NA&amp;gt;            
##  9 MORE ATTRACTI… Bujalski,… People           &amp;lt;NA&amp;gt;             BUJALSKI, WLODZ…
## 10 PIZZA PACKAGI… Ehrno Fle… Corporate; Peop… Ehrno Flexible … &amp;quot;EHRNO FLEXIBLE…
## # ... with 12,719 more rows, and 26 more variables: inventors_cleaned &amp;lt;chr&amp;gt;,
## #   inventors_original &amp;lt;chr&amp;gt;, ipc_class &amp;lt;chr&amp;gt;, ipc_codes &amp;lt;chr&amp;gt;,
## #   ipc_names &amp;lt;chr&amp;gt;, ipc_original &amp;lt;chr&amp;gt;, ipc_subclass_codes &amp;lt;chr&amp;gt;,
## #   ipc_subclass_detail &amp;lt;chr&amp;gt;, ipc_subclass_names &amp;lt;chr&amp;gt;,
## #   priority_country_code &amp;lt;chr&amp;gt;, priority_country_code_names &amp;lt;chr&amp;gt;,
## #   priority_data_original &amp;lt;chr&amp;gt;, priority_date &amp;lt;chr&amp;gt;,
## #   publication_country_code &amp;lt;chr&amp;gt;, publication_country_name &amp;lt;chr&amp;gt;,
## #   publication_date &amp;lt;chr&amp;gt;, publication_date_original &amp;lt;chr&amp;gt;,
## #   publication_day &amp;lt;int&amp;gt;, publication_month &amp;lt;int&amp;gt;, publication_number &amp;lt;chr&amp;gt;,
## #   publication_number_espacenet_links &amp;lt;chr&amp;gt;, pubyear &amp;lt;int&amp;gt;,
## #   title_cleaned &amp;lt;chr&amp;gt;, title_nlp_cleaned &amp;lt;chr&amp;gt;,
## #   title_nlp_multiword_phrases &amp;lt;chr&amp;gt;, title_nlp_raw &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will want to create a plot with the applicants data in our infographic software. For that we need to introduce a field to count on. We might also want to establish a cut off point based on the number of records per applicant.&lt;/p&gt;
&lt;p&gt;In this code we will simply print the applicants ranked in descending order. The second to last line of the code provides a filter on the number of records. This value can be changed after inspecting the data. The final line omits NA values (otherwise the top result) where an applicant name is not available.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)
library(dplyr)
applicant_count &amp;lt;- select(applicants, applicants, publication_number) %&amp;gt;% mutate(n = sum(publication_number = 1)) %&amp;gt;% 
    count(applicants, wt = n) %&amp;gt;% arrange(desc(nn)) %&amp;gt;% filter(nn &amp;gt;= 1) %&amp;gt;% na.omit()
applicant_count&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6,178 x 2
##    applicants                              nn
##    &amp;lt;chr&amp;gt;                                &amp;lt;dbl&amp;gt;
##  1 Graphic Packaging International, Inc  154.
##  2 Kraft Foods Holdings, Inc             132.
##  3 Google Inc                            123.
##  4 Microsoft Corporation                  88.
##  5 The Pillsbury Company                  83.
##  6 General Mills, Inc                     77.
##  7 Nestec                                 77.
##  8 The Procter &amp;amp; Gamble Company           59.
##  9 Pizza Hut, Inc                         57.
## 10 Yahoo! Inc                             54.
## # ... with 6,168 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we inspect applicant count using &lt;code&gt;View(applicant_count)&lt;/code&gt; we have 6,178 rows. That is far too many to display in an infographic. So, next we will filter the data on the value for the top ten (54). Then we will write the data to a .csv file using the simple &lt;code&gt;write_csv()&lt;/code&gt; from &lt;code&gt;readr&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidyr)
library(readr)
applicant_count &amp;lt;- select(applicants, applicants, publication_number) %&amp;gt;% mutate(n = sum(publication_number = 1)) %&amp;gt;% 
    count(applicants, wt = n) %&amp;gt;% arrange(desc(nn)) %&amp;gt;% filter(nn &amp;gt;= 54) %&amp;gt;% na.omit()
applicant_count&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##    applicants                              nn
##    &amp;lt;chr&amp;gt;                                &amp;lt;dbl&amp;gt;
##  1 Graphic Packaging International, Inc  154.
##  2 Kraft Foods Holdings, Inc             132.
##  3 Google Inc                            123.
##  4 Microsoft Corporation                  88.
##  5 The Pillsbury Company                  83.
##  6 General Mills, Inc                     77.
##  7 Nestec                                 77.
##  8 The Procter &amp;amp; Gamble Company           59.
##  9 Pizza Hut, Inc                         57.
## 10 Yahoo! Inc                             54.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(applicant_count, &amp;quot;pizza_applicants_1990_2012.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we inspect &lt;code&gt;applicant_count&lt;/code&gt; we will see that Graphic Packaging International is the top result with 154 results with Google ranking third with 123 results followed by Microsoft. This could suggest that Google and Microsoft are suddenly entering the market for online pizza sales or pizza making software or, as is more likely, that there are uses other uses of the word pizza in patent data that we are not aware of.&lt;/p&gt;
&lt;p&gt;As part of our infographic we will want to explore this intriguing result in more detail. We can do this by creating a subdataset for Google using &lt;code&gt;filter()&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;selecting-applicants-using-filter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Selecting applicants using &lt;code&gt;filter()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;As we saw above, while &lt;code&gt;select()&lt;/code&gt; functions with columns, &lt;code&gt;filter()&lt;/code&gt; from &lt;code&gt;dplyr&lt;/code&gt; works with rows. Here we will filter the data to select the rows in the applicants column that contain Google Inc. and then write that to a .csv for use in our infographic. Note the use of double &lt;code&gt;==&lt;/code&gt; and the quotes around “Google Inc”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(readr)
google &amp;lt;- filter(applicants, applicants == &amp;quot;Google Inc&amp;quot;)
google&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 123 x 31
##    title_original applicants applicants_clea… applicants_orga… applicants_orig…
##    &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;           
##  1 Location base… Google Inc Corporate; Peop… Google Inc       Hafsteinsson Gu…
##  2 AUTHORITATIVE… Google Inc Corporate        Google Inc       GOOGLE INC.     
##  3 Location-Base… Google Inc Corporate; Peop… Google Inc       GOOGLE INC.;HAF…
##  4 Controlling t… Google Inc Corporate; Peop… Google Inc       GOOGLE, INC.;BE…
##  5 METHOD AND SY… Google Inc Corporate        Google Inc       GOOGLE INC.     
##  6 Routing queri… Google Inc Corporate        Google Inc       GOOGLE INC.     
##  7 METHODS AND S… Google Inc Corporate        Google Inc       GOOGLE INC.     
##  8 Aspect-based … Google Inc Corporate; Peop… Google Inc       Reis George;Goo…
##  9 Interpreting … Google Inc Corporate        Google Inc       GOOGLE INC.     
## 10 Interpreting … Google Inc Corporate        Google Inc       GOOGLE INC.     
## # ... with 113 more rows, and 26 more variables: inventors_cleaned &amp;lt;chr&amp;gt;,
## #   inventors_original &amp;lt;chr&amp;gt;, ipc_class &amp;lt;chr&amp;gt;, ipc_codes &amp;lt;chr&amp;gt;,
## #   ipc_names &amp;lt;chr&amp;gt;, ipc_original &amp;lt;chr&amp;gt;, ipc_subclass_codes &amp;lt;chr&amp;gt;,
## #   ipc_subclass_detail &amp;lt;chr&amp;gt;, ipc_subclass_names &amp;lt;chr&amp;gt;,
## #   priority_country_code &amp;lt;chr&amp;gt;, priority_country_code_names &amp;lt;chr&amp;gt;,
## #   priority_data_original &amp;lt;chr&amp;gt;, priority_date &amp;lt;chr&amp;gt;,
## #   publication_country_code &amp;lt;chr&amp;gt;, publication_country_name &amp;lt;chr&amp;gt;,
## #   publication_date &amp;lt;chr&amp;gt;, publication_date_original &amp;lt;chr&amp;gt;,
## #   publication_day &amp;lt;int&amp;gt;, publication_month &amp;lt;int&amp;gt;, publication_number &amp;lt;chr&amp;gt;,
## #   publication_number_espacenet_links &amp;lt;chr&amp;gt;, pubyear &amp;lt;int&amp;gt;,
## #   title_cleaned &amp;lt;chr&amp;gt;, title_nlp_cleaned &amp;lt;chr&amp;gt;,
## #   title_nlp_multiword_phrases &amp;lt;chr&amp;gt;, title_nlp_raw &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(google, &amp;quot;google_1990_2012.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the correct result for the period 1990 to 2012 for Google is 123 records from 191 records across the whole pizza dataset. The correct result will be achieved only where you use the filtered, separated and trimmed data we created in the applicants data frame.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generating-ipc-tables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generating IPC Tables&lt;/h2&gt;
&lt;p&gt;In the next step we will want to generate two tables containing International Patent Classification (IPC) data. IPC codes and the Cooperative Patent Classification (CPC, not present in this dataset) provide information on the technologies involved in a patent document. The IPC is hierarchical and proceeds from the general class level to the detailed group and subgroup level. Experience reveals that the majority of patent documents receive more than one IPC code to more fully describe the technological aspects of patent documents.&lt;/p&gt;
&lt;p&gt;The pizza dataset contains IPC codes on the class and the subclass level in concatenated fields. One important consideration in using IPC data is that the descriptions are long and can be difficult for non-specialists to grasp. This can make visualising the data difficult and often requires manual efforts to edit labels for display.&lt;/p&gt;
&lt;p&gt;We now want to generate three IPC tables.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A general IPC table for the pizza dataset&lt;/li&gt;
&lt;li&gt;A general IPC table for the Google dataset&lt;/li&gt;
&lt;li&gt;A more detailed IPC subclass table for the Google dataset&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For ease of presentation in an infographic we will use the &lt;code&gt;ipc_class&lt;/code&gt; field. For many patent analytics purposes this will be too general. However it has the advantage of being easy to visualise.&lt;/p&gt;
&lt;p&gt;To generate the table we can use a generic function based on the code developed for dealing with the applicants data. We will call the function patent_count().&lt;/p&gt;
&lt;!--- updated to tidyeval in 2018. Note that the whitespace was not trimmed in the earlier version due to an oversight and counts will now be higher.. and correct... as a result. ---&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;patent_count &amp;lt;- function(data, col, count_col, sep, n_results) {
    p_count &amp;lt;- dplyr::select(data, !!col, !!count_col) %&amp;gt;% tidyr::separate_rows(col, 
        sep = sep) %&amp;gt;% dplyr::mutate(`:=`(!!col, stringr::str_trim(.[[col]], side = &amp;quot;both&amp;quot;))) %&amp;gt;% 
        dplyr::mutate(n = sum(count_col = 1)) %&amp;gt;% dplyr::group_by(`:=`(!!col, .[[col]])) %&amp;gt;% 
        dplyr::tally(sort = TRUE) %&amp;gt;% dplyr::rename(records = nn) %&amp;gt;% na.omit() %&amp;gt;% 
        head(n_results)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;patent_count()&lt;/code&gt; function is based on the the code we developed for applicants. It contains variations to make it work as a function. The function takes four arguments:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;col = the concatenated column that we want to split and gather back in&lt;/li&gt;
&lt;li&gt;col_count = a column for generating counts (in this dataset the publication_number)&lt;/li&gt;
&lt;li&gt;n_results = the number of results we want to see in the new table (typically 10 or 20 for visualisation). This is equivalent to the number of rows that you want to see.&lt;/li&gt;
&lt;li&gt;sep = the separator to use to separate the data in col. With patent data this is almost always “;” (as &lt;code&gt;;space&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To generate the &lt;code&gt;ipc_class&lt;/code&gt; data we can do the following and then write the file to .csv. Note that we have set the number of results &lt;code&gt;n_results&lt;/code&gt; to 10.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pizza_ipc_class &amp;lt;- patent_count(data = pizza_1990_2012, col = &amp;quot;ipc_class&amp;quot;, count_col = &amp;quot;publication_number&amp;quot;, 
    n_results = 10, sep = &amp;quot;;&amp;quot;)
pizza_ipc_class&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##    ipc_class                                           records
##    &amp;lt;chr&amp;gt;                                                 &amp;lt;dbl&amp;gt;
##  1 A21: Baking                                           2233.
##  2 A23: Foods Or Foodstuffs                              1843.
##  3 B65: Conveying                                        1383.
##  4 G06: Computing                                        1326.
##  5 A47: Furniture                                         932.
##  6 H04: Electric Communication Technique                  747.
##  7 H05: Electric Techniques Not Otherwise Provided For    613.
##  8 F24: Heating                                           512.
##  9 A61: Medical Or Veterinary Science                     318.
## 10 G07: Checking                                          226.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(pizza_ipc_class, &amp;quot;pizza_ipcclass_1990_2012.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that this dataset is based on the main &lt;code&gt;pizza_1990_2012&lt;/code&gt; dataset (including cases where no applicant name is available). The reason we have not used the applicants dataset is because that dataset will duplicate the IPC field for each split of an applicant name. As a result it will over count the IPCs by the number of applicants on a document name. As this suggests, it is important to be careful when working with data that has been tidied because of the impact on other counts.&lt;/p&gt;
&lt;p&gt;This problem does not apply in the case of our Google data because the only applicant listed in that data is Google (excluding co-applicants). We can therefore safely use the Google dataset to identify the IPC codes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;google_ipc_class &amp;lt;- patent_count(data = google, col = &amp;quot;ipc_class&amp;quot;, count_col = &amp;quot;publication_number&amp;quot;, 
    n_results = 10, sep = &amp;quot;;&amp;quot;)
google_ipc_class&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 2
##   ipc_class                             records
##   &amp;lt;chr&amp;gt;                                   &amp;lt;dbl&amp;gt;
## 1 G06: Computing                           105.
## 2 H04: Electric Communication Technique     17.
## 3 G01: Measuring                            14.
## 4 G09: Educating                            11.
## 5 G10: Musical Instruments                   7.
## 6 A63: Sports                                1.
## 7 G08: Signalling                            1.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(google_ipc_class, &amp;quot;google_ipcclass_1990_2012.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are only 7 classes and as we might expect they are dominated by computing. We might want to dig into this in a little more detail and so let’s also create an IPC subclass field.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;google_ipc_subclass &amp;lt;- patent_count(data = google, col = &amp;quot;ipc_subclass_detail&amp;quot;, count_col = &amp;quot;publication_number&amp;quot;, 
    n_results = 10, sep = &amp;quot;;&amp;quot;)
google_ipc_subclass&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##    ipc_subclass_detail                                                  records
##    &amp;lt;chr&amp;gt;                                                                  &amp;lt;dbl&amp;gt;
##  1 G06F: Electric Digital Data Processing                                   89.
##  2 G06Q: Data Processing Systems Or Methods, Specially Adapted For Adm…     24.
##  3 G01C: Measuring Distances, Levels Or Bearings                            14.
##  4 G09B: Educational Or Demonstration Appliances                             9.
##  5 G10L: Speech Analysis Or Synthesis                                        7.
##  6 H04W: Wireless Communication Networks                                     7.
##  7 G09G: Arrangements Or Circuits For Control Of Indicating Devices Us…      5.
##  8 H04B: Transmission                                                        4.
##  9 H04L: Transmission Of Digital Information, E.G. Telegraphic Communi…      4.
## 10 H04M: Telephonic Communication                                            4.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(google_ipc_subclass, &amp;quot;google_ipcsubclass_1990_2012.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have the data on technology areas that we need to understand our data. The next and final step is to generate data from the text fields.&lt;/p&gt;
&lt;div id=&#34;phrases-tables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Phrases Tables&lt;/h3&gt;
&lt;p&gt;We will be using data from words and phrases in the titles of patent documents for use in a word cloud in our infographic. It is possible to generate this type of data in R directly using the &lt;code&gt;tm&lt;/code&gt; and &lt;code&gt;NLP&lt;/code&gt; packages. Our pizza dataset already contains a title field broken down into phrases using Vantagepoint software and so we will use that. We will use the field &lt;code&gt;title_nlp_multiword_phrases&lt;/code&gt; as phrases are generally more informative than individual words. Once again we will use our general &lt;code&gt;patent_count()&lt;/code&gt; function although experimentation may be needed to identify the number of phrases that visualise well in a word cloud.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pizza_phrases &amp;lt;- patent_count(data = pizza_1990_2012, col = &amp;quot;title_nlp_multiword_phrases&amp;quot;, 
    count_col = &amp;quot;publication_number&amp;quot;, n_results = 15, sep = &amp;quot;;&amp;quot;)
pizza_phrases&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 15 x 2
##    title_nlp_multiword_phrases records
##    &amp;lt;chr&amp;gt;                         &amp;lt;dbl&amp;gt;
##  1 Food Product                   179.
##  2 Microwave Ovens                137.
##  3 Making Same                     48.
##  4 conveyor Oven                   46.
##  5 Crust Pizza                     44.
##  6 microwave Heating               41.
##  7 Bakery Product                  40.
##  8 pizza Box                       40.
##  9 Microwave Cooking               39.
## 10 Pizza Oven                      37.
## 11 pizza Dough                     35.
## 12 Cook Food                       34.
## 13 Baked Product                   33.
## 14 Related Method                  32.
## 15 Food Item                       29.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(pizza_phrases, &amp;quot;pizza_phrases_1990_2012.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we do the same with the Google data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;google_phrases &amp;lt;- patent_count(data = google, col = &amp;quot;title_nlp_multiword_phrases&amp;quot;, 
    count_col = &amp;quot;publication_number&amp;quot;, n_results = 15, sep = &amp;quot;;&amp;quot;)
google_phrases&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 15 x 2
##    title_nlp_multiword_phrases           records
##    &amp;lt;chr&amp;gt;                                   &amp;lt;dbl&amp;gt;
##  1 Digital Map System                        10.
##  2 conversion Path Performance Measures       9.
##  3 Mobile Device                              8.
##  4 Search Results                             8.
##  5 Geographical Relevance                     4.
##  6 Local Search Results                       4.
##  7 Location Prominence                        4.
##  8 Network Speech Recognizers                 4.
##  9 Processing Queries                         4.
## 10 Search Query                               4.
## 11 aspect-Based Sentiment Summarization       3.
## 12 authoritative Document Identification      3.
## 13 Business Listings Search                   3.
## 14 Content Providers                          3.
## 15 indexing Documents                         3.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(google_phrases, &amp;quot;google_phrases_1990_2012.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have the following .csv files.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;pizza_total_1990_2012&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pizza_country_1990_2012&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pizza_applicants_1990_2012&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pizza_ipcclass_1990_2012&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pizza_phrases_1990_2012&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Google_1990_2012&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Google_ipclass_1990_2012&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Google_ipcsubclass_1990_2012&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Google_phrases-1990_2012&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-an-infographic-in-infogr.am&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating an infographic in infogr.am&lt;/h2&gt;
&lt;p&gt;If you are starting this chapter here then download the datasets we will be using as a single zip file from the Manual repository &lt;a href=&#34;https://github.com/wipo-analytics/opensource-patent-analytics/blob/master/2_datasets/infographic/infographic.zip?raw=true&#34;&gt;here&lt;/a&gt; and then unzip the file.&lt;/p&gt;
&lt;p&gt;We first need to sign up for a free account with &lt;a href=&#34;https://infogr.am/&#34;&gt;infogr.am&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/infogram/fig1_infogram_front.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will then see a page with some sample infographics to provide ideas to get you started.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/infogram/fig2_infogram_login.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt; Click on one of the infograms with a graph such as Trends in Something and then click inside the graph box itself and select the edit button in the top right.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/infogram/fig3_infogram_findedit.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This will open up a data panel with the toy data displayed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;images/infogram/fig4_infogram_datapanel.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We want to replace this data by choosing the upload button and selecting our &lt;code&gt;pizza_country_1990_2012.csv&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/infogram/fig5_infogram_panelgraph.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We now have a decent looking graph for our country trends data where we can see the number of records per country and year by hovering over the relevant data points. While some of the countries with low frequency data are crunched at the bottom (and would be better displayed in a separate graph), hovering over the data or over a country name will display the relevant country activity. We will therefore live with this.&lt;/p&gt;
&lt;p&gt;We now want to start adding story elements by clicking on the edit button in the title. Next we can start adding new boxes using the menu icons on the right. Here we have changed the title, added a simple body text for the data credit and then a quote from someone describing themselves as the Head of Pizza Analytics.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/infogram/fig6_infogram_paneltext.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next we need to start digging into the data using our IPC, applicants and phrases data.&lt;/p&gt;
&lt;p&gt;To work with our IPC class data we will add a bar chart and load the data. To do this select the graph icon in the right and then Bar. Once again we will choose edit and then load our &lt;code&gt;pizza_ipcclass_1990_2012&lt;/code&gt; dataset. Then we can add a descriptive text box. We can then continue to add elements as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;applicants bar chart&lt;/li&gt;
&lt;li&gt;pizza phrases by selecting graph and word cloud&lt;/li&gt;
&lt;li&gt;Google ipc-subclass&lt;/li&gt;
&lt;li&gt;Google word cloud.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One useful approach to developing an infographic is to start by adding the images and then add titles and text boxes to raise key points. In infogram new text boxes appear below existing boxes but can be repositioned by dragging and dropping boxes onto each other.&lt;/p&gt;
&lt;p&gt;One nice feature of infogram is that it is easy to share the infographic with others through a url, an embed code or on facebook or via twitter.&lt;/p&gt;
&lt;p&gt;At the end of the infographic it is a good idea to provide a link where the reader can obtain more information, such as the full report or the underlying data. In this case we will add a link to the Tableau workbook on pizza patent activity that we developed in an earlier &lt;a href=&#34;https://public.tableau.com/profile/wipo.open.source.patent.analytics.manual#!/vizhome/pizzapatents/Overview&#34;&gt;chapter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Our final infographic should look something like &lt;a href=&#34;https://infogr.am/trends_in_something&#34;&gt;this&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Round Up&lt;/h3&gt;
&lt;p&gt;In this chapter we have concentrated on using R to tidy patent data in order to create an online infographic using free software. Using our trusty pizza patent data from WIPO Patentscope we walked through the process of wrangling and tidying patent data first using short lines of code that we then combined into a reusable function. As this introduction to tidying data in R has hopefully revealed, R and packages such as &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;tidyr&lt;/code&gt; and &lt;code&gt;stringr&lt;/code&gt; provide very useful tools for working with patent data, and they are free and well supported.&lt;/p&gt;
&lt;p&gt;In the final part of the chapter we used the data we had generated in RStudio to create an infographic using infogr.am that we then shared online. Infogram is just one of a number of online infographic services and it is well worth trying other services such as &lt;a href=&#34;https://www.easel.ly&#34;&gt;easel.ly&lt;/a&gt; to find a service that meets your needs.&lt;/p&gt;
&lt;p&gt;As regular users of R will already know, it is already possible to produce all of these graphics (such as word clouds) directly in R using tools such as &lt;code&gt;ggplot2&lt;/code&gt;, &lt;code&gt;plotly&lt;/code&gt; and word clouds using packages such as &lt;code&gt;wordcloud&lt;/code&gt;. Some of these topics have been covered in other chapters and for more on text mining and word clouds in R see this recent article on &lt;a href=&#34;http://www.r-bloggers.com/building-wordclouds-in-r/&#34;&gt;R-bloggers&lt;/a&gt;. None of the infographic services we viewed appeared to offer an API that would enable a direct connection with R. There also seems to be a gap in R’s packages where infographics might sit with this &lt;a href=&#34;http://www.r-bloggers.com/r-how-to-layout-and-design-an-infographic/&#34;&gt;2015 R-bloggers article&lt;/a&gt; providing a walk through on how to create a basic infographic.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://stackoverflow.com/questions/25975827/how-to-feed-the-result-of-a-pipe-chain-magrittr-to-an-object&#34; class=&#34;uri&#34;&gt;http://stackoverflow.com/questions/25975827/how-to-feed-the-result-of-a-pipe-chain-magrittr-to-an-object&lt;/a&gt;&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Accessing Patent Data with the Lens</title>
      <link>https://www.pauloldham.net/lens/</link>
      <pubDate>Sun, 26 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>https://www.pauloldham.net/lens/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this article we provide a brief introduction to &lt;a href=&#34;https://www.lens.org/lens/&#34;&gt;The Lens&lt;/a&gt; patent database as a free source of data for patent analytics.&lt;/p&gt;
&lt;p&gt;The Lens is a patent database based in Australia that describes itself as “an open global cyberinfrastructure to make the innovation system more efficient and fair, more transparent and inclusive.” The main way it seeks to do this is by providing access to patent information with a particular focus on sequence information as well as analysis of issues such as DNA related patent activity. An important feature of The Lens for those working on biotechnology related subjects is &lt;a href=&#34;https://www.lens.org/lens/bio&#34;&gt;PatSeq&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This article is now a chapter in the &lt;a href=&#34;https://wipo-analytics.github.io/&#34;&gt;WIPO Manual on Open Source Patent Analytics&lt;/a&gt;. You can read the chapter in electronic book format &lt;a href=&#34;https://wipo-analytics.github.io/the-lens-1.html&#34;&gt;here&lt;/a&gt; and find all the materials including presentations at the &lt;a href=&#34;https://github.com/wipo-analytics&#34;&gt;WIPO Analytics Github homepage&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-started&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;To get the most out of the Lens the first step is to sign up for an account from the front page.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/lens/fig1_front.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is possible to begin searching directly from the front page. However, selecting the small button next to the search box takes you to the search controls.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/lens/fig2_controls.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see we can use boolean queries, for searching a range of fields including the full text, title, abstract or claims (a major plus). We can also select one or multiple jurisdictions. In addition the results can be refined to patent applications or grants, and there are options for full text or one doc per family (which greatly reduces the number of results).&lt;/p&gt;
&lt;p&gt;We used our standard query “pizza”, all jurisdictions, and one document per family. We turned stemming off.&lt;/p&gt;
&lt;p&gt;Our search for pizza returned 13,714 families from a total of 29,617 publications containing the term in the full text. This approach assists with refining searches by reducing duplication.&lt;/p&gt;
&lt;p&gt;The Lens allows users to create collections of up to 10,000 results from a search. To create a collection use the &lt;code&gt;Create Collection&lt;/code&gt; button and name the collection. How you add records to a collection is not obvious and involves 2 steps.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Check the arrow next to Document as in the image below. When the mouse hovers over the arrow a menu will pop up. Choose &lt;code&gt;Top 10k Results&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;In the box displaying the name of the collection above the results press the + arrow to add the 10,000 documents to the Collection.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/lens/fig3_addtocollection.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Once you understand this process it is easy to add documents to collections. One very nice feature of the Lens is that when a collection has been created we can share it with others using the &lt;code&gt;Share&lt;/code&gt; button. Users have the option of maintaining a private collection or publicly sharing. The URL for the collection we just generated is &lt;a href=&#34;https://www.lens.org/lens/collection/9606&#34; class=&#34;uri&#34;&gt;https://www.lens.org/lens/collection/9606&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We could imagine that for more restricted searches, and taking confidentiality issues into account, this could be a useful way of sharing patent data with colleagues. One useful addition would be the ability to share with groups based on email addresses or something similar (although that may be possible by choosing a private link and sharing it).&lt;/p&gt;
&lt;p&gt;Using the small icons above &lt;code&gt;Document&lt;/code&gt; on the left we can save our query for later use, limit the data to simple families or expand to publications, and download the data.&lt;/p&gt;
&lt;p&gt;There are two main options for downloading data. The first is to download 1000 records by selecting the export button above &lt;code&gt;Document&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When we select the export button we will be presented with a choice on the number of records to export and whether to export in JSON (for programmatic use), RIS for bibliographic software or .csv for use in tools such as Excel or other programmes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/lens/fig4_exportoptions.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The outputs of the export are clean and clear about what they represent when compared with some patent databases. A &lt;code&gt;url&lt;/code&gt; link to the relevant file on the Lens is also provided which can assist in reviewing documents.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/lens/fig5_export.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The JSON output (in the lower right of the image above) is also nice and clean.&lt;/p&gt;
&lt;p&gt;The second route to exporting data is to download up to 10,000 results using the collections. When we select the &lt;code&gt;Work Area&lt;/code&gt; icon at the top of the screen and select &lt;code&gt;Collections&lt;/code&gt; we will see a new screen with a range of icons next to an individual collection.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/lens/fig5a_export_collection.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When we select the download icon we can now download the 10,000 records in the collection in either .csv, ris or JSON formats. This is very easy to use once you understand how to navigate the interface.&lt;/p&gt;
&lt;p&gt;We also have an option to upload documents into a collection using the upload button and then enter comma separated identifiers. However, at the time of writing we were not able to make this very useful function work.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;additional-features&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Additional features&lt;/h2&gt;
&lt;p&gt;In addition to these features, it is also important to note that data exports include a cited count that counts the number of patent/non-patent records &lt;code&gt;cited&lt;/code&gt; by the applicant.&lt;/p&gt;
&lt;p&gt;The online data also shows the citing documents. For example &lt;a href=&#34;https://www.lens.org/lens/patent/US_3982033_A/citations#c/out&#34;&gt;US 3982033 A Process for Coating Pizza Shells With Sauce&lt;/a&gt; cites three patent documents but has &lt;a href=&#34;https://www.lens.org/lens/patent/US_3982033_A/citations#c/in&#34;&gt;11 forward citations by later applicants&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;While the citing documents are not included with the downloaded data it is possible to visit a record of interest online and then create a new set with the citing documents. Where a number of documents of interest have been identified this could be the basis for creating a new collection of cited or citing literature on a topic of interest linked to a core query.&lt;/p&gt;
&lt;p&gt;As such, one possible workflow using the Lens would involve initial exploratory queries and refinement, downloading the results of a refined query for closer inspection and then selecting documents of interest to explore the backward (cited) and forward (citing) citations and generate a new dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualisation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualisation&lt;/h2&gt;
&lt;p&gt;The Lens makes good use of online visualisation options using &lt;a href=&#34;http://www.highcharts.com&#34;&gt;Highcharts&lt;/a&gt; and HTML5. To access the visualisations choose the small icon on the right above the &lt;code&gt;Sort by&lt;/code&gt; pull down menu.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/lens/fig6_visual.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We now see a set of charts for our results. Using the up icon in the top right of each image we can get an expanded view and work with the charts. The Lens uses the Highcharts Javascript library and a very nice feature of this approach is that it the visuals are interactive and can be used to refine search results. In the image below we have opened the applicants image. As an aside, note that each image can be copied as an iframe to embed in your own web page.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/lens/fig7_applicants.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This suggests that Google is the top user of the word pizza in the patent system with &lt;a href=&#34;https://www.lens.org/lens/collection/9608&#34;&gt;880 documents in 353 families&lt;/a&gt;. We can then select the top result and the charts will regenerate focusing on our selection (in this case Google). To view the results we need to select the results button (the first on the right above the charts) to see the following.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/lens/fig8_google.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What is very useful about is that it is easy to create a &lt;a href=&#34;https://www.lens.org/lens/collection/9608&#34;&gt;new collection&lt;/a&gt; for an applicant of interest, to download the results or select areas of a portfolio based on a jurisdiction or technology area or to explore highly cited patents. In short, we can easily dig into the data.&lt;/p&gt;
&lt;p&gt;Other interesting features of the chart area are references to authors, DOIs, and PubMed Ids for exploration of data extracted from the documents. This reflects the interest at the Lens in researching the relationship between basic scientific research and innovation. Accessing the literature related information requires opening a chart (for example authors) and selecting the top result and the moving into the results view. We then select one of the results such as &lt;a href=&#34;https://www.lens.org/lens/patent/US_8200847_B2/citations#c/publications&#34;&gt;Voice Actions On Computing Devices&lt;/a&gt; and the Citations tab. This reveals a publication from a workshop on Wireless Geographical Information Systems from 2003 as we can see below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/lens/fig9_crossref.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An impressive feature of this approach is the effort that has been made to link the citation data to the publication using &lt;a href=&#34;http://www.crossref.org&#34;&gt;crossref&lt;/a&gt;. According to the documentation around 15 million non-patent literature citations have been linked so far. Note that one additional feature of the Lens download data is that it includes a non-patent literature citation field. For example, downloading the &lt;a href=&#34;https://www.lens.org/lens/collection/9608&#34;&gt;google pizza portfolio&lt;/a&gt; and a search for the citation above will reveal the citation but without the added value of the DOI. As such, the download provided the raw NPL data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;working-with-texts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Working with texts&lt;/h2&gt;
&lt;p&gt;In common with other free databases, the Lens is not designed to allow downloads of multiple full texts. However, you can access the full text of documents, including .pdf files, and you can make notes that will be stored with a collection in your account. The image below provides an example of our ongoing efforts to understand why Google is so dominant in the results of searches for pizza in patent documents.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/lens/fig10_notes.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;patseq&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PatSeq&lt;/h2&gt;
&lt;p&gt;One important focus of the development of the Lens has been DNA sequence data including an &lt;a href=&#34;https://www.lens.org/about/&#34;&gt;ongoing series of articles&lt;/a&gt; on the interpretation and significance of sequence data in patent activity.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/lens/fig11_patseq.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Patseq includes a number of tools.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;PatSeq data permits access to patent documents disclosing sequences available for bulk download from a growing number of countries. This is a very useful site for obtaining sequence data. Note that you will need to request access to download sequence data in your account area.&lt;/li&gt;
&lt;li&gt;Species finder and keyword search focuses on searching documents that contain a sequence for a species name or key term.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/lens/fig12_patseq_species.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A series of patent portfolios have been generated for some major plant and animal species, e.g. rice, maize, humans, chickens etc. That can be downloaded as collections.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The PatSeq Explorer allows the exploration of sequence data for four genomes (at present), notably the human and mouse genome for animals and the soybean, maize and rice genome for plants.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/lens/fig13_patseq_explorer.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is an area where researchers with &lt;a href=&#34;http://www.cambia.org/daisy/cambia/home.html&#34;&gt;Cambia&lt;/a&gt;, the non-profit organisation behind the Lens, have invested considerable effort and it is well worth reading the research articles listed on the Cambia and Lens websites on this topic. PatSeq Analyzer is closely related to the Explorer and presently provides details on the genomes mentioned above with a detailed summary of sequences by document including the region, sequence, transcript, single nucleotide polymorphisms (SNPs) and grants with sequences in the patent claims.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;PatSeq Finder&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The PatSeq Finder allows a user to enter a DNA or amino acid sequence into the search box and find applications and grants with identical or similar sequences. We selected a sequence at random from the WIPO Patentscope sequence listings browser &lt;a href=&#34;http://www.wipo.int/patentscope/search/en/detail.jsf?LANGUAGE=ENG&amp;amp;KEY=16/026850&amp;amp;ELEMENT_SET=F&#34;&gt;W016/026850&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/lens/fig14_seq_explorer.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;After processing we will see a list of results that can be downloaded in a variety of formats. The results indicate that our random sequence does not appear in the claims of a granted patent or a patent application but does appear in a number of applications and grants. Further details are provided by hovering over the individual entries and additional controls are available for similarity and other scores to refine the results.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/lens/fig15_seq_results.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As far as we could tell, while the data can be downloaded, it is not presently possible to generate a collection of documents from the results of the PatSeq Finder.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round Up&lt;/h2&gt;
&lt;p&gt;The Lens is a very useful patent database that, when you have worked out the meaning of icons, is easy to use. The ease with which collections can be shared and up to 10,000 records downloaded is a real plus for the Lens. In addition, the use of HTML5 and Highcharts makes this a highly interactive experience. The ability to use charts to drill down into the data is very welcome. The link to the &lt;code&gt;crossref&lt;/code&gt; service for non-patent literature is very useful but it would be good to see this data included in some way as a field in the data downloads.&lt;/p&gt;
&lt;p&gt;With the addition of data downloads (in 2015) the Lens is becoming a very useful platform for searching, refining, visualizing and downloading patent data. What would perhaps be useful would be a set of demonstration walkthroughs or use cases that explain the way in which the Lens can be used in common work flows. For example, developing and refining a search, testing results, then retrieving backward and forward citations for refinement and visualization are quite common tasks in patent landscape analysis. Use cases would help users make the most of what the Lens has to offer.&lt;/p&gt;
&lt;p&gt;The Lens also stands out for its distinctive long term work on sequence data in patents and this will be of particular interest to researchers working on biotech particularly in exploring the analytical tools.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An Overview of Patent Analytics Tools</title>
      <link>https://www.pauloldham.net/overview-open-tools/</link>
      <pubDate>Sun, 26 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>https://www.pauloldham.net/overview-open-tools/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This article provides an overview of the open source and free software tools that are available for patent analytics. The aim of the chapter is to serve as a quick reference guide for some of the main tools in the tool kit.&lt;/p&gt;
&lt;p&gt;This article is now a chapter in the &lt;a href=&#34;https://wipo-analytics.github.io/&#34;&gt;WIPO Manual on Open Source Patent Analytics&lt;/a&gt;. You can read the chapter in electronic book format &lt;a href=&#34;https://wipo-analytics.github.io/an-overview-of-tools.html&#34;&gt;here&lt;/a&gt; and find all the materials including presentations at the &lt;a href=&#34;https://github.com/wipo-analytics&#34;&gt;WIPO Analytics Github homepage&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We will go into some of the tools covered in his aticle in more depth elsewhere in the WIPO Manual on Open Source Patent Analytics and leave you to explore the rest of the tools for yourself.&lt;/p&gt;
&lt;p&gt;Before we start it is important to note that we cover only a fraction of the available tools that are out there. We have simply tried to identify some of the most accessible and useful tools. Data mining and visualization are growing rapidly to the point that it is easy to be overwhelmed by the range of choices. The good news is that there are some very high quality free and open source tools out there. The difficulty lies in identifying those that will best serve your specific needs relative to your background and the time available to acquire some programming skills. That decision will be up to you. However, to avoid frustration it will be important to recognise that the different tools take time to master. In some cases, such as R and Python, there are lots of free resources out there to help you take the first steps into programming. In making a decision about a tool to use, think carefully about the level of support that is already out there. Try to use a tool with an active and preferably large community of users. That way, when you get stuck, there will be someone out there who has run into similar issues who will be able to help. Sites such as &lt;a href=&#34;http://stackoverflow.com&#34;&gt;Stack Overflow&lt;/a&gt; are excellent for finding solutions to problems.&lt;/p&gt;
&lt;p&gt;This chapter is divided into 8 sections:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;General Tools&lt;/li&gt;
&lt;li&gt;Cleaning Tools&lt;/li&gt;
&lt;li&gt;Data Mining&lt;/li&gt;
&lt;li&gt;Data visualization&lt;/li&gt;
&lt;li&gt;Network visualization&lt;/li&gt;
&lt;li&gt;Infographics&lt;/li&gt;
&lt;li&gt;Geographic Mapping&lt;/li&gt;
&lt;li&gt;Text Mining&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In some cases tools are multifunctional and so may appear in one section where they could also appear in another. Rather than repeating information we will let you figure that out.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;general-tools&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;General Tools&lt;/h2&gt;
&lt;p&gt;Quite a number of free tools are available for multi-purpose tasks such as basic cleaning of patent data and visualization. We highlight three free tools here.&lt;/p&gt;
&lt;div id=&#34;open-office&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.openoffice.org&#34;&gt;Open Office&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Many patent analysts will use Excel as a default programme including basic cleaning of smaller datasets. However, it is well worth considering Apache Open Office as a free alternative. While patent analysis will typically use the Spreadsheet (Open Office Calc) there is also a very useful Database option as an alternative to Microsoft Access.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download and Install &lt;a href=&#34;https://www.openoffice.org&#34;&gt;Apache Open Office&lt;/a&gt; for your system.&lt;/li&gt;
&lt;li&gt;Tip: When saving spreadsheet files, choose save as &lt;strong&gt;.csv&lt;/strong&gt; to avoid situations where a programme can’t read the default &lt;strong&gt;.odt&lt;/strong&gt; files.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;google-sheets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.google.co.uk/sheets/about/&#34;&gt;Google Sheets&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Google Sheets require a free Google account and those who are comfortable with Excel may wonder why it is worth switching. However, Google Sheets can be shared online with others and there are a large number of free add ons that could be used to assist with cleaning data such as Split Names or Remove duplicates as shown below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/google_sheets_addon.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;google-fusion-tables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://support.google.com/fusiontables/answer/2571232?hl=en&#34;&gt;Google Fusion Tables&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Fusion Tables are similar to Google Sheets but can work with millions of records. However, it is worth trying with smaller datasets to see if Fusion Tables suit your needs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/Fusion-Tables.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Fusion Tables appear very much like a spreadsheet. However, the Table also contains a &lt;code&gt;cards&lt;/code&gt; feature which allows each record to be seen as a whole and easily filtered. The cards can be much easier to work with than the standard row format where information in a record can be difficult to take in. Fusion Tables also attempts to use geocoded data to draw a Google Map as we can see in the second image below for the publication country from a sample patent dataset.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/Fusion-Tables-Cards.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/Fusion-Tables-Map.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;cleaning-tools&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cleaning Tools&lt;/h2&gt;
&lt;div id=&#34;open-refine-formerly-google-refine&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://openrefine.org&#34;&gt;Open Refine&lt;/a&gt; (formerly Google Refine)&lt;/h3&gt;
&lt;p&gt;A fundamental rule of data analysis and visualization is: &lt;code&gt;rubbish in = rubbish out&lt;/code&gt;. If your data has not been cleaned in the first place, do not be surprised if the results of analysis or visualization are rubbish.&lt;/p&gt;
&lt;p&gt;An in depth chapter is available &lt;a href=&#34;http://poldham.github.io/openrefine-patent-cleaning/&#34;&gt;here&lt;/a&gt; on the use of &lt;a href=&#34;http://openrefine.org&#34;&gt;Open Refine&lt;/a&gt;, formerly Google Refine, for cleaning patent data. For patent analytics Open Refine is an important free tool for cleaning applicant and inventor names.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/openrefine/OpenRefine-download.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A number of platforms provide data cleaning facilities and it is possible to do quite a lot of basic cleaning in either Open Office or Excel. Open Refine is the most accessible tool for timely cleaning of patent name fields. In particular, it is very useful for splitting and cleaning thousands of patent applicant and inventor names.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-mining&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Mining&lt;/h2&gt;
&lt;p&gt;There are an ever growing number of data mining tools out there. Here are a few of those that have caught our attention with additional tools listed below.&lt;/p&gt;
&lt;div id=&#34;rstudio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.rstudio.com&#34;&gt;RStudio&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A very powerful tool for working with data and visualising data using R and then writing about it (this chapter, and the wider Manual, is entirely written in Rmarkdown with RStudio). While the learning curve with R can be intimidating a great deal of effort goes in to making R accessible through &lt;a href=&#34;http://www.rstudio.com/resources/training/online-learning/&#34;&gt;tutorials&lt;/a&gt; such as those on &lt;a href=&#34;https://www.datacamp.com&#34;&gt;DataCamp&lt;/a&gt;, &lt;a href=&#34;http://www.rstudio.com/resources/webinars/&#34;&gt;webinars&lt;/a&gt;, &lt;a href=&#34;http://www.r-bloggers.com&#34;&gt;R-Bloggers&lt;/a&gt; and &lt;a href=&#34;http://stackoverflow.com/questions/tagged/r&#34;&gt;Stack Overflow&lt;/a&gt; and free university courses such as the well known John Hopkins University R Programming Course on &lt;a href=&#34;https://www.coursera.org/course/rprog&#34;&gt;Coursera&lt;/a&gt;. Indeed, as with Python, there is so much support for users at different levels that it is hard ever to feel alone when using R and RStudio.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/RStudio.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To get started with R download RStudio for your platform by &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/&#34;&gt;following these instructions&lt;/a&gt; and making sure to install R from the link provided.&lt;/p&gt;
&lt;p&gt;If you are completely new to R then &lt;a href=&#34;https://www.datacamp.com/&#34;&gt;DataCamp&lt;/a&gt; is a good place to start. The free John Hopkins University &lt;a href=&#34;https://www.coursera.org/course/rprog&#34;&gt;R Programming Course on Coursera&lt;/a&gt; is also very good. The John Hopkins University course is accompanied by the Swirl tutorial package that can be installed using `install.packages(“swirl”) when you have installed R. This is a real asset when getting started.&lt;/p&gt;
&lt;p&gt;In developing this Manual we mainly focused on developing resources with R. However, we would emphasise that Python may also be important for your needs. For a recent discussion on the strengths and weaknesses of R and Python see this &lt;a href=&#34;http://www.r-bloggers.com/choosing-r-or-python-for-data-analysis-an-infographic/&#34;&gt;Datacamp article on the &lt;code&gt;Data Science Wars&lt;/code&gt; and accompanying excellent infographic&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rapidminer-studio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://rapidminer.com/products/studio/&#34;&gt;RapidMiner Studio&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Comes with a free service and a variety of tiered paid plans. RapidMiner focuses on machine learning, data mining, text mining and analytics.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/RapidMiner.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;knime&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.knime.org&#34;&gt;KNIME&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;An open platform for data mining.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/KNIME.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Other data mining tools (such as &lt;a href=&#34;http://www.cs.waikato.ac.nz/ml/weka/&#34;&gt;WEKA&lt;/a&gt; and &lt;a href=&#34;http://www.nltk.org&#34;&gt;NLTK&lt;/a&gt; in Python are covered below). If you would like to explore other data mining software try this &lt;a href=&#34;http://www.predictiveanalyticstoday.com/top-free-data-mining-software/&#34;&gt;article&lt;/a&gt; for some ideas.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-visualization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data visualization&lt;/h2&gt;
&lt;p&gt;If you are new to data visualization we suggest that you might be interested in the work of Edward Tufte at Yale University and his famous book &lt;a href=&#34;http://en.wikipedia.org/wiki/Edward_Tufte&#34;&gt;The Visual Display of Quantitative Information&lt;/a&gt;. His &lt;a href=&#34;http://users.ha.uth.gr/tgd/pt0501/09/Tufte.pdf&#34;&gt;critique of the uses and abuses of Powerpoint&lt;/a&gt; is also entertaining and insightful. The work of Stephen Few, such as &lt;a href=&#34;http://www.analyticspress.com/show.html&#34;&gt;Show Me the Numbers: Designing Tables and Graps to Enlighten&lt;/a&gt; is also popular.&lt;/p&gt;
&lt;p&gt;Remember that data visualization is first and foremost about communication with an audience. That involves choices about how to communicate and finding ways to communicate clearly. In very many cases the outcome of patent analysis and visualization will be a report and a presentation. Tufte’s critique of &lt;a href=&#34;http://users.ha.uth.gr/tgd/pt0501/09/Tufte.pdf&#34;&gt;powerpoint presentations&lt;/a&gt; should be required reading for presenters. You may also like to take a look at Nancy Duarte’s &lt;a href=&#34;http://resonate.duarte.com/#!page0&#34;&gt;Resonate&lt;/a&gt; for ideas on polishing up presentations and storytelling. The style may not suit everyone but &lt;a href=&#34;http://resonate.duarte.com/#!page0&#34;&gt;Resonate&lt;/a&gt; contains very useful messages and insights. In an offline environment, consider Katy Borner’s &lt;a href=&#34;https://mitpress.mit.edu/index.php?q=books/atlas-science&#34;&gt;Atlas of Science: Visualising What We Know&lt;/a&gt; as an excellent guide to the history of visualizations of scientific activity including pioneering visualizations of patent activity. Bear in mind that effective visualization takes practice and is a quite well trodden path.&lt;/p&gt;
&lt;p&gt;There are a lot of choices out there for data visualization tools and the number of tools is growing rapidly. For business analytics Gartner provides a useful (but subscription based) &lt;a href=&#34;http://www.informationweek.com/big-data/big-data-analytics/gartner-bi-magic-quadrant-2015-spots-market-turmoil/d/d-id/1319214&#34;&gt;Magic Quadrant for Business Intelligence and Analytics&lt;/a&gt; report that seeks to map out the leaders in the field. These types of reports can be useful for spotting up and coming companies and checking if there is a free version of the software (other than a short free trial).&lt;/p&gt;
&lt;p&gt;We would suggest thinking carefully about your needs and the learning curve involved. For example, if you have limited programming knowledge (or no time or desire to learn) choose a tool that will largely do the job for you. If you already have experience with javascript, Java, R or Python, or similar, then choose a tool that you feel most comfortable with. In particular, keep an eye out for tools with an API (application programming interface) in a variety of language flavours (such as Python or R) that are likely to meet your needs.&lt;/p&gt;
&lt;p&gt;If you are completely new to data visualization &lt;a href=&#34;https://public.tableau.com/s/gallery&#34;&gt;Tableau Public&lt;/a&gt; and &lt;a href=&#34;http://poldham.github.io/tableau-patents/&#34;&gt;our walk through chapter&lt;/a&gt; are a good place to learn without knowing anything about programming. Some other tools in this list are similar to Tableau Public (in part because Tableau is the market leader). We will also provide some pointers to visualization overview sites at the end of this section where you can find out about what is new and interesting in data visualization.&lt;/p&gt;
&lt;div id=&#34;google-charts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://developers.google.com/chart/interactive/docs/gallery&#34;&gt;Google Charts&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Create a Google Account to Access Google Spreadsheets and other Google programmes&lt;/li&gt;
&lt;li&gt;Take a look at the &lt;a href=&#34;https://developers.google.com/chart/interactive/docs/gallery&#34;&gt;Google Charts Gallery&lt;/a&gt; and &lt;a href=&#34;https://developers.google.com/chart/interactive/docs/reference&#34;&gt;API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;For an overview of using Google Charts in R then see the &lt;code&gt;GoogleVis&lt;/code&gt; package and its examples &lt;a href=&#34;http://cran.r-project.org/web/packages/googleVis/vignettes/googleVis.pdf&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;For an overview using Google Charts with Python see the &lt;a href=&#34;https://code.google.com/p/google-chartwrapper/&#34;&gt;google-chartwrapper&lt;/a&gt; or &lt;a href=&#34;http://python-google-charts.readthedocs.org/en/latest/#&#34;&gt;Python Google Charts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/google_sheets_addon.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tableau-public&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://public.tableau.com/s/&#34;&gt;Tableau Public&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;An in depth chapter on getting started with patent analysis and visualization using Tableau Public is available &lt;a href=&#34;http://poldham.github.io/tableau-patents/&#34;&gt;here&lt;/a&gt;. When your patent data has been cleaned, Tableau Public is a powerful way of developing interactive dashboards and maps with your data and combining it with other data sources. Bear in mind that Tableau Public data is, by definition, public and it should not be used with sensitive data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/dashboard_completed.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The workbook can be viewed online &lt;a href=&#34;https://public.tableau.com/profile/wipo.open.source.patent.analytics.manual#!/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-and-rstudio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R and RStudio&lt;/h3&gt;
&lt;p&gt;R is a statistical programming language for working with all kinds of different types of data. It also has powerful visualization tools including &lt;code&gt;packages&lt;/code&gt; that provide an interface with Google Charts, &lt;a href=&#34;https://plot.ly&#34;&gt;Plotly&lt;/a&gt; and others. If you are interested in using R then we suggest using RStudio which can be downloaded &lt;a href=&#34;http://www.rstudio.com/&#34;&gt;here&lt;/a&gt;. The entire WIPO Open Source Patent Analytics Manual was written in RStudio using Rmarkdown to output the articles for the web, .pdf and presentations. As this suggests, it is not simply about data visualization. To get started with R and RStudio try the free tutorials at &lt;a href=&#34;https://www.datacamp.com/&#34;&gt;DataCamp&lt;/a&gt;. We will cover R in more detail in other chapters and online articles.&lt;/p&gt;
&lt;p&gt;As part of an approach described as &lt;code&gt;The Grammar of Graphics&lt;/code&gt;, inspired by &lt;a href=&#34;https://en.wikipedia.org/wiki/Leland_Wilkinson&#34;&gt;Leland Wilkinson’s work&lt;/a&gt;, developers at RStudio and others have created packages that provide very useful ways to visualise and map data. The links below will take you to the documentation for some of the most popular data visualization packages.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;http://cran.r-project.org/web/packages/ggplot2/index.html&#34;&gt;ggplot2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cran.r-project.org/web/packages/ggvis/index.html&#34;&gt;ggvis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cran.r-project.org/web/packages/ggmap/index.html&#34;&gt;ggmap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cran.r-project.org/web/packages/googleVis/index.html&#34;&gt;googleVis&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will cover &lt;code&gt;ggplot2&lt;/code&gt; and &lt;code&gt;ggvis&lt;/code&gt; in greater depth in future chapters. Until then, to get started see the chapters on &lt;code&gt;ggplot2&lt;/code&gt; on &lt;a href=&#34;http://www.r-bloggers.com/search/ggplot2&#34;&gt;R-Bloggers&lt;/a&gt; and here for &lt;a href=&#34;http://www.r-bloggers.com/?s=ggvis&#34;&gt;ggvis&lt;/a&gt;. Datacamp offers a free tutorial on the use of &lt;code&gt;ggvis&lt;/code&gt; that can be accessed &lt;a href=&#34;http://www.r-bloggers.com/ggvis-tutorial-become-a-data-visualization-expert-with-rstudio/&#34;&gt;here&lt;/a&gt;. For a wider overview of some of the top R packages see Qin Wenfeng’s recent &lt;a href=&#34;https://github.com/qinwf/awesome-R&#34;&gt;awesome R list&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;shiny&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Shiny&lt;/h3&gt;
&lt;p&gt;Shiny from &lt;a href=&#34;(http://www.rstudio.com/)&#34;&gt;RStudio&lt;/a&gt; is a web application framework for R. What that means is that you can output tables and visual data from R such as those from the tools mentioned above to the web.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/Shiny.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://shiny.rstudio.com&#34;&gt;Shiny apps&lt;/a&gt; for R users allows for the creation of online interactive apps (upto 5 for free). See the &lt;a href=&#34;http://shiny.rstudio.com/gallery/&#34;&gt;Gallery&lt;/a&gt; for examples. See &lt;a href=&#34;http://www.r-bloggers.com/search/shiny&#34;&gt;RBloggers&lt;/a&gt; for more examples and tutorials.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://vnijs.github.io/radiant/index.html&#34;&gt;Radiant&lt;/a&gt; is a browser based platform for business analytics in R. It is based on Shiny (above) but is specifically business focused.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/Radiant.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For a series of starter videos on Radiant see &lt;a href=&#34;http://vnijs.github.io/radiant/tutorials.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ibm-many-eyes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www-01.ibm.com/software/analytics/many-eyes/&#34;&gt;IBM Many Eyes&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;You need to Register for a free account to really understand what this is about, &lt;a href=&#34;http://www-969.ibm.com/software/analytics/manyeyes/&#34;&gt;try this page&lt;/a&gt; and select register in the top right.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/manyeyes.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-visualization-tools&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other visualization Tools&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://tulip.labri.fr/TulipDrupal/&#34;&gt;Tulip&lt;/a&gt;: data visualization framework in C++&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sigmajs.org/&#34;&gt;SigmaJS&lt;/a&gt;: JavaScript library dedicated to graph drawing. It allows the creation of interactive static and dynamic graphs&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.telerik.com/download/kendo-ui-web-open-source&#34;&gt;Kendo UI&lt;/a&gt;: Create widgets for responsive visualizations.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://timeline.knightlab.com/&#34;&gt;Timeline&lt;/a&gt;: A KnightLab (northwestern university) is a tool allowing for the creation of interactive timelines and is available in 40 languages.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://misoproject.com/&#34;&gt;Miso Project&lt;/a&gt;: open source toolkit facilitating the creation of interactive storytelling and data visualization&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sci2.cns.iu.edu/user/index.php&#34;&gt;Sci2&lt;/a&gt;: A toolset for studying science.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.simile-widgets.org&#34;&gt;Simile Widgets&lt;/a&gt; Web widgets for storytelling as a spin off from the SIMILE Project at MIT.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.jqplot.com&#34;&gt;jqPlot&lt;/a&gt;. An open source jQuery based Plotting Plugin.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.dipity.com&#34;&gt;dipity&lt;/a&gt; for Timelines (free and premium services)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For additional visualization tools and ideas see &lt;a href=&#34;http://www.visualizing.org/&#34;&gt;visualizing.org&lt;/a&gt; and &lt;a href=&#34;http://opendata-tools.org/en/visualization/&#34;&gt;Open Data Tools&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;network-visualization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Network Visualization&lt;/h2&gt;
&lt;p&gt;Network visualization software is an important tool for visualising actors in a field of science and technology and, in particular, the relationships between them. For patent analysis it can be used for a range of purposes including:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Visualising networks of applicants and inventors in a particular field or scientific researchers. An example of this type of work for synthetic biology is &lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0034368&#34;&gt;here&lt;/a&gt; for a network of approximately 2,000 authors of articles on synthetic biology.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/synbio.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Visualising areas of technology and their relationships using the International Patent Classification and the Cooperative Patent Classification (CPC). Previous work at WIPO pioneered the use of large scale patent network analysis to identify the &lt;a href=&#34;http://www.wipo.int/patentscope/en/programs/patent_landscapes/reports/animal_gr.html&#34;&gt;patent landscape for animal genetic resources&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The image below displays a network map of Cooperative Patent Classification Codes and International Patent Classification codes for 10s of thousands of patent documents that contain references to a range of farm animals (cows, pigs, sheep etc.). The dots are CPC/IPC codes describing areas of technology. The clusters show tightly linked documents that share the same codes that can then be described as ‘modules’ or clusters. The authors of the landscape report on animal genetic resources used this network as an exploratory tool to extract and examine the documents in the cluster for relevance. Distant clusters, such as Cooking equipment and Animal Husbandry (housing of animals etc.), were discarded. The authors later used network mapping to explore and classify the individual clusters.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/animals.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Visualising networks of key terms in patent documents and their relationships with other terms as part of the exploration and refinement of analysis. In this case the authors have clustered similar terms onto each other using word stemming to understand the contents of the new breeds of animals cluster above in relation to animal breeding.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/breeds_cluster.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As such, network visualization can be seen as both an exploratory tool for defining the object of interest and as the end result (e.g. a defined network of actors in a specific area).&lt;/p&gt;
&lt;div id=&#34;gephi&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://gephi.github.io&#34;&gt;Gephi&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Gephi is Java based open source network generating software. It can cope with large datasets (depending on your computer) to produce powerful visualizations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/Gephi.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One issue that may be encountered, particularly by Mac users, is problems with installing the right Java version. This problem appears to have been solved with the latest release Version 0.9.&lt;/p&gt;
&lt;p&gt;To create &lt;code&gt;.gexf&lt;/code&gt; network files in R try the &lt;a href=&#34;http://cran.r-project.org/web/packages/rgexf/index.html&#34;&gt;gexf&lt;/a&gt; package and example code and source code &lt;a href=&#34;https://bitbucket.org/gvegayon/rgexf/wiki/Home&#34;&gt;here&lt;/a&gt;. In Python try the &lt;a href=&#34;https://github.com/paulgirard/pygexf&#34;&gt;pygexf&lt;/a&gt; library and for anything else such as Java, Javascript C++ and Perl see &lt;a href=&#34;http://gexf.net/format/&#34;&gt;gexf.net&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;nodexl&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://nodexl.codeplex.com&#34;&gt;NodeXL&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;For die hard Excel users, &lt;a href=&#34;http://nodexl.codeplex.com&#34;&gt;NodeXL&lt;/a&gt; is a plug in that can be used to visualise networks. It works well.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/NodeXL.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cytoscape&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.cytoscape.org/what_is_cytoscape.html&#34;&gt;Cytoscape&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://www.cytoscape.org/what_is_cytoscape.html&#34;&gt;Cytoscape&lt;/a&gt; is another network visualization programme. It was originally designed for the visualization of biological networks and interactions but, as with so many other bioinformatics tools, can be applied to a wider range of visualization tasks.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/cytoscape.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We mainly have experience with using Gephi (above) but Cytoscape is well worth exploring. Cytoscape works with Windows, Mac and Linux.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pajek&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://mrvar.fdv.uni-lj.si/pajek/&#34;&gt;Pajek&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This is one of the oldest and most established of the free network tools and is Windows only (or run via a Virtual Machine). It is widely used in bibliometrics and can handle large datasets. It is a matter of personal preference but tools such as Gephi may be superseding Pajek because they are more flexible. However, Pajek may possibly have an edge in precision, ease of reproducibility and the important ability to easily save work that Gephi can lack as a Beta programme.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/Pajek.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Data can also be exported from Pajek to Gephi for those who prefer the look and feel of Gephi.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;vos-viewer&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.vosviewer.com/Home&#34;&gt;VOS Viewer&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;VOS Viewer from Leiden University is similar to Gephi and Cytoscape but also presents different types of landscape (as opposed to pure network node and edge visuals). The latest version can also speak to both Gephi and Cytoscape. It is worth testing for different visual display options and its ability to handle Web of Science and Scopus bibliographic data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/VOSviewer.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hive-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.hiveplot.net&#34;&gt;Hive Plots&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We are not entirely sure what to make of Hive Plots. However, we have a lot of sympathy with the aims. The aim of network visualization should be to clarify the complex… not “wow, look, I made something that looks like spaghetti” (although that is normally part of the process). So, we find Hive Plots developed by Martin Krzywinski at the Genome Sciences Center at the BC Cancer Agency interesting.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/HivePlots.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Designed for large networks there are packages for Hive plots in Python through &lt;a href=&#34;https://pypi.python.org/pypi/pyveplot/&#34;&gt;pyveplot&lt;/a&gt; and &lt;a href=&#34;https://github.com/ericmjl/hiveplot&#34;&gt;hiveplot&lt;/a&gt;. For R there is &lt;a href=&#34;http://academic.depauw.edu/~hanson/HiveR/HiveR.html&#34;&gt;HiveR&lt;/a&gt; with documentation available on CRAN &lt;a href=&#34;http://cran.r-project.org/web/packages/HiveR/index.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In closing this discussion of network mapping tools it is also important to note that network visualizations need to be exported as images. This means that there are additional requirements for image handling software. Open source tools such as &lt;a href=&#34;http://www.gimp.org&#34;&gt;The GNU Image Manipulation Program or GIMP&lt;/a&gt; are perfectly adequate and easy to use for image handling. Where using labels particular attention should be paid to outlining the text to ensure consistency of display across different computers. These kinds of tasks can be performed in tools such as GIMP.&lt;/p&gt;
&lt;p&gt;For other sources of network visualization see &lt;a href=&#34;http://opendata-tools.org/en/visualization/&#34;&gt;FlowingData&lt;/a&gt;. Also try &lt;a href=&#34;http://www.visualcomplexity.com/vc/&#34;&gt;Visual Complexity&lt;/a&gt; and &lt;a href=&#34;http://www.visualisingdata.com/index.php/resources/&#34;&gt;visualising data&lt;/a&gt; for sources of inspiration.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;infographics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Infographics&lt;/h2&gt;
&lt;p&gt;Infographics are increasingly part of the communication toolkit. They are particularly useful for communicating the results of research in an easily digestible yet informative form. The WIPO Patent Landscape Project has developed a range of infographics with the latest being for the &lt;a href=&#34;http://www.wipo.int/export/sites/www/patentscope/en/programs/patent_landscapes/reports/documents/animal_genetics_infographic.pdf&#34;&gt;Animal Genetic Resoutces Patent Landscape Report&lt;/a&gt; and &lt;a href=&#34;http://www.wipo.int/patentscope/en/programs/patent_landscapes/reports/assistive_devices.html&#34;&gt;Assistive Devices and Technologies&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The growing popularity of infographics has witnessed the rise of a range of online services including free services. In more cases these will have limitations such as the number of icons etc. that can be used in a graphic. However, as a growing sector that may change. Here are a few services with free options that may be worth exploring.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;http://piktochart.com&#34;&gt;Piktochart.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.canva.com/create/infographics/&#34;&gt;Canva.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://infogr.am/pricing&#34;&gt;Infogr.am&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.visme.co&#34;&gt;Visme&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.easel.ly/create/#&#34;&gt;Easel.ly&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Websites such as &lt;a href=&#34;http://www.coolinfographics.com&#34;&gt;Cool Infographics&lt;/a&gt; can be useful for finding additional sources, exploring what is hot in the infographics world and tutorials. Tools such as Apple Keynote, Open Office Presentation or Powerpoint can be very useful for wire framing (sketching out) infographics to see what works.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;geographical-mapping&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Geographical Mapping&lt;/h2&gt;
&lt;p&gt;In addition to the ubiquitous &lt;a href=&#34;https://www.google.com/maps/&#34;&gt;Google Maps&lt;/a&gt; or well known &lt;a href=&#34;https://www.google.co.uk/intl/en_uk/earth/&#34;&gt;Google Earth&lt;/a&gt; we think it is well worth taking a close look at other services.&lt;/p&gt;
&lt;div id=&#34;openstreetmap&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.openstreetmap.org/#map=5/51.500/-0.100&#34;&gt;OpenStreetMap&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Rightly popular.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/OpenStreetMap.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;leaflet&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://leafletjs.com&#34;&gt;Leaflet&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A very popular Open Source JavaScript library for interactive maps&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/Leaflet.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Accessible through an API. R users could use the &lt;code&gt;leafletr&lt;/code&gt; package with tutorials and walk throughs available at &lt;a href=&#34;http://www.r-bloggers.com/?s=leaflet&#34;&gt;R-bloggers&lt;/a&gt;. For Python users try &lt;code&gt;folium&lt;/code&gt; &lt;a href=&#34;https://github.com/python-visualization/folium&#34;&gt;here&lt;/a&gt; or &lt;a href=&#34;https://pypi.python.org/pypi/folium&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tableau-public-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://public.tableau.com/s/&#34;&gt;Tableau Public&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Already mentioned above. Tableau Public uses Open Street Map to create a powerful combination of interactive graphs that can be linked to maps geocoded at various levels of detail. See an example &lt;a href=&#34;https://public.tableau.com/profile/poldham#!/vizhome/SyntheticBiologyScientificLandscape/SyntheticBiologyTrends&#34;&gt;here&lt;/a&gt; for the scientific literature on synthetic biology.&lt;/p&gt;
&lt;p&gt;Tableau Public is probably the easiest way to get started with creating your own maps with patent data. The map below was produced using custom geocoding and connecting the data to publication country and the titles of scientific publications. &lt;img src=&#34;https://www.pauloldham.net/images/overview/tableaumap.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To view the interactive version try this &lt;a href=&#34;http://public.tableau.com/profile/poldham#!/vizhome/SyntheticBiologyScientificLandscape/SyntheticBiologyTrends&#34;&gt;page&lt;/a&gt;. It is possible to easily create simple yet effective maps in Tableau Public.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;qgis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.qgis.org/en/site/&#34;&gt;QGIS&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A very popular and sophisticated software package running on all major platforms.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/QGIS.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Using QGIS Oldham and Hall et al mapped the worldwide geographical locations of marine scientific research and patent documents making reference to deep sea locations such as hydrothermal vents (see &lt;a href=&#34;https://www.researchgate.net/publication/273139809_Valuing_the_Deep_Marine_Genetic_Resources_in_Areas_Beyond_National_Jurisdiction&#34;&gt;Valuing the Deep&lt;/a&gt;). This is a low resolution QGIS map of scientific research locations in the oceans based on text mining of the scientific literature.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/valuing1.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;geonames.org.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.geonames.org&#34;&gt;Geonames.org&lt;/a&gt;.&lt;/h3&gt;
&lt;p&gt;Not a mapping programme, instead geonames is an incredibly useful database of georeferenced place names from around the world along with a RESTful &lt;a href=&#34;http://www.geonames.org/export/web-services.html&#34;&gt;web service&lt;/a&gt;. If you need to obtain the georeferenced data for a large number of places then this should be your first stop. geonames can be accessed in R using the &lt;a href=&#34;https://cran.r-project.org/web/packages/geonames/geonames.pdf&#34;&gt;&lt;code&gt;geonames&lt;/code&gt;&lt;/a&gt; along with client libraries for Python, Ruby, PHP and others.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/GeoNames.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;icharts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://icharts.net/product/web-data-visualization&#34;&gt;iCharts&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A Free and Premium data visualization service:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/icharts.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;openlayers3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://openlayers.org&#34;&gt;OpenLayers3&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;OpenLayers3 allows you to add your own layers to OpenStreetMap and other data sources and may come in very useful if you are seeking to create your own layers. It also has an API and tutorials.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/OpenLayer.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cartodb&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://cartodb.com/gallery/&#34;&gt;CartoDB&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Free and paid accounts with a nice looking gallery of examples&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/CartoDB.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;d3.js&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://d3js.org&#34;&gt;d3.js&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A javascript library for manipulating data and documents. This is the library behind some of the other frequently mentioned visualization tools on the web.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/D3.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;highcharts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.highcharts.com&#34;&gt;Highcharts&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Free for non-commercial use with a variety of pricing plans.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/highcharts.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;datawrapper&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://datawrapper.de&#34;&gt;Datawrapper&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;An entirely open source service for creating charts and maps with your data. Widely used by big newspapers and so the graphics will seem familiar. Either create an account or fork the source from Github &lt;a href=&#34;https://github.com/datawrapper/datawrapper&#34;&gt;here&lt;/a&gt;. There is a free option and a set of pricing plans.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/Datawrapper.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plotly&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://plot.ly/feed/&#34;&gt;Plotly&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Free and with an API with clients for R, Python and Matlab, Plotly is an increasingly popular free service that uses the D3.js library mentioned above with the enterprise version used by companies such as Google. Plotly is increasingly popular and has a range of API clients for Python, Matlab, R, Node.js, and Excel. Plotly’s ease of use and access from a range of environments are big reasons for its growing success.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/Plotly.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;text-mining&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Text Mining&lt;/h2&gt;
&lt;p&gt;There are a lot of text mining tools out there and many of them are free or open source. Here are some that we have come across.&lt;/p&gt;
&lt;div id=&#34;jigsaw-visual-analytics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.cc.gatech.edu/gvu/ii/jigsaw/&#34;&gt;Jigsaw Visual Analytics&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;For exploring and understanding document collections.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/Jigsaw.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weka&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.cs.waikato.ac.nz/ml/weka/&#34;&gt;Weka&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Java based text mining software.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/Weka.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;word-trees&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Word Trees&lt;/h3&gt;
&lt;p&gt;Word trees can be used for detailed investigation of texts such as claims trees. The first two examples are taken from the &lt;a href=&#34;http://www.wipo.int/edocs/pubdocs/en/wipo_pub_946.pdf&#34;&gt;WIPO Guidelines for Preparing Patent Landscape Reports&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;google-word-trees-on-the-google-developers-site-provides-instructions-for-generating-word-trees-using-javascript.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://developers.google.com/chart/interactive/docs/gallery/wordtree&#34;&gt;Google Word Trees&lt;/a&gt; on the Google Developers site provides instructions for generating word trees using Javascript.&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/Word-Trees.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;and &lt;a href=&#34;https://www.jasondavies.com/wordtree/&#34;&gt;Jason Davies&lt;/a&gt; tree creator.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/Word-Tree-Davies.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kh-coder-free-software-allowing-quantitative-content-analysistext-mining.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://sourceforge.net/projects/khc/?source=directory&#34;&gt;KH Coder&lt;/a&gt;: free software allowing quantitative content analysis/text mining.&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/KHCoder.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-and-the-tm-package&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R and the &lt;code&gt;tm&lt;/code&gt; package&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;tm&lt;/code&gt; package in R (e.g. using RStudio) provides access to a range of text mining tools. For an introduction from the package developers see &lt;a href=&#34;http://cran.r-project.org/web/packages/tm/vignettes/tm.pdf&#34;&gt;here&lt;/a&gt;. A number of very useful tutorials are also available for text mining on &lt;a href=&#34;http://www.r-bloggers.com/?s=text+mining&#34;&gt;R-bloggers&lt;/a&gt;. For a step by step approach see &lt;a href=&#34;http://onepager.togaware.com/TextMiningO.pdf&#34;&gt;Graham Williams (2014) Hands-On Data Science with R Text Mining&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For a recent overview of text mining tools in R see &lt;a href=&#34;http://cran.r-project.org/web/views/NaturalLanguageProcessing.html&#34;&gt;Fridolin Wild’s (2014) CRAN Task View: Natural Language Processing&lt;/a&gt; listing the various packages and their uses.&lt;/p&gt;
&lt;p&gt;Note that many text mining packages in general focus on generating words. For non-academic purposes this is not very useful. Patent analysis will typically focus on extracting and analysing &lt;code&gt;phrases&lt;/code&gt; (ngrams). Therefore look for tools that will extract phrases and allow them to be interrogated in depth.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;python-and-text-mining&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Python and Text Mining&lt;/h3&gt;
&lt;p&gt;There are quite a few resources available on text mining using Python. Note that Python may well be ahead of R in terms of text mining resources (until we are proven wrong). However, note that Python and R are increasingly used together to exploit their different strengths. Here are a few resources to help you get started.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-natural-language-toolkitntlk&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.nltk.org&#34;&gt;The Natural Language Toolkit(NTLK)&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;NTLK appears to be the leading package and covers almost all major needs. The accompanying book &lt;a href=&#34;http://shop.oreilly.com/product/9780596516499.do&#34;&gt;Natural Language Processing with Python&lt;/a&gt; may also be worth considering. The &lt;a href=&#34;http://www.christianpeccei.com/textmining/&#34;&gt;Python Textmining Package&lt;/a&gt; is simpler than the giant NTLK package but may suit your needs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/overview/ntlk.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This &lt;a href=&#34;http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk&#34;&gt;detailed tutorial&lt;/a&gt; may be helpful for those wanting to get started with the NTLK package in Python.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-text-mining-resources&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other text mining resources&lt;/h3&gt;
&lt;p&gt;For a wider range of text mining options see this predictive analytics article on the &lt;a href=&#34;http://www.predictiveanalyticstoday.com/top-free-software-for-text-analysis-text-mining-text-analytics/&#34;&gt;top 20 free text mining software tools&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For other free text mining tools try some of the corpus linguistics websites such as &lt;a href=&#34;http://linguistlist.org:8888/sp/SearchWRListing-action.cfm?subclassid=7223&amp;amp;SearchType=LF&amp;amp;WRTypeID=2&#34;&gt;The Linguist List&lt;/a&gt;, this &lt;a href=&#34;http://www.uow.edu.au/~dlee/software.htm&#34;&gt;list&lt;/a&gt;, or this &lt;a href=&#34;http://ucrel.lancs.ac.uk/tools.html&#34;&gt;list&lt;/a&gt;. Bear in mind that most of these tools are designed with linguists in mind and quite a number may be showing their age. However, even simple concordancing tools, such as &lt;a href=&#34;http://www.laurenceanthony.net/software/antconc/&#34;&gt;AntConc&lt;/a&gt; can play an important role in filtering large numbers of documents to extract useful information.&lt;/p&gt;
&lt;p&gt;Some analysis tools such as &lt;a href=&#34;https://www.thevantagepoint.com&#34;&gt;VantagePoint from Search Technology Inc.&lt;/a&gt; have been especially developed and adapted for processing patent data and are available in a subsidised version for students from the &lt;a href=&#34;http://vpinstitute.org/wordpress/vp-marketplace/&#34;&gt;vpinstitute&lt;/a&gt;. There are also a number of qualitative data analysis software tools that can be applied to patent analysis such as &lt;a href=&#34;http://www.maxqda.com&#34;&gt;MAXQDA&lt;/a&gt;, &lt;a href=&#34;http://www.qsrinternational.com/products_nvivo.aspx&#34;&gt;NVivo&lt;/a&gt;, &lt;a href=&#34;http://atlasti.com&#34;&gt;Atlas TI&lt;/a&gt; and &lt;a href=&#34;http://provalisresearch.com/products/qualitative-data-analysis-software/&#34;&gt;QDA Miner&lt;/a&gt;. However, with the exception of &lt;a href=&#34;http://provalisresearch.com/products/qualitative-data-analysis-software/freeware/&#34;&gt;QDA Miner Lite&lt;/a&gt; (Windows only), while they offer free trials they do not fall into the category of free or open source software that is our focus.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round Up&lt;/h2&gt;
&lt;p&gt;In this chapter we have covered some of the major free and open source tools that are available for patent analysis. These are not patent specific tools but can be readily adapted to patent analysis. With the exception of cleaning of patent applicant and inventor names and concatenated fields, patent data is very well suited for visualization and network mapping. The availability of country level data, address fields and place names in texts also means that patent data can be readily used for geographical mapping.&lt;/p&gt;
&lt;p&gt;In practice, it is important to identify a set of tools that work best for you and the type of patent analysis tasks that work best for you.&lt;/p&gt;
&lt;p&gt;It is also important to emphasise that in practice you may use a mixture of paid for tools and free tools. For example, the recent &lt;a href=&#34;http://www.wipo.int/patentscope/en/programs/patent_landscapes/reports/animal_gr.html&#34;&gt;WIPO Patent Landscape for Animal Genetic Resources&lt;/a&gt; involved the use of GNU Parallel and Map Reduce for large scale text mining of 11 million whole text patents using pattern matching in Ruby, combined with the use of PATSTAT for statistics, Thomson Innovation and VantagePoint for validation, and Tableau and Gephi for visualization. In short, it is possible to perform almost all patent analysis tasks, using free tools, but in practice a mixed ecosystem of open source and commercial tools may produce the best workflow for the tasks you perform. As such, it is important to think about the tools that are needed and where they support and strengthen existing analysis workflows.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-checklist&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Checklist&lt;/h2&gt;
&lt;p&gt;If moving into open source software for the first time it may be useful to develop a list of basic questions to assess whether they a tool or a set of tools will meet your particular needs. The following list is not meant to be definitive or exhaustive but aims to encourage thinking about your particular requirements.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Does this tool make sense? That is, is it immediately clear what the purpose of a tool is? If the answer is yes, this is a good sign. If the answer is no, the tool may be too specialised for your particular needs or the creators may be struggling with clearly expressing what the tool is trying to do (a bad sign).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Do you understand the language the tool is written in? Is it a problem if you don’t (see below)? Is it worth training someone in this language? Are there free or affordable courses available?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is the source code open or proprietary and what are the terms and conditions of the open source licence? When using open source or free software it is important to be clear what the precise provisions of the licence mean. For example, are you required to make any modifications to the source code available to others on exactly the same terms as the original licence? If you are working with source code this is an important IP question. If you are not working at the source code level this may not be an issue, but it always always makes sense to understand the open source licence.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Who owns the data? If you upload data to a web based service, who owns the data once it is uploaded and who else may have access to it and under what conditions? These questions are particularly pertinent where the data is commercially relevant.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What does free actually mean? Free versions are often a lead in to premium services (hence the term &lt;code&gt;freemium&lt;/code&gt;). This transition is a key feature of open source business models. In some cases free may be highly restricted in terms of the amount of data that can be processed, saved or exported. In other cases, no restrictions on use of the tool are imposed. However, knowledge about the use of the tool may be the real premium or cost factor, particularly if you come to depend on that tool. Be prepared for this.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What other companies (or patent offices) are using this tool? This can be an indicator of confidence and also provides examples of concrete use cases.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is the tool well supported with documentation and tutorials? This is an indicator of maturity and ‘buy in’ by a community of developers and users.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How large is the community of users and are they active in creating forums and blogs etc. to support the wider community of users?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is this a one function tool or a multi-purpose tool? That is, will this tool cover almost all needs or is it a specific &lt;code&gt;good to have&lt;/code&gt; component in a toolkit. In some cases a tool that does one thing very well is a real asset where other tools fall down because they try to do too many things and do them badly. Of the tools listed above, R and Python (possibly in combination) come closest to tools that could be used for a complete patent analysis workflow from data acquisition right through to visualization. In practice, most patent analysis tool kits will consist of both general and specific tools.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Can I break this tool? It is always a very good idea to work out where the limitations of software lie so that you are not taken by surprise later when trying to do something that is mission critical. In particular, software may claim to perform particular tasks, such as handling thousands or millions of records, but do them very badly, if at all. By pushing a tool past its limits it is possible to determine where the limits are and how to get the best out of it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is the tool proportionate to my needs? There has recently been a great deal of excitement about &lt;code&gt;Big Data&lt;/code&gt; and the use of &lt;a href=&#34;https://hadoop.apache.org&#34;&gt;Hadoop&lt;/a&gt; for dealing with large volumes of data using distributed computing. While Hadoop is open source, so anyone can use it, its adoption would generally be disproportionate for the needs of most patent analysis except where dealing with almost the entire corpus of global patent documents, large volumes of literature and considerable quantities of scientific data. By way of illustration, as noted above, the WIPO Animal Genetic Resources Landscape report used GNU Parallel to process 11 million patent records. The decision to use GNU Parallel was partly made on the basis that Hadoop would have been complicated to implement and overkill for the particular use case. In short, it is worth carefully considering whether a tool is both appropriate and proportionate to the task at hand.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, the golden rule for adopting any tool for patent analysis can be expressed in very simple terms. Does this work for me?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you have any suggestions for free or open source tools that we should include in the manual please feel welcome to add a comment to the electronic version of this chapter.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;credits&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Credits&lt;/h2&gt;
&lt;p&gt;The development of the list of open source tools benefited from the following articles.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;http://www.creativebloq.com/design-tools/data-visualization-712402&#34;&gt;Creative Bloq 11/11/2014 The 37 best tools for data visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://thenextweb.com/dd/2015/04/21/the-14-best-data-visualization-tools/&#34;&gt;Nismith Sharma 2015 The 14 best data visualization tools. TNW News&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Network Visualisation with Gephi</title>
      <link>https://www.pauloldham.net/gephi_patent_network/</link>
      <pubDate>Sun, 17 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://www.pauloldham.net/gephi_patent_network/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;!---update dataset links---&gt;
&lt;p&gt;This article focuses on visualising patent data in networks using the open source software &lt;a href=&#34;http://gephi.github.io&#34;&gt;Gephi&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Gephi is one of a growing number of free network analysis and visualisation tools with others including &lt;a href=&#34;http://www.cytoscape.org&#34;&gt;Cytoscape&lt;/a&gt;, &lt;a href=&#34;http://tulip.labri.fr/TulipDrupal/&#34;&gt;Tulip&lt;/a&gt;, &lt;a href=&#34;http://www.graphviz.org&#34;&gt;GraphViz&lt;/a&gt;, &lt;a href=&#34;http://mrvar.fdv.uni-lj.si/pajek/&#34;&gt;Pajek&lt;/a&gt; for Windows, and &lt;a href=&#34;http://www.vosviewer.com/Home&#34;&gt;VOSviewer&lt;/a&gt; to name but a few. In addition, network visualisation packages are available for R and Python. We have chosen to focus on Gephi because it is a good all round network visualisation tool that is quite easy to use and to learn.&lt;/p&gt;
&lt;p&gt;This article is now a chapter in the WIPO Manual on Open Source Patent Analytics. You can read the chapter in electronic book format &lt;a href=&#34;https://wipo-analytics.github.io/gephi-1.html&#34;&gt;here&lt;/a&gt; and find all the materials including presentations at the &lt;a href=&#34;https://github.com/wipo-analytics&#34;&gt;WIPO Analytics Github homepage&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/gephi_front.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this article we will focus on creating a simple network visualisation of the relationship between patent applicants (assignees). However, network visualisation can be used to visualise a range of fields and relationships, such as inventors, key words, IPC and CPC codes, and citations among other options.&lt;/p&gt;
&lt;p&gt;For this chapter we will use a dataset on drones from the &lt;a href=&#34;https://www.lens.org/lens/search?q=%22drone%22+OR+%22drones%22&amp;amp;predicate=%26%26&amp;amp;l=en&#34;&gt;Lens patent database&lt;/a&gt;. The dataset consists of 5884 patent documents containing the terms “drone or drones” in the full text deduplicated to individual families from the full publication set. The dataset has been extensively cleaned in Vantage Point by separating out applicant and inventor names and then using fuzzy logic matching to clean up names. Very very similar results can be achieved using Open Refine as described in Chapter 9 of this Manual.&lt;/p&gt;
&lt;p&gt;The dataset can be downloaded from Github in a zip file to unzip &lt;a href=&#34;https://github.com/wipo-analytics/drones_data/raw/master/use_me/gephi/gephi_drones_fulltext_cleaned_5884.csv.zip&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-gephi&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Installing Gephi&lt;/h2&gt;
&lt;p&gt;You should install gephi 9.1 (the latest release) rather than an earlier version. Note that any later updates may not contain the key functionality that is needed below (as it takes a while for some of the plugins and features to catch up).&lt;/p&gt;
&lt;p&gt;To install for your operating system follow these &lt;a href=&#34;https://gephi.org/users/download/&#34;&gt;instructions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;After you have finished this chapter you may want to follow the &lt;a href=&#34;https://gephi.github.io/tutorials/gephi-tutorial-quick_start.pdf&#34;&gt;Quick start guide&lt;/a&gt; although we will cover those topics in the article. The &lt;a href=&#34;http://gephi.github.io/users/&#34;&gt;Learn section&lt;/a&gt; of the website provides additional tutorials.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;opening-gephi-and-installing-plugins&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Opening Gephi and Installing Plugins&lt;/h2&gt;
&lt;p&gt;When you have installed Gephi, open it and you should see the following welcome screen.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/welcome.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Before we do anything else, we need to install a plugin developed by &lt;a href=&#34;http://www.em-lyon.com/en/faculty-research-education/faculty-research/international-business-school-professors/Permanent-Professors/Clement-LEVALLOIS&#34;&gt;Clement Levallois&lt;/a&gt; to convert Excel and csv files into gephi network files. To install the plugin select the &lt;code&gt;Tools&lt;/code&gt; menu in the menu bar and then &lt;code&gt;Plugins&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/plugin.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You will see a pop up menu for the plugins. At this point you may want to press &lt;code&gt;Reload Catalog&lt;/code&gt; to make sure everything is loaded. Then head to &lt;code&gt;Available Plugins&lt;/code&gt;. Click on &lt;code&gt;name&lt;/code&gt; to sort them alphabetically. You now want to look for a plugin called &lt;code&gt;Convert Excel and csv files to networks&lt;/code&gt;. Select the check box, press &lt;code&gt;Install&lt;/code&gt; and follow through the menus. Just keep pressing at the prompts and then you will need to restart at the end.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/install.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You will need to restart Gephi for it to take effect but if you return to the Plugins menu and then choose the installed tab you should see this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/result.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You are good to go. While you are there you may want to check out the other plugins to get an idea of what is available. For more on the conversion plugin see this description &lt;a href=&#34;https://marketplace.gephi.org/plugin/excel-csv-converter-to-network/&#34;&gt;Excel/csv converter to network plugin&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;importing-a-file-to-gephi-with-the-converter-plugin&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Importing a file to Gephi with the converter plugin&lt;/h2&gt;
&lt;p&gt;We will concentrate on using the &lt;code&gt;drones&lt;/code&gt; patent dataset in the zipped .csv version &lt;a href=&#34;https://github.com/wipo-analytics/drones_data/raw/master/use_me/gephi/gephi_drones_fulltext_cleaned_5884.csv.zip&#34;&gt;here&lt;/a&gt; and don’t forget to unzip the file. While Gephi works with .csv files, the import plugin includes a timeline option that only works with Excel. For that reason we will use the Excel version.&lt;/p&gt;
&lt;div id=&#34;step-1.-open-gephi-and-choose-file-import&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 1. Open Gephi and Choose File &amp;gt; Import&lt;/h3&gt;
&lt;p&gt;For this to work we need to use the &lt;code&gt;Import&lt;/code&gt; function under the File menu.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/import.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You should now see a menu like that below. Make sure that you choose the co-occurrence option.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/wizard.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next you will be asked to select the file to use. We will download and then unzip the &lt;a href=&#34;https://github.com/wipo-analytics/drones_data/raw/master/use_me/gephi/gephi_drones_fulltext_cleaned_5884.csv.zip&#34;&gt;gephi_drones_fulltext_cleaned_5884.csv&lt;/a&gt; file that is located on the WIPO Analytics website on Git Hub.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/select.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When you have chosen &lt;code&gt;Data importer(co-occurrences)&lt;/code&gt; then choose &lt;code&gt;Next&lt;/code&gt;. Make sure the column headers stays selected (unless using your own data). You will then need to choose a delimiter. In this case it is a comma but in other cases it may be a semicolon or a tab.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/figures_gephi/fig5_idelimiter.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We now need to choose the agents, that is the actors or objects that we want to create a network map with. We will use &lt;code&gt;patent_assignees_cleaned&lt;/code&gt; as this is a relatively small set. We will choose the same field in the two boxes because we are interested in co-occurrence analysis.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/applicants.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the next step we need to specify the delimiter for splitting the contents of the &lt;code&gt;applicants_use_me&lt;/code&gt; column. In all the fields it is a semicolon so let’s choose that. Note that if you are doing this with raw Lens data that you have not previously cleaned the Lens delimited is a double semi-colon (which is not helpful) and will need to be replaced prior to import.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/delimiter.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will then be asked if we want a dynamic network. This presently only works with Excel files and even then it does not always work well. We will leave this blank as we are using a .csv file. Note that if we were using an Excel file the choices we would use would normally be the publication year or publication date or the priority year or date for patent data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/dynamic.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The next menu provides us with a list of options. Unfortunately, with one exception, it is not entirely clear what the consequences of these choices are so experimentation may be needed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/figures_gephi/fig8_options.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Choice1. Create links between &lt;code&gt;applicants_use_me&lt;/code&gt;. Choice 2. Remove duplicates. We don’t need that as we know that they are unique. Choice 3. Remove self-loops. Generally we do want this (otherwise known as removing the diagonal to prevent actors counting onto themselves - this will produce a large black hoop or handle for a self-loop in Gephi).&lt;/p&gt;
&lt;p&gt;We will choose to create the links and to remove the self loops.&lt;/p&gt;
&lt;p&gt;Next we will see a create network screen setting out our choices. &lt;img src=&#34;https://www.pauloldham.net/images/gephi9/create.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Press Finish&lt;/p&gt;
&lt;p&gt;Next we will see an import screen.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/warning.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is quite common to see warning messages on this screen.&lt;/p&gt;
&lt;p&gt;In this case some of the applicants cells in the worksheet are blank because no data is available. When you see warning messages it is a good idea to check the underlying file to make sure that you understand the nature of the warning.&lt;/p&gt;
&lt;p&gt;A second common warning with dynamic networks is that the year field is not correctly formatted. In that case, check that the format of the date/year field is what gephi is expecting in the underlying data. You can review the data in the data laboratory.&lt;/p&gt;
&lt;p&gt;Note that the import screen also provides options on the type of graph. Normally for networks of authors, inventors and actors leave this as an undirected (unordered) network. Undirected is the basic default for patent data and scientific literature. We will also see the number of nodes (dots) and edges (connections). It is important to keep an eye on these values. If the nodes are much lower than you expect then it is useful to go back to your data and inspect for problems such as concatenation of cells and so on.&lt;/p&gt;
&lt;p&gt;Click OK. You should now see a raw network that looks like this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/network.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that we can see the number of Nodes and Edges in the top right. If we switch to the top left, we will see three tabs, for &lt;code&gt;Overview&lt;/code&gt;, &lt;code&gt;Data Laboratory&lt;/code&gt; and &lt;code&gt;Preview&lt;/code&gt;. Choose &lt;code&gt;Data Laboratory&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In the Data laboratory we can see the ID, Label, type of field and the frequency (the count of the number of times the name appears). Note that these fields are editable by clicking inside the entry and can also be grouped (for example where a variant of the same name has been missed during the name cleaning process in a tool such as Open Refine).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/laboratory.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In some cases you may have filled any blank cells in the dataset with NA (for Not Available). If this is the case NA will show up as a node on the network. You can address this type of issue in the Data Laboratory by right clicking on the NA value and then Delete. Note also that you can always exclude or combine nodes after you have laid out the network by editing in the Data Laboratory.&lt;/p&gt;
&lt;p&gt;The second part of the Data Laboratory is the Edges under Data Table in the Data Laboratory. The edges table involves a source and a target, where the source is the source node and the target is another node where there is a link between the nodes. We can see the edges table sorted alphabetically (click the source heading to sort) where the value in weight is the number of shared records.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/edges.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again, note that it is possible to export the edges set and import a set. Also note the menus at the bottom of the screen which allow column values to be copied over. This can be useful where the label value is not populated meaning that a name will not display on the node when the graph is laid out.&lt;/p&gt;
&lt;p&gt;Most of the time we can simply proceed with laying out the network without paying much attention to the data laboratory. However, it is important to become familiar with the data laboratory to avoid unexpected problems or to spot corruption in the data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sizing-and-colouring-nodes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sizing and Colouring Nodes&lt;/h2&gt;
&lt;p&gt;When we look at the &lt;code&gt;Overview&lt;/code&gt; screen we have quite a wide range of options. We will start on the upper right with the Statistics panel.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/statistics.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Run&lt;/code&gt; buttons will calculate a range of statistics on the network. Probably the two most useful are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Modularity Class. This algorithm iterates over the connections (edges) and allocates the nodes to communities or clusters based on the strength of the connections. This algorithm is explained in detail in this article &lt;a href=&#34;http://arxiv.org/abs/0803.0476&#34;&gt;Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, Etienne Lefebvre, Fast unfolding of communities in large networks, in Journal of Statistical Mechanics: Theory and Experiment 2008 (10), P1000&lt;/a&gt;. The ability to detect communities in networks based on the strength of connections is a powerful tool in patent analytics.&lt;/li&gt;
&lt;li&gt;Network Diameter. This calculates two measures of &lt;code&gt;betweeness&lt;/code&gt;, that is &lt;code&gt;betweeness centrality&lt;/code&gt; (how often a node appears on the shortest path between nodes) and centrality(the average distance from a starting node to other nodes in the network). Network Diameter also calculates Eccentricity which is the distance between a given node and the farthest node from it in the network. For background on this see the Wikipedia entry and also &lt;a href=&#34;http://www.inf.uni-konstanz.de/algo/publications/b-fabc-01.pdf&#34;&gt;Ulrik Brandes, A Faster Algorithm for Betweenness Centrality, in Journal of Mathematical Sociology 25(2):163-177, (2001)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Whereas Modularity Class identifies communities (particularly in large networks), centrality measures examine the position of a node in the graph relative to other nodes. This can be useful for identifying key actors in networks based on the nature of their connections with other actors (rather than simply the number of records).&lt;/p&gt;
&lt;p&gt;If we run Modularity Class as in the figure a pop up message will inform us that there are 246 communities in the network. Given that there are only 362 nodes this suggests a weakly connected network made up of small individual clusters.&lt;/p&gt;
&lt;div id=&#34;filtering-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Filtering the data&lt;/h3&gt;
&lt;p&gt;We have a total of 5,265 nodes which is quite dense. After running the modularity class algorithm above, we will now move over to the Filters tab next to Statistics. Our aim here is to reduce the size of the network&lt;/p&gt;
&lt;p&gt;Move over to the left where it says Ranking and then select the red inverted triangle. Set the largest value to 200 and the smallest to 20 (it is up to you what you choose). Then apply. The network will now change.&lt;/p&gt;
&lt;p&gt;Open the Filters menu and choose Attributes. That will open a set of Folders and we would like to use Range. When the Range folder is open drag frequency into the Queries area below (marked with a red icon and Drag message when empty). Then either drag the range bar until you see a frequency of 5 as the minimum or change the number by clicking on it. Note that as we drag the results the number of Nodes and Edges in the Context above will change. We are looking for a manageable number. In the image below I have set the number to 5.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/filter.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-node-size&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting Node Size&lt;/h3&gt;
&lt;p&gt;Next we want to size the nodes. On the left, look for the Appearance tab and then with Nodes in grey choose the Ranking button. Here the minimum size is set to 20 and the maximum to 150. Note that the default setting is 10 and this is generally too small for easy visibility. Press Apply and you will see the network changes to display the size of nodes based on the frequency. You can always adjust the size of nodes later if you are unhappy with them.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/size.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;colouring-the-nodes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Colouring the Nodes&lt;/h3&gt;
&lt;p&gt;To colour the nodes choose the small palette icon next to the size icon. We now have choices on Unique (simply grey), Partition or Ranking. In this case we will choose Ranking and frequency. Note that a range of colour palettes can be accessed by clicking on the small icon to the right of the colour bar under ranking. When you have found a palette that you like then click Apply.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/colour.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An alternative way of colouring the graph in earlier versions of gephi was to partition on Modularity class. This would colour the nodes as ‘communities’ of closely linked nodes. However, at present in Gephi 9 this option does not appear to be consistently available but may return in a future update.&lt;/p&gt;
&lt;p&gt;There are a range of other options for colouring nodes including a colour plugin that will detect if a column with a colour value is present in the imported data. This can be very useful if you have colour coded categories of data prior to importing to gephi.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;laying-out-the-graph&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Laying out the Graph&lt;/h2&gt;
&lt;p&gt;In the bottom left panel called Layout in the figure above there are a range of network visualisation options with more that can be imported from the plugin menus. Among the most useful are Fruchterman-Reingold, Force Atlas, OpenOrd and Circular with specialist plugins for georeferenced layouts that are well worth experimenting with.&lt;/p&gt;
&lt;p&gt;We will illustrate network layout using Fruchterman-Reingold. The first setting is the area for the graph. The default is 10,000 but we will start with 20,000 because 10,000 tends to be too crunched. The default for the gravity setting is 10. This is intended to stop the nodes dispersing too far but is often too tight when labels are applied. Try changing the setting to 5 (which reduces the gravitational pull). The settings in the different layout options can take some getting used too and it is worthwhile creating a record of useful settings. Gephi does not save your settings so make sure you write down useful settings.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/layout_settings.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We are now good to go. But, before we start take note of two important options for later use.&lt;/p&gt;
&lt;p&gt;The first is the NoOverlap plugin we installed above. This will help us to deal with overlapping nodes after layout. The second is Expansion which will help us to increase the size of a network are to make it easier to see the labels. Note also the Contraction option which will allow us to pull a network back in if it expands too far.&lt;/p&gt;
&lt;p&gt;Now make sure that Fruchterman-Reingold is selected with the settings mentioned above and click &lt;code&gt;Run&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can leave the network to run and the nodes will start to settle. If the network disappears from view (depending on your mouse) try scrolling to zoom out. Our aim is to arrive at a situation where lines only cross through nodes where they are connected. As you become more experienced with layout you may want to assist the nodes with moving into a clear position for a tidier graph.&lt;/p&gt;
&lt;p&gt;You will now have a network that looks something like this (note that 15,000 for the Area may have been enough).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/layout.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that some of the nodes are set very close together. That will affect the ability to label the nodes in a clear way. To address this we first use the NoOverlap function and later we may want to use the Expansion function in the Layout drop down menu items.&lt;/p&gt;
&lt;p&gt;Choose nooverlap from the menu and Run.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/noverlap.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While the difference is very minor in this case we have at least moved the nodes into separate positions. At a later stage you may want to use the Expansion function. This will increase the size of the network and is useful when working with labels.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/noverlap_laidout.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;save-your-work&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Save your work&lt;/h4&gt;
&lt;p&gt;At this stage we will save our work. One feature of Gephi as a Java programme is that there is no undo option. As a result it is a good idea to save work at a point where you are fairly happy with the layout as it stands.&lt;/p&gt;
&lt;p&gt;Go to File and choose Save As and give the file name a &lt;code&gt;.gephi&lt;/code&gt; extension. Do not forget to do this or gephi will not know how to read the file. If all goes well the file will save. On some occasions Java may throw an exception and you will basically have to start again. That is one reason to save work in Gephi regularly because it is a beta programme and subject to the predilections of Java on your computer.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-labels&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding Labels&lt;/h2&gt;
&lt;p&gt;The next step is to add some labels. In the bottom menu bar there are a range of options. What we want is the small grey triangle on the right of this menu bar that will open up a new bar. Click in the triangle and you will see a set of options. Choose labels and then at the far left check the &lt;code&gt;Node&lt;/code&gt; box. We will not see any labels yet.&lt;/p&gt;
&lt;p&gt;To the right is a menu with size. This is set to scaled. To see some labels move the scale slider as far as it will go. We will see labels come into view and a first hint that we will need to do some more work on laying out the graph to make it readable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/labels.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, change size to Node Size, the screen will now fill with text. Go to the scaler and pull it back until there is something more or less readable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/labels2.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At this stage we may need to take a couple of actions.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Where it is clear that our nodes are too close together we will need to run Expansion from the layout menu. As a general rule of thumb you should only need to do this twice at most… but it may depend on your graph.&lt;/li&gt;
&lt;li&gt;If you have very long labels such as Massachusetts Institute of Technology you will probably want to head over to the Observatory and edit the Node Label to something manageable such as MIT. This can make a big difference in cleaning up the labels.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the image below we have used Expansion twice and then manually resize the labels using the slider.&lt;/p&gt;
&lt;p&gt;You will now have something that looks more or less like this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/expanded.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that you can use the slider to the right in the bottom menu to adjust the sizes and you could of course adjust the font. In some cases you may be happy with a rough and ready network rather than the detailed adjustments that are required for a final network graph.&lt;/p&gt;
&lt;p&gt;Note the small camera icon on the left of the bottom menu. Either press that to take a screenshot or hold to bring up a configure menu that will allow you to choose a size.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/screenshot.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you pursuing this option you may also want to adjust the font or the colour and to use the bottom menu to arrive at a result you are happy with for display. In some cases (as we will deal with below) manually moving the nodes will allow you to arrive at a cleaner network for a screenshot.&lt;/p&gt;
&lt;p&gt;Screenshots can be a very useful step in exploring data or sharing data internally. For publication quality graphics you will need to move over to using the Preview Options and engage in the &lt;code&gt;gephi shuffle&lt;/code&gt; to progressively clean up the network for a publication quality graphic.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-preview-options&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using the Preview Options&lt;/h2&gt;
&lt;p&gt;A more involved option for network visualisation is to move over to the Preview tab next to the Data Laboratory.&lt;/p&gt;
&lt;p&gt;The default option uses curved edges. To use this press &lt;code&gt;Refresh&lt;/code&gt;. This is fine but we can’t see any labels. In the presets now try default curved. You can play around with the different settings until you find a version that you like.&lt;/p&gt;
&lt;p&gt;The main issue that we have here is that the labels are too large and the line weigths may also be very heavy.&lt;/p&gt;
&lt;p&gt;To address the line weigth look for and check the rescale weight option under edges.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/rescale_weight.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note here the difference with the visualisation in the Overview. With Gephi what you see is not what you get.&lt;/p&gt;
&lt;p&gt;To arrive at a more readable network the first option is to adjust the size of the font in the &lt;code&gt;Node Labels&lt;/code&gt; panel of the preview settings. Note here that label size is set to be proportional to the font (uncheck that and experiment if you wish). If we stick with proportional font size then we will start smaller and move upwards. For example, if we adjust the font size to 3 then the proportional font size will be reduced. In deciding on the font size an important consideration will be how many nodes you want to be legible to the reader. In this case setting the font size to 3 and this produces a pretty legible network.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/font3.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That is quite an acceptable graph for seeing the larger nodes. However, note that the labels of some of the nodes are overlapping some of the other nodes. This can produce a very cluttered look. The larger the base font size the more cluttered the graph will look and the more adjustment it is likely to need.&lt;/p&gt;
&lt;p&gt;To make adjustments to this network we will use size 3. We will now need to move back and forwards between the Preview and the Overview adjusting the position of the nodes. For very complex graphs it can help to print out the preview to see what you need to adjust. Another sensible way to proceed is to mentally divide the graph into quarters and proceed clockwise quarter by quarter adjusting the nodes as you go. It is a very good idea to save your work at this point and as you go along.&lt;/p&gt;
&lt;p&gt;In the first and second quarter moving clockwise things look good with no overlapping labels. However, some adjustments are needed in the third quarter in the middle of the network where Campanella and Kurs are overlapping. To make the adjustment move to the Overview tab, then select the small hand in the left vertical menu for grab. Locate Campanella and move it out of the way so it is not overlapping. Be gentle. Now go back to Preview and hit Refresh. When doing this quarter by quarter it can be helpful to zoom in in the Overview and in the Preview. For each of the overlapping nodes quarter by quarter make an adjustment periodically checking back by using Refresh in Preview and saving as you go along. Note that the aim is minor adjustments rather than major adjustments to node position (it is also possible to attempt to use Label Adjust in the Layout options but in practice this can distort the network). In the process it is also worth watching out for edges that intersect with nodes where there is no actual link. In those cases adjust the node position by trying to move it to the side of the unrelated edge. Note that this is often not possible with complex graphs and you will need to explain in the text that nodes may intersect with unrelated edges. Also check that any edits to labels do not contain mistakes (such as CATECH rather than CALTECH) and adjust accordingly. Typically long labels cause problems at this point and can be edited down in the Data Laboratory.&lt;/p&gt;
&lt;p&gt;Through a series of minor adjustments in a clockwise direction you should arrive at a final network graph. Expect to spend about 20 minutes on clean up when you are familiar with Gephi depending on the number of nodes. It is worth noting that you will often need to go back to make final adjustments.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/final.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The basic principle here is that each node should have a readable label when you zoom in and that edges should not intersect with unrelated nodes (except if this is unavoidable). In this case we have taken a screen shot of the core of the network.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/zoom.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is quite common when you arrive at a publication quality graphic to suddenly discover a mistake in the network. For example, at the data cleaning stage you may have decided not to group two companies with very similar names. However, at the network visualization stage the network suggests that in practice the two companies are one and the same. In this case check the data and head over to the Data Laboratory to group the nodes to a single node. As this suggests, time spent on data cleaning and data preparation will normally reap dividends in terms of time saved later in the analysis and visualization process.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exporting-from-preview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exporting from Preview&lt;/h2&gt;
&lt;p&gt;At this stage we will want to do a final check and then export the data. Arriving at a publication quality export can in fact be the most time consuming and troublesome aspect of using Gephi. Before going any further save your work.&lt;/p&gt;
&lt;p&gt;When exporting note that what you see on the screen and what you get are not exactly the same thing. The main issues are borders around nodes and the weight of lines in the edges. To adjust for this in the Nodes panel in Preview change the border width to a lower setting or 0 (the option we are choosing here). In the edges panel, if you do not want heavy lines then adjust the thickness or opacity (or both). In this case we have reduced the opacity of the edges to 50 and left the thickness as is. If you change something remember to hit Refresh.&lt;/p&gt;
&lt;p&gt;Next select the export button in the bottom left. We will export to .pdf.&lt;/p&gt;
&lt;p&gt;When you choose Export note that there is an &lt;code&gt;Options&lt;/code&gt; menu for tighter control of the export.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/export.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The defaults are sensible and we will use those. If you are tempted to adjust them note that Gephi does not remember your settings, even when saved, so write them down. In reality the defaults work very well.&lt;/p&gt;
&lt;p&gt;If all goes well you will end up with an image that looks like this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/gephi9/drones.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Because the default size is portrait you will want to crop the image. For publication you will also want to outline the text (to fix the font across system). This can be done with the free GIMP software or paid software such as Adobe Illustrator.&lt;/p&gt;
&lt;p&gt;Congratulations, you have now created your first Gephi network graph.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;resources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Resources&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;http://gephi.github.io/&#34;&gt;Gephi website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gephi/gephi&#34;&gt;Gephi github repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Quick start &lt;a href=&#34;https://gephi.github.io/tutorials/gephi-tutorial-quick_start.pdf&#34;&gt;guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Installation instructions for &lt;a href=&#34;http://gephi.github.io/users/install/&#34;&gt;All platforms&lt;/a&gt;. Gephi 8 suffers from a known issue for Mac users. That is, it uses Java 6 which is not installed by default on Macs. To resolve this you should follow the instructions posted &lt;a href=&#34;http://sumnous.github.io/blog/2014/07/24/gephi-on-mac/&#34;&gt;here&lt;/a&gt; and works very well in most cases. It basically involves downloading a mac version of Java containing Java 6 and then running three or four commands in the Terminal on the mac to configure Gephi. If that doesn’t work try this more &lt;a href=&#34;https://lbartkowski.wordpress.com/2014/11/28/gephi-0-8-2-on-apple-osx-yosemite/&#34;&gt;detailed account&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://marketplace.gephi.org/plugin/excel-csv-converter-to-network/&#34;&gt;Excel/csv converter to network plugin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;For ideas on patent network visualisation you might want to try this article on &lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0034368&#34;&gt;synthetic biology&lt;/a&gt;, this &lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0078737&#34;&gt;article&lt;/a&gt; on species names in patent data, and the use of exploratory network analysis using IPC/CPC co-occurrence analysis in the &lt;a href=&#34;http://www.wipo.int/patentscope/en/programs/patent_landscapes/reports/animal_gr.html&#34;&gt;WIPO Patent Landscape for Animal Genetic Resources&lt;/a&gt;. For more try this &lt;a href=&#34;https://www.google.co.uk/webhp?sourceid=chrome-instant&amp;amp;ion=1&amp;amp;espv=2&amp;amp;ie=UTF-8#q=patent%20network%20analysis&#34;&gt;Google Search&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualising Data with Tableau Public</title>
      <link>https://www.pauloldham.net/tableau-patents/</link>
      <pubDate>Sun, 17 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://www.pauloldham.net/tableau-patents/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this article we will be analysing and visualising patent data using Tableau Public.&lt;/p&gt;
&lt;p&gt;Tableau Public is a free version of Tableau Desktop and provides a very good practical introduction to the use of patent data for analysis and visualisation. In many cases Tableau Public will represent the standard that other open source and free tools will need to meet.&lt;/p&gt;
&lt;p&gt;This is a practical demonstration of the use of Tableau in patent analytics. We have created a set of cleaned patent data tables on &lt;code&gt;pizza patents&lt;/code&gt; using a sample of 10,000 records from WIPO Patentscope that you can download as a .zip file from &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium_clean/pizza_medium_clean.zip?raw=true&#34;&gt;here&lt;/a&gt; to use during the walkthrough. Details of the cleaning process to reach this stage are provided in the codebook that can be viewed &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium_clean/pizza_medium_code_book_15052105.txt&#34;&gt;here&lt;/a&gt;. The &lt;a href=&#34;http://poldham.github.io/openrefine-patent-cleaning/&#34;&gt;Open Refine walkthrough&lt;/a&gt; can be used to generate cleaned files very similar to those used in this walkthrough using your own data. You will not need to clean any data using our training set files.&lt;/p&gt;
&lt;p&gt;This article will take you through the main features of Tableau Public and the types of analysis and visualisation that can be performed using Tableau. In the process you will be creating something very similar to this &lt;a href=&#34;https://public.tableau.com/profile/wipo.open.source.patent.analytics.manual#!/vizhome/pizzapatents/Overview&#34;&gt;workbook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This article is now a chapter in the WIPO Manual on Open Source Patent Analytics. You can read the chapter in electronic book format &lt;a href=&#34;https://wipo-analytics.github.io/tableau-public-2.html&#34;&gt;here&lt;/a&gt; and find all the materials including presentations at the &lt;a href=&#34;https://github.com/wipo-analytics&#34;&gt;WIPO Analytics Github homepage&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-tableau&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Installing Tableau&lt;/h2&gt;
&lt;p&gt;Tableau can be installed for your operating system by visiting the &lt;a href=&#34;https://public.tableau.com/s/&#34;&gt;Tableau Public website&lt;/a&gt; and entering your email address as in the image below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/providemail.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While you are waiting for the app to download it is a good idea to select &lt;code&gt;Sign In&lt;/code&gt; and then &lt;code&gt;Create one now for Free&lt;/code&gt; to sign up for a Tableau Public Account that will allow you to load up your workbooks to the web and share them. We will deal with privacy issues in making workbooks public or private below but as its name suggests Tableau Public is not for sensitive commercial information.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/signup.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This will lead you to an empty profile page.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/profile.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While you are there you might want to check out the &lt;a href=&#34;https://public.tableau.com/s/gallery&#34;&gt;Gallery&lt;/a&gt; of other Tableau Public workbooks to get some ideas on what it is possible to achieve with Tableau. You may want to view a &lt;a href=&#34;https://public.tableau.com/profile/poldham#!/&#34;&gt;Tableau Workbook&lt;/a&gt; for scientific literature that accompanied this &lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0034368&#34;&gt;PLOS ONE article on synthetic biology&lt;/a&gt;. While it is now a few years old it gives an idea of the possibilities of Tableau and the feel of an existing profile page.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/gallery.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-started&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;When you first open the application you will see a blank page. Before we load some data, note the helpful &lt;code&gt;How-to-Videos&lt;/code&gt; on the right and the link to a &lt;code&gt;visualisation of the day&lt;/code&gt;. There are also quite a lot of training videos &lt;a href=&#34;http://www.tableau.com/learn/training&#34;&gt;here&lt;/a&gt; and a very useful &lt;a href=&#34;http://community.tableau.com/community/forums&#34;&gt;community forum&lt;/a&gt;. If you get stuck, or wonder how somebody produced a cool visualisation, this is the place to go.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/open.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To avoid staring at a blank page we now need to load some data. In Tableau Public this is limited to text or Excel files. To download the data as a single &lt;code&gt;.zip&lt;/code&gt; file click &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium_clean/pizza_medium_clean.zip?raw=true&#34;&gt;here&lt;/a&gt; or visit the &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium_clean/pizza_medium_clean.zip&#34;&gt;GitHub repository&lt;/a&gt;. unzip the file and you will see a collection of &lt;code&gt;.csv&lt;/code&gt; files. The excel file and codebook should be ignored as supplementary.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/github.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see above there are a number of files in this dataset. The &lt;code&gt;core&lt;/code&gt; or reference file is &lt;code&gt;pizza.csv&lt;/code&gt;. All other files are aspects of that file, such as applicants, inventors and international patent classification codes. That is concatenated fields in pizza have been separated out and cleaned up. One file, &lt;code&gt;applicants_ipc&lt;/code&gt; is a child file of &lt;code&gt;applicants&lt;/code&gt; that will allow us to access IPC information for individual applicants. This may not make a lot of sense at the moment but don’t worry it will shortly.&lt;/p&gt;
&lt;p&gt;To get started we will select the &lt;code&gt;pizza.csv&lt;/code&gt; file:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/load_file.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will then see a new screen showing some of the data and the other files in the folder. At the bottom is a flag with &lt;code&gt;Go to Worksheet&lt;/code&gt;, so let’s do that.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/pizza1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will now see a screen that is divided in to &lt;code&gt;Dimensions&lt;/code&gt; on the left, with &lt;code&gt;Measures&lt;/code&gt; below. We can see that in the dimensions there are quite a large number of data fields. Note that Tableau will attempt to guess the type of data (for example numeric or date information is marked with &lt;code&gt;#&lt;/code&gt;, geographic data is marked with a globe, text fields are marked with &lt;code&gt;Abc&lt;/code&gt;). Note that Tableau does not always get this right and that it is possible to change a data type by selecting a field and right clicking as we can see below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/tableau_fields.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On the right hand side we can see a floating panel menu. This can be hidden as a menu bar by clicking the x. This panel displays the visualisation options that are available for the data field that we have selected. In this case two map options are available because Tableau has automatically recognised the country names as geographic information. Note that persuading Tableau to present the option that you want (for example visualising year on year data as a line graph) can involve changing the settings for the field until the option you want becomes available.&lt;/p&gt;
&lt;p&gt;At the bottom of the screen we will see a worksheet number &lt;code&gt;Sheet 1&lt;/code&gt; and then options for adding three types of sheet:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A New Worksheet&lt;/li&gt;
&lt;li&gt;A New Dashboard&lt;/li&gt;
&lt;li&gt;A New Story&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For the moment we will focus on building worksheets with the data and then move into creating Dashboards and then Stories around our pizza data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;publication-trends&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Publication Trends&lt;/h2&gt;
&lt;p&gt;One of the first things we normally want to do with patent data is to map trends, either in first filings, publications or family members. In the case of our pizza patents from Patentscope we have a single member of a dossier of files linked to a particular application. This data is fine for demonstration needs and we can easily map trends for this data.&lt;/p&gt;
&lt;p&gt;To do that we simply drag the publication year in the dimensions to the columns field and the number of records from the measures field. Note that Tableau automatically counts the number of rows in a set to create this field. If working with data where accurate counts are important it is important to make sure that the data has been deduplicated on the relevant field before starting. While it does not apply in this case, another important tip is to always have a way of checking key counts in Tableau such as using quick pivot tables in Excel or Open Office. We do not need to worry about this now, but while Tableau is clever software it is still software: it will not always perform calculations as you expect them. For that reason a cross check of counts is a sensible if not vital part of a Tableau workflow.&lt;/p&gt;
&lt;p&gt;Tableau will guess what we are after and draw a graph.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/publication_trend.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see we now have a graph that plunges off a cliff as we approach the present and contains one null. Null values are typically rows or columns containing blank cells. If there is only 1 null value then the data can probably be left as is (in this case it was a blank row at the bottom of the dataset introduced during cleaning in R). However, it pays to inspect nulls by right clicking on the file in &lt;code&gt;Data&lt;/code&gt; and selecting &lt;code&gt;View data&lt;/code&gt;. If there are large numbers of nulls then you may need to go back and inspect the data and ensure that blank cells are filled with &lt;code&gt;NA&lt;/code&gt; values. Let’s go back to our graph.&lt;/p&gt;
&lt;p&gt;What we see here is the &lt;code&gt;data cliff&lt;/code&gt; that is common with patent data. That is, the cliff does not represent a radical decline in the use of the term &lt;code&gt;pizza&lt;/code&gt;, it represents a radical decline in the availability of patent data the closer we move to the present day. The reason for this is that it generally, as a rough rule of thumb, takes about 24 months for an application to be published and can take longer for patent databases to catch up. As such, our &lt;code&gt;data cliff&lt;/code&gt; reflects a lack of available data in recent years, not a lack of activity. Typically we need to pull back about 2 to 3 years to gain an impression of the trend.&lt;/p&gt;
&lt;p&gt;Before we go any further and adjust the axis we will change the graph to something more attractive. To do that we will select filled graph in the floating panel. Behind that panel is a small colour button that will allow us to select a colour we like. The reason that we do this before adjusting the axis is that when we change the graphic type Tableau will revert any changes made to the axis.&lt;/p&gt;
&lt;p&gt;Next we right click the x (lower) axis and adjust the time frame to something more sensible such as 1980 to 2013 by selecting the &lt;code&gt;fixed&lt;/code&gt; option. As a very rough rule of thumb moving back two or three years from the present will take out the data cliff from the lack of published patent information. Note that if we were counting first filings (patent families) the decline would be earlier and much steeper. These lag effects, and ways to deal with them, have been investigated in detail by the &lt;a href=&#34;http://www.oecd.org/sti/inno/intellectual-property-statistics-and-analysis.htm&#34;&gt;OECD patent statistics team&lt;/a&gt;, see in particular work on &lt;a href=&#34;http://www.oecd.org/science/inno/39485567.pdf&#34;&gt;nowcasting patent data&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We now have a good looking graph with a sensible axis. Note here that if we were graphing multiple trends on the same graph (family and family members) we might prefer a straightforward line graph for the sake of clarity.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/publication_trend_fill.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will give this a name &lt;code&gt;Trends&lt;/code&gt; and add a new worksheet by clicking the icon next to our existing sheet.&lt;/p&gt;
&lt;p&gt;The next piece of information we would like is who the most active applicants are. This will also start to expose issues about the different actors who use the term &lt;code&gt;pizza&lt;/code&gt; in the patent system and encourage us to think about ways to drill down into the data to get more accurate information on technologies we might be interested, such as, in this case, pizza boxes and &lt;a href=&#34;http://www.google.co.uk/patents/US8720690&#34;&gt;musical pizza boxes&lt;/a&gt; in particular.&lt;/p&gt;
&lt;p&gt;It is at this point that the work we did in a previous article on separating individual applicant names into their own rows and cleaning them up using Open Refine, becomes important. In this dataset we have taken this a step further using VantagePoint to separate out individuals from organisations. This information is found in the &lt;code&gt;Applicants Organisations&lt;/code&gt; field in the dataset. Lets just drop that onto the worksheet as a row and then add the number of records as a column (tip, simply drop it onto the sheet).&lt;/p&gt;
&lt;p&gt;At first sight everything seems pretty good. But now we need to rank our applicants. To do that we select the small icon in the menu bar with a stacked bar pointing down.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/sort_applicants.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We now see, as we would in the Excel raw file, that there are a significant number of blank entries for applicants in the underlying data, followed by 85 records for Google and 77 for Microsoft. This is also a very good indicator that there may be multiple uses of the word pizza in the patent system unless these software companies have started selling pizzas online.&lt;/p&gt;
&lt;p&gt;In reality this is &lt;strong&gt;&lt;em&gt;a partial view of activity&lt;/em&gt;&lt;/strong&gt; by the applicants because elsewhere in the data the names are concatenated together. This is normally more obvious than in the present dataset through the presence of multiple names separated by &lt;code&gt;;&lt;/code&gt;(to see this scroll down to the first entry for Unilever).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/applicants_original.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To understand why this is a partial view we will now import the &lt;code&gt;applicants.csv&lt;/code&gt; file. The correct way to do this is to select the menu called &lt;code&gt;Data&lt;/code&gt; then &lt;code&gt;New Data Source&lt;/code&gt; and the file &lt;code&gt;applicants.csv&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Next, drag &lt;code&gt;Applicants Orgs All&lt;/code&gt; onto the Rows. Note that Tableau is interpreting these titles for us (the original is &lt;code&gt;applicants_orgs_all&lt;/code&gt;). Then drag &lt;code&gt;Number of Records&lt;/code&gt; from the dimensions onto the sheet or into the columns entry. Now choose the stacked bar icon as above to rank the applicants by the number of records. We will now see the following.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/applicants_organisations.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note the difference between the original applicants field (where Google scored a total of 85 records) and our separated and cleaned field where Google now scores 191 records. In short, before the separation and cleaning exercises we were only seeing 44% of activity in our dataset by Google involving the term pizza. This still does not mean that they have entered the online pizza business… . What it does tell us is that patent analysis that does not separate or split the concatenated data and clean up name variants is missing over 60% of the story when viewed in terms of applicant activity. As this makes clear, the gains from separating or splitting and cleaning data are huge even where, as in this case, the original data appeared to be quite ‘clean’. That appearance was deceptive.&lt;/p&gt;
&lt;p&gt;Now we have a clearer view of what is happening with our applicants we can make this more attractive. To do that first select the blue bar in the floating panel. The worksheet will now be presented as ranked bars. Next, drag the number of records from Measures onto the &lt;code&gt;Label&lt;/code&gt; button next to &lt;code&gt;Color&lt;/code&gt;. That looks pretty good. If we wanted to go a step further we could now turn to the dimensions panel and drag &lt;code&gt;Applicants Orgs All&lt;/code&gt;, onto the &lt;code&gt;Color&lt;/code&gt; button. The bars will now turn to different colours for each applicant. If this is too bright simply grab the green &lt;code&gt;Applicants Orgs All&lt;/code&gt; box from under the buttons menu and move it towards dimensions to remove it. Finally, if we want to adjust the right alignment of the text to the left, then first right click on the name of a company, pick &lt;code&gt;Format&lt;/code&gt; then alignment and left. While the default is right align, in practice left align creates more readable labels. To change the default do this with the first worksheet you create before creating any others.&lt;/p&gt;
&lt;p&gt;We now have an applicants data table that looks, depending on your aesthetic sensibilities, like this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/applicants_cleaned.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At this stage we might want to take a couple of actions. To make the labels more visible, drag the line between the names and the columns to the right. This will open up some space. Next, think about editing long names down to something short. For example, International Business Machines Corporation, who are also not famous for pizzas, is a little bit too long. Right click on the name and select &lt;code&gt;Edit alias&lt;/code&gt; as in the image below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/edit_alias.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now edit the name to IBM. As a tip note that where you discover you have missed a duplicate name in clean up (remember that we focus on good enough rather than perfect in data cleaning) it is also possible to highlight two rows, right click, look for a filing clip icon and group two entries onto a new name. However, the resulting named group must be used in all later analysis. It is also important to realise that data cleaning is not a Tableau strength, Tableau is about data analysis and exploration through visualisation. For data cleaning use a tool such as Open Refine.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-new-data-sources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding New Data Sources&lt;/h2&gt;
&lt;p&gt;We will follow the same procedure that we used for applicants to add the remaining files as data sources. We will add the following four files (as they appear in the folder in alphabetical order).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;applicants_ipc.csv&lt;/li&gt;
&lt;li&gt;inventors.csv&lt;/li&gt;
&lt;li&gt;ipc_class.csv&lt;/li&gt;
&lt;li&gt;ipc_subclass.detail.csv&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To add the data sources either click the &lt;code&gt;Data&lt;/code&gt; menu and &lt;code&gt;New Data Source&lt;/code&gt; or (faster) the cylinder with a plus sign. Then select &lt;code&gt;Text file&lt;/code&gt;, add each file and allow it to load.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/add_data.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If all goes well the &lt;code&gt;Data&lt;/code&gt; panel will now contain the following files.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/data_panel.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note here that the &lt;code&gt;applicants&lt;/code&gt; data displays a blue tick. This is because it was the last data source that we used and is therefore active. The fields we see in Dimensions belong to that data source. Next click in the bottom menu to create a new worksheet and then click &lt;code&gt;inventors&lt;/code&gt; in the &lt;code&gt;Data&lt;/code&gt; field. The field names will now change slightly. It is important to keep an eye on the data source that you are using because it is quite easy to drop a field from one data source onto another. In some cases this is a good thing. But, if you receive a warning message you will be attempting to drop a data source on to another data source where there is no matching field. We will come back to this on data &lt;code&gt;blending&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Next follow the same procedure for ranking applicants with inventors using the &lt;code&gt;Inventors All&lt;/code&gt;. For anyone interested in seeing the dramatic impacts of concatenated fields try dropping the &lt;code&gt;Inventors Original&lt;/code&gt; field onto the worksheet.&lt;/p&gt;
&lt;p&gt;Using &lt;code&gt;Inventors All&lt;/code&gt; you should now see the following ranked list of inventors.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/inventors_ranked.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now repeat this exercise for the remaining data sources by first creating a sheet and then selecting the data source. As you move through this select the following dimensions to add to the sheet and then drop number of&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;applicants_ipc. Drop &lt;code&gt;Ipc Subclass Detail&lt;/code&gt; onto the sheet. Then drop number of records onto the sheet where the field says Abc. Note that a number &lt;code&gt;6&lt;/code&gt; will appear in the first row. This is an artifact from the separation process. Select that cell, right click and then choose &lt;code&gt;Exclude&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Do not rank this data, but instead drag the field &lt;code&gt;Applicants Orgs All&lt;/code&gt; onto the sheet so that it is the first row (tip, it is easiest to do this by dragging the field into the row bar before the IPC field). You will now see a list of company names followed by a list of IPCs. Congratulations, we now have an idea of who is patenting in a particular area of technology using the word pizza at the level of individual applicants.&lt;/p&gt;
&lt;p&gt;Add a new sheet. Then click on &lt;code&gt;ipc_subclass_detail&lt;/code&gt;. Note that if you click on the data source first, the dimensions panel will go orange. Don’t panic. The reason is that Tableau thinks you are trying to blend data from the ipc_subclass_detail source with applicants_ipc. If you do this simply click on ipc_subclass-detail again.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;ipc_subclass-detail. Drop the &lt;code&gt;Ipc Subclass Detail&lt;/code&gt; dimension on to the sheet. Then drop the number of records onto the sheet. Then click on the first cell containing &lt;code&gt;6&lt;/code&gt; as an artifact and exclude. Repeat for &lt;code&gt;7&lt;/code&gt;. Then select the bar chart in the floating &lt;code&gt;Show Me&lt;/code&gt; panel, then drag &lt;code&gt;Number of Records&lt;/code&gt; onto the &lt;code&gt;Label&lt;/code&gt; button. Now rank the column using the descending button in the upper menu as before.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At this point, if we had not trimmed the leading white space the ranked list would display indentations and there would be duplicates of the same IPC code. For that reason it is important to trim leading white space before attempting to visualise data (and this applies to all our separated fields).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-an-overview-dashboard&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating an Overview Dashboard&lt;/h2&gt;
&lt;p&gt;You should now have five worksheets each of which displays aspects of our core &lt;code&gt;pizza&lt;/code&gt; set. We have named the sheets as follows and suggest that you might want to do the same. Note that where there is more than one sheet containing similar but distinct information it will be helpful to give them distinct names (e.g. IPC Subclass and Applicants IPC Subclasses). We might even start using less technical labels by calling the IPC something clearer like Technology Area, to aid communication with non-IP specialists&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/sheet_names.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s get a quick overview of the data so far. Next to the add worksheet button in the worksheets bar is a second icon to create a dashboard. Click on that and we will now see a sheet called &lt;code&gt;Dashboard 1&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Dashboards are perhaps Tableau’s best known feature and are rightly very popular. We can fill our dashboard by dragging the worksheets from the &lt;code&gt;Dashboard&lt;/code&gt; side menu. The order in which we do this can make life easier or more difficult to adjust later. Let’s do it in the following steps&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Drag &lt;code&gt;Trends&lt;/code&gt; onto the dashboard and it will now fill the view.&lt;/li&gt;
&lt;li&gt;Drag &lt;code&gt;Organisations&lt;/code&gt; onto the dashboard.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/dashboard1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That is rather messy, but all is not lost. Simply click in the top right corner of the organisations panel on the right to remove it (in the original worksheet click on it and select &lt;code&gt;Hide&lt;/code&gt;). We now have an &lt;code&gt;Organisations&lt;/code&gt; column that still looks crunched.&lt;/p&gt;
&lt;p&gt;Now select the top of the organisations box and a small inverted triangle will appear. Click on that and then choose &lt;code&gt;Fit &amp;gt; Fit Width&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/dashboard_fitwidth.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The bars may now disappear. Click into the box on the line where the bars start and drag them back into view. At this point long names may start to be obscured. If desired, right click on a long name such as &lt;code&gt;Graphic Packaging International&lt;/code&gt;, choose &lt;code&gt;Edit alias&lt;/code&gt; and edit it down to something sensible such as &lt;code&gt;Graphic Packaging Int&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We now have two panels on the dashboard. Let’s add two more. First drag technology areas below the line where Trends and Organisations finish. Grey shaded boxes will appear that show the placement, across the width is fine. This can take some time to get right, when the whole of the bottom area is highlighted let go of the mouse. If it goes somewhere strange either select the box and in the top right press &lt;code&gt;x&lt;/code&gt; to remove it, or try moving it (in our experience it is often easier to remove it and try again).&lt;/p&gt;
&lt;p&gt;Do not try to format this box yet. Instead, grab inventors and drag it into the space before the technology areas below.&lt;/p&gt;
&lt;p&gt;We now have four panels in the dashboard but they need some tidying up. First, in the two boxes we have just edited repeat the &lt;code&gt;Fit Width&lt;/code&gt; exercise and then drag the line for the bars around until they are in view and satisfactory. Next, we have names such as &lt;code&gt;Applicants Orgs All&lt;/code&gt; that are our internal reference names. Click on them in each of the three panels one at a time and select &lt;code&gt;Hide Field Labels for Rows&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Hmm… our Technology Areas panel is proving troublesome because even the edited version of the IPC is rather long.&lt;/p&gt;
&lt;p&gt;Before we do any editing, first experiment with the &lt;code&gt;Size&lt;/code&gt; menu in the bottom right. The default dashboard size in Tableau Public is actually quite small. Change the settings until you have something that looks cleaner even if there are still some overlaps. Options such as &lt;code&gt;Desktop&lt;/code&gt;, &lt;code&gt;Laptop&lt;/code&gt; and &lt;code&gt;Large blog&lt;/code&gt; are generally decent sizes but in part the decision depends on where you believe it will be displayed.&lt;/p&gt;
&lt;p&gt;To fix the long Technology areas labels we go back to the original sheet (tip: if you move the mouse to the top right in the panel an arrow with &lt;code&gt;Go to Sheet&lt;/code&gt; will appear, it is very useful for large workbooks). Inside the original sheet, try dragging the line separating the text and bars so that the bars now cover some of the longer text. Then switch back to the dashboard. If you feel unhappy with the result then right click in the panel in the dashboard and then choose &lt;code&gt;Edit alias&lt;/code&gt;. This is useful for simply making labels in the view more visible (it does not change the original data).&lt;/p&gt;
&lt;p&gt;If all goes well you will now have a dashboard that looks more or less like this. Note that depending on the worksheet settings you may want to make the font size consistent (right click and choose Format, then font size). Note also that if you increase the font size (the default is 8 point) then you may need to edit some of the labels again.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;images/tableau/dashboard_completed.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have now done quite a lot of work and produced an Overview dashboard. It is time to save the workbook to the server before doing anything else.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;saving-display-and-privacy-settings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Saving, Display and Privacy Settings&lt;/h2&gt;
&lt;p&gt;The only option for saving a Tableau Public workbook is to save it online. To save the file go to &lt;code&gt;File&lt;/code&gt; and Save to Tableau Public. If you want to save the workbook as a new file (after previously saving) then choose Save As.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/save_public.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You will then be asked to enter your username and password (Tableau does not remember the password) and the file will upload. Tableau will then compress the data. As of June 2015 it is possible to store 10GB of data overall and to have up to 10 million rows in a workbook (which is generally more than enough).&lt;/p&gt;
&lt;p&gt;Tableau will then open a web browser at your profile page and it will look a lot like this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/profile_dashboard.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Having read the message, click &lt;code&gt;Got it&lt;/code&gt; on the right. Do you notice anything strange. Yes, we can only see the Dashboard and not any of the other sheets. To change this and any other details click on &lt;code&gt;edit details&lt;/code&gt; near the title and some menus will open up as follows.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/profile_viewtabs3.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To make sure the worksheets are visible select the check box marked &lt;code&gt;Show workbook sheets as tabs&lt;/code&gt; and then &lt;code&gt;Save&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/profile_viewtabs3.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To access this demonstration workbook go &lt;a href=&#34;https://public.tableau.com/profile/wipo.open.source.patent.analytics.manual#!/vizhome/pizzapatents/Overview&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;privacy-and-security&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Privacy and Security&lt;/h2&gt;
&lt;p&gt;As emphasised above, Tableau Public is by definition a place for publicly sharing workbooks and visualisations. It is not for sensitive data. In the past users, such as journalists, relied on what might be called ‘security by obscurity’ but the trend towards storing data on a Tableau public profile (the only option) makes that less of an option. If this a concern there are two actions that might potentially be considered that limit the visibility of a workbook and its wider use. Logically, the answer to any concerns about Tableau Public and sensitive information is &lt;strong&gt;&lt;em&gt;not to include sensitive information in the first place&lt;/em&gt;&lt;/strong&gt;. The following are not recommendations but simply highlight the available options.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In the discussion on the settings above, there is a check box to prevent users from downloading a workbook. You might want to select that option where a workbook contains information that you do not want to be seen other than what you choose to make visible.&lt;/li&gt;
&lt;li&gt;It is possible to create a setting so that a workbook does not show up on a user’s profile. This is hard to spot, and appears by hovering over the workbook in the Profile view.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/profile_visibility.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As the message points out using this option does not prevent a workbook being found through search engines or seen by users. It just means it is not visible on the profile page.&lt;/p&gt;
&lt;p&gt;As such, Tableau public is fundamentally about sharing information with others through visualisation. That is, it is for information that you want others to see. Here it is briefly worth returning to the completed dashboard above and clicking the share button.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.pauloldham.net/images/tableau/profile_sharing.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see here, Tableau generates embed codes for use on websites or for emailing as a link along with twitter and facebook.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round Up&lt;/h2&gt;
&lt;p&gt;In this chapter we have introduced the visualisation of patent data using a set of nearly 10,000 patent documents from WIPO Patentscope that mention pizza. As should by now be clear Tableau Public is a very powerful free tool for data visualisation. It requires attention to detail and care in construction but is one of the best free tools that is out there for visualisation and dashboarding.&lt;/p&gt;
&lt;p&gt;To take working with Tableau on pizza patents forward on your own here are some tips.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;You already know how to use Tableau to create a map of publication countries.&lt;/li&gt;
&lt;li&gt;The pizza source file contains a set of publication numbers. Try a) creating a visualisation with the publication numbers, b) looking in the pizza source file for a set of URL and then exploring what can be done with &lt;code&gt;Worksheet &amp;gt; Action&lt;/code&gt; with that URL.&lt;/li&gt;
&lt;li&gt;In dashboards consider using one field as a filter for another field (such as applicant and title). What data source or data sources would you need to do that?&lt;/li&gt;
&lt;li&gt;What kinds of stories does the pizza data tell us and how might we visualise them using the information provided on applicants and its subset Applicants IPCs?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you get stuck, and it does take time to become familiar with Tableau’s potential, perhaps try exploring this &lt;a href=&#34;https://public.tableau.com/profile/poldham#!/&#34;&gt;workbook on synthetic biology&lt;/a&gt; and the use of Tableau images in this article &lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0034368&#34;&gt;PLOS ONE article&lt;/a&gt;. As a tip, try clicking on the bars and then the titles to understand Actions. Downloading workbooks prepared by others can be a very good way of learning the tips and tricks of tableau visualisation and dashboarding.&lt;/p&gt;
&lt;p&gt;If you would like to download the pizza workbook it is &lt;a href=&#34;https://public.tableau.com/profile/wipo.open.source.patent.analytics.manual#!/vizhome/pizzapatents/Overview&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, one of the most important issues exposed by working with Tableau is that you must ensure that fields you want to visualise are &lt;code&gt;tidy&lt;/code&gt;, that is not concatenated, and also that they are as clean as it is reasonable to make them. For researchers wishing to work up their own data we suggest the Open Refine article as a good starting point.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
