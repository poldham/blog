<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paul Oldham&#39;s Analytics Blog</title>
    <link>/</link>
    <description>Recent content on Paul Oldham&#39;s Analytics Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright @ Paul Oldham 2017-2018</copyright>
    <lastBuildDate>Fri, 13 Jul 2018 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Introduction to ORCID Researcher Identifiers in R with rorcid</title>
      <link>/introduction-to-orcid-with-rorcid/</link>
      <pubDate>Fri, 13 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/introduction-to-orcid-with-rorcid/</guid>
      <description>&lt;p&gt;This article provides a practical introduction to the &lt;code&gt;rorcid&lt;/code&gt; package from &lt;a href=&#34;https://ropensci.org/&#34;&gt;ROpenSci&lt;/a&gt; to access the &lt;a href=&#34;https://orcid.org/organizations/integrators/API&#34;&gt;ORCID researcher ID API&lt;/a&gt;. ORCID stands for Open Researcher or Contributor ID &lt;span class=&#34;citation&#34;&gt;(Haak et al. 2012; Meadows 2016; Youtie et al. 2017)&lt;/span&gt;. ORCID is a non-profit organisation that provides researchers with a free unique researcher identifier and a profile. To date over 5 million ORCID IDs have been issued.&lt;/p&gt;
&lt;p&gt;An ORCID ID provides a researcher with a unique identifier and a single place where they can gather together details on their career, funding, publications, patents and datasets. The ORCID profile is under the researcher’s control and they can decide what to make public or to keep private. An important feature of the ORCID system is that it integrates with services such as Crossref and so can automate updates of researcher publications. ORCID is also important to research funding organisations, employers and publishers. A growing number of funding organisations, such as Research Councils UK, now keep track of research investments and outcomes using ORCID.&lt;/p&gt;
&lt;p&gt;The key idea behind ORCID is researcher name disambiguation. There are two main problems here that can be described in terms of lumps and splits &lt;span class=&#34;citation&#34;&gt;(Fegley and Torvik 2013)&lt;/span&gt;. The main problem is lumped names. The scientific literature is rife with people who share the same name but are distinct persons. In English a classic example would be John Smith while in Spanish it would be Carlos Garcia or Wei Wang in East Asia. This presents the problem of how to distinguish between distinct persons. The second problem arises from splits or variations in the same persons name. This can be described as the James T Kirk or Captain Kirk problem because this name might be represented as JT Kirk, James Tiberius Kirk or Kirk T James and so on with punctuation throwing additional confusion into the mayhem.&lt;/p&gt;
&lt;p&gt;ORCID contributes to solving this problem through the use of unique identifiers. It is not the only researcher identifier system out there but it has the huge advantage of being free and open access while ORCID profiles are controlled by researchers themselves.&lt;/p&gt;
&lt;p&gt;Scott Chamberlain from ROpenSci has developed the &lt;code&gt;rorcid&lt;/code&gt; package to access the ORCID API in R. &lt;code&gt;rorcid&lt;/code&gt; is well written and documented with plenty of examples. This article provides an introduction to &lt;code&gt;rorcid&lt;/code&gt; focusing on common tasks you are likely to want to use ORCID for and how to deal with processing the return from the API in R in a way that produces useful data. Please feel welcome to suggest improvements!&lt;/p&gt;
&lt;p&gt;When working with ORCID we commonly start from two different positions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;We have the name of a researcher and possibly other information about them such as their organisation. We want to look up their ORCID profile.&lt;/li&gt;
&lt;li&gt;We have an ORCID identifier and we want to retrieve information such as publications or funding information.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will deal with each of these in turn.&lt;/p&gt;
&lt;div id=&#34;getting-started&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;We need to install the &lt;code&gt;rorcid&lt;/code&gt; package. We will also install some helper packages. You will probably already have the tidyverse (we’ll mainly use &lt;code&gt;purrr&lt;/code&gt;, &lt;code&gt;dplyr&lt;/code&gt; and the pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt;) and we’ll use &lt;code&gt;janitor&lt;/code&gt; to consistently clean up column names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;rorcid&amp;quot;)
install.packages(&amp;quot;tidyverse&amp;quot;)
install.packages(&amp;quot;janitor&amp;quot;)
install.packages(&amp;quot;usethis&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rorcid)
library(tidyverse)
library(janitor)
library(usethis)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ORCID requires us to authenticate with an ORCID API Key to use the public API. There are two ways to do this. The easiest way to get started is simply to run &lt;code&gt;orcid_auth()&lt;/code&gt; which will open a browser window and invite you to login (you will of course need to sign up to login). You will then be asked to close the browser and you will be good to go. A token will be cached locally in your working directory.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;orcid_auth()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second way, which works better for regular use, is to copy the API key displayed by &lt;code&gt;orcid_auth&lt;/code&gt; minus the Bearer. Then call &lt;code&gt;usethis&lt;/code&gt; to open up the Renviron file and enter and save the key as as ORCID_TOKEN=“yourkey”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;usethis::edit_r_environ()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will need to restart the R session for the key to take effect the first time you do this and reload any libraries. You should now be able to call the key with.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Sys.getenv(&amp;quot;ORCID_TOKEN&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;looking-up-researchers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Looking up Researchers&lt;/h2&gt;
&lt;p&gt;We have two main choices when we want to look up researchers with ORCID&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We use the most exact search criteria that we can (such as name, country, organisation, keywords)&lt;/li&gt;
&lt;li&gt;We cast the net wide and then narrow the potential results down.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The choice you make will partly depend on the information that you have at hand. However, in reality you will often end up using the second option for reasons we will explore below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-searching&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basic Searching&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;rorcid&lt;/code&gt; divides calls to the API up into a range of functions providing access to distinct chunks of data. We will start with a simple query using &lt;code&gt;orcid()&lt;/code&gt;. If we use a simple open query we will get a lot of results back (up to the default maximum of 100).&lt;/p&gt;
&lt;p&gt;One tip when exploring ORCID is to use yourself as the example… because you should know what the right answer is. You can also use the fictitious, but incomplete, ORCID profile for Josiah Carberry, a specialist in psychoceramics, at &lt;a href=&#34;https://orcid.org/0000-0002-1825-0097&#34; class=&#34;uri&#34;&gt;https://orcid.org/0000-0002-1825-0097&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I’ll just run a simple open search.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;oldham &amp;lt;- orcid(query = &amp;quot;paul oldham&amp;quot;)
oldham&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 100 x 3
##    `orcid-identifier.uri`                `orcid-identifi… `orcid-identifi…
##  * &amp;lt;chr&amp;gt;                                 &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;           
##  1 https://orcid.org/0000-0002-0628-5540 0000-0002-0628-… orcid.org       
##  2 https://orcid.org/0000-0002-1013-4390 0000-0002-1013-… orcid.org       
##  3 https://orcid.org/0000-0003-1938-0798 0000-0003-1938-… orcid.org       
##  4 https://orcid.org/0000-0002-4058-1490 0000-0002-4058-… orcid.org       
##  5 https://orcid.org/0000-0001-5920-3804 0000-0001-5920-… orcid.org       
##  6 https://orcid.org/0000-0002-2440-8444 0000-0002-2440-… orcid.org       
##  7 https://orcid.org/0000-0001-5141-827X 0000-0001-5141-… orcid.org       
##  8 https://orcid.org/0000-0002-1800-0530 0000-0002-1800-… orcid.org       
##  9 https://orcid.org/0000-0003-2915-3169 0000-0003-2915-… orcid.org       
## 10 https://orcid.org/0000-0002-4243-8807 0000-0002-4243-… orcid.org       
## # ... with 90 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ORCID will return a default of 100 results for searches. Note that we only receive three fields back, the url, the orcid identifier in path and the host.&lt;/p&gt;
&lt;p&gt;This is not really what we want because the query is looking for Paul OR Oldham. We can get closer by being more specific using the basic guide to search syntax &lt;a href=&#34;https://members.orcid.org/api/tutorial/search-orcid-registry&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;oldham_gen &amp;lt;- orcid(query = &amp;quot;given-names:paul AND family-name:oldham&amp;quot;) %&amp;gt;% 
  janitor::clean_names() 
oldham_gen&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
##   orcid_identifier_uri                  orcid_identifier… orcid_identifie…
## * &amp;lt;chr&amp;gt;                                 &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;           
## 1 https://orcid.org/0000-0002-0628-5540 0000-0002-0628-5… orcid.org       
## 2 https://orcid.org/0000-0002-1013-4390 0000-0002-1013-4… orcid.org&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use &lt;code&gt;janitor::clean_names&lt;/code&gt; in this code to convert awkward punctuation in column names to underscores. This makes life easier because we don’t have to play with names like &lt;code&gt;orcid-identifier.uri&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The search returns 2 people who share this name. We only have orcid identifiers at the moment but we can use the very useful browse function to view the data in a browser. Normally you will use this with a single ORCID at a time. This call will trigger a browser window.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rorcid::browse(as.orcid(oldham$orcid_identifier_path[[1]]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But of course you can always browse multiple ORCIDs if you want to courtesy of &lt;code&gt;purrr&lt;/code&gt;. This will open multiple tabs containing the ORCID profiles in your browser. Use this with caution if you have lots and lots of ORCID ids or you will live in interesting times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;purrr::map(oldham_gen$orcid_identifier_path, rorcid::browse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point you probably want to start exploring other search options to make the search more accurate. As Scott explains in the &lt;code&gt;orcid()&lt;/code&gt; documentation you can use &lt;a href=&#34;https://cwiki.apache.org/confluence/display/solr/The+Standard+Query+Parser&#34;&gt;SOLR 3.6&lt;/a&gt;) including Lucene with &lt;a href=&#34;https://wiki.apache.org/solr/DisMax&#34;&gt;DisMax&lt;/a&gt; and &lt;a href=&#34;http://wiki.apache.org/solr/ExtendedDisMax&#34;&gt;Extended Dismax&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If we have more information available we might want to try something like this. In this case the query include the researchers previous affiliations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;oldham &amp;lt;- orcid(query = &amp;quot;given-names:paul AND family-name:oldham AND 
                affiliation-org-name:London School of Economics&amp;quot;) %&amp;gt;% 
  janitor::clean_names()
oldham&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##   orcid_identifier_uri                  orcid_identifier… orcid_identifie…
## * &amp;lt;chr&amp;gt;                                 &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;           
## 1 https://orcid.org/0000-0002-1013-4390 0000-0002-1013-4… orcid.org&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A table of fields is available in this &lt;a href=&#34;https://members.orcid.org/api/tutorial/search-orcid-registry&#34;&gt;SOLR tutorial&lt;/a&gt; and there are also quite a number of examples in the &lt;code&gt;rorcid&lt;/code&gt; function documentation to experiment with. The following query searches for the author name and the word patents across all text fields. Other useful fields to try with AND/OR are &lt;code&gt;other-names&lt;/code&gt;, &lt;code&gt;keyword&lt;/code&gt;, &lt;code&gt;work-titles&lt;/code&gt;, and &lt;code&gt;digital-object-ids&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;oldham &amp;lt;- orcid(query = &amp;quot;given-names:paul AND family-name:oldham AND
                text:patents&amp;quot;) %&amp;gt;% 
  janitor::clean_names()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;dealing-with-nosiy-names&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dealing with Nosiy Names&lt;/h2&gt;
&lt;p&gt;ORCID is intended to help address the problem of name disambiguation (same name but different persons or variants of names) but we still confront the problem of how much information we have in the first place. We are also presented with the problem of variations in the form of the same information (e.g. the London School of Economics or LSE or the London School of Economics and Political Science). The challenge with using precise match criteria at the outset is that we might miss valid variants of our terms. This means that we will often want to start by capturing the universe of things that need to be captured and then filter the data to arrive at the information we are looking for.&lt;/p&gt;
&lt;p&gt;To illustrate, let’s pull back some information on the common name John Smith.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;smith &amp;lt;- orcid(query = &amp;quot;given-names:john AND family-name:smith&amp;quot;) %&amp;gt;% 
  janitor::clean_names() %&amp;gt;% 
  mutate(source_source_orcid_path = orcid_identifier_path)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At the end of this code we have added a call to &lt;code&gt;dplyr::mutate&lt;/code&gt; that copies the &lt;code&gt;orcid_identifier_path&lt;/code&gt; to &lt;code&gt;source_source_orcid_path&lt;/code&gt;. The reason for this is that when we send the &lt;code&gt;orcid_identifier_path&lt;/code&gt; to other functions it comes back called &lt;code&gt;source_source_orcid_path&lt;/code&gt;. To enable joins to input tables we simply add this column.&lt;/p&gt;
&lt;p&gt;We have pulled back 81 identifiers for john smith. If you would like to pull back all data beyond the default maximum of 100 results (the first page) try using &lt;code&gt;recursive = TRUE&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;All we have to go on at present is the ORCID id. To pull back other information we will need to pass the orcid IDs to other &lt;code&gt;rorcid&lt;/code&gt; functions. Here we will use &lt;code&gt;orcid_address&lt;/code&gt; to retrieve the address data and then restrict the data to the UK (“GB”).&lt;/p&gt;
&lt;p&gt;Many &lt;code&gt;rorcid&lt;/code&gt; functions return a list containing one or more data frames so we will use &lt;code&gt;purrr&lt;/code&gt; to extract the data frames. When using &lt;code&gt;map_df&lt;/code&gt; from &lt;code&gt;purrr&lt;/code&gt; note that NULLs and NAs can lead to errors. One tip when exploring list data for the first time is to use &lt;code&gt;purr::map&lt;/code&gt; at first to inspect the data because it always returns a list and then experiment with &lt;code&gt;map_df&lt;/code&gt; (see also &lt;code&gt;safely&lt;/code&gt; and &lt;code&gt;possibly&lt;/code&gt;). Also note that we use back ticks and not quotes around &lt;code&gt;[[&lt;/code&gt; to subset into the list.&lt;/p&gt;
&lt;p&gt;We will then filter the data on the &lt;code&gt;country_value&lt;/code&gt; field. In this case we will find John Smiths in the UK (GB).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;smith_country &amp;lt;- orcid_address(smith$orcid_identifier_path) %&amp;gt;% 
  purrr::map_df(., `[[`, 2) %&amp;gt;% # access address level
  janitor::clean_names()
  
smith_country %&amp;gt;% 
  filter(country_value == &amp;quot;GB&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 12
##   visibility path                 put_code display_index created_date_val…
##   &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;                   &amp;lt;int&amp;gt;         &amp;lt;int&amp;gt;             &amp;lt;dbl&amp;gt;
## 1 PUBLIC     /0000-0002-7709-550…   648480             1     1488985416579
## 2 PUBLIC     /0000-0002-1963-409…   167571             0     1453659484730
## 3 PUBLIC     /0000-0003-0079-969…   921160             1     1520958186511
## 4 PUBLIC     /0000-0003-2119-855…   590213             1     1481331153320
## 5 PUBLIC     /0000-0003-1184-791…   631888             1     1487068984706
## # ... with 7 more variables: last_modified_date_value &amp;lt;dbl&amp;gt;,
## #   source_source_client_id &amp;lt;lgl&amp;gt;, source_source_orcid_uri &amp;lt;chr&amp;gt;,
## #   source_source_orcid_path &amp;lt;chr&amp;gt;, source_source_orcid_host &amp;lt;chr&amp;gt;,
## #   source_source_name_value &amp;lt;chr&amp;gt;, country_value &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We still have five results and not much more to go on. So the next step is to retrieve information about employment, education and keywords for the IDs. We will start with the raw oldham sets to get a feel for it. We use &lt;code&gt;orcid_employments()&lt;/code&gt; to pull back the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;oldham_employ &amp;lt;- rorcid::orcid_employments(oldham_gen$orcid_identifier_path) %&amp;gt;% 
  map_df(., `[[`, &amp;quot;employment-summary&amp;quot;) %&amp;gt;% 
  janitor::clean_names()

oldham_employ %&amp;gt;% 
  select(1:2) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;department_name&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;role_title&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;UniSA College&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Course Coordinator/Lecturer/Tutor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;DIrector&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ESRC Centre for Economic and Social Aspects of Genomics&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Research Fellow, Senior Research Associate, Research Associate&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Lecturer&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The employment records may be of different lengths. For example one oldham above has one entry and another has three. The &lt;code&gt;source_source_orcid_path&lt;/code&gt; column is the key field for identifying which oldham the records belong to.&lt;/p&gt;
&lt;p&gt;We will often be looking for data on more than one ORCID and entries with different numbers of rows will create a headache later on. So, we will often want to concatenate this data. To do this we use &lt;code&gt;dplyr::group_by&lt;/code&gt; to group the data on the ORCID id. We then nest the data into a list column using &lt;code&gt;tidyr::nest&lt;/code&gt; and give it a name.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nested &amp;lt;- oldham_employ %&amp;gt;%
  group_by(source_source_orcid_path) %&amp;gt;% 
  nest(.key = &amp;quot;employ&amp;quot;)
nested&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   source_source_orcid_path employ           
##   &amp;lt;chr&amp;gt;                    &amp;lt;list&amp;gt;           
## 1 0000-0002-0628-5540      &amp;lt;tibble [1 × 24]&amp;gt;
## 2 0000-0002-1013-4390      &amp;lt;tibble [3 × 24]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now see that we have a tibble for the first entry and a second tibble with 3 rows for the second. We use nest because it allows us to build up a data frame consisting of tibbles of different lengths linked to the ORCID ID. To access a nested field we can subset as usual.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nested$employ[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 24
##   department_name role_title      end_date visibility put_code path       
##   &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;           &amp;lt;lgl&amp;gt;    &amp;lt;chr&amp;gt;         &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;      
## 1 UniSA College   Course Coordin… NA       PUBLIC      3834445 /0000-0002…
## # ... with 18 more variables: created_date_value &amp;lt;dbl&amp;gt;,
## #   last_modified_date_value &amp;lt;dbl&amp;gt;, source_source_client_id &amp;lt;lgl&amp;gt;,
## #   source_source_orcid_uri &amp;lt;chr&amp;gt;, source_source_orcid_host &amp;lt;chr&amp;gt;,
## #   source_source_name_value &amp;lt;chr&amp;gt;, start_date_year_value &amp;lt;chr&amp;gt;,
## #   start_date_month_value &amp;lt;chr&amp;gt;, start_date_day_value &amp;lt;chr&amp;gt;,
## #   organization_name &amp;lt;chr&amp;gt;, organization_address_city &amp;lt;chr&amp;gt;,
## #   organization_address_region &amp;lt;chr&amp;gt;, organization_address_country &amp;lt;chr&amp;gt;,
## #   organization_disambiguated_organization_disambiguated_organization_identifier &amp;lt;chr&amp;gt;,
## #   organization_disambiguated_organization_disambiguation_source &amp;lt;chr&amp;gt;,
## #   end_date_year_value &amp;lt;chr&amp;gt;, end_date_month_value &amp;lt;chr&amp;gt;,
## #   end_date_day_value &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or we can use &lt;code&gt;tidyr::unnest()&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unnest(nested)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 25
##   source_source_o… department_name role_title end_date visibility put_code
##   &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;      &amp;lt;lgl&amp;gt;    &amp;lt;chr&amp;gt;         &amp;lt;int&amp;gt;
## 1 0000-0002-0628-… UniSA College   Course Co… NA       PUBLIC      3834445
## 2 0000-0002-1013-… &amp;lt;NA&amp;gt;            DIrector   NA       PUBLIC      1749490
## 3 0000-0002-1013-… ESRC Centre fo… Research … NA       PUBLIC      1749502
## 4 0000-0002-1013-… &amp;lt;NA&amp;gt;            Lecturer   NA       PUBLIC      1880285
## # ... with 19 more variables: path &amp;lt;chr&amp;gt;, created_date_value &amp;lt;dbl&amp;gt;,
## #   last_modified_date_value &amp;lt;dbl&amp;gt;, source_source_client_id &amp;lt;lgl&amp;gt;,
## #   source_source_orcid_uri &amp;lt;chr&amp;gt;, source_source_orcid_host &amp;lt;chr&amp;gt;,
## #   source_source_name_value &amp;lt;chr&amp;gt;, start_date_year_value &amp;lt;chr&amp;gt;,
## #   start_date_month_value &amp;lt;chr&amp;gt;, start_date_day_value &amp;lt;chr&amp;gt;,
## #   organization_name &amp;lt;chr&amp;gt;, organization_address_city &amp;lt;chr&amp;gt;,
## #   organization_address_region &amp;lt;chr&amp;gt;, organization_address_country &amp;lt;chr&amp;gt;,
## #   organization_disambiguated_organization_disambiguated_organization_identifier &amp;lt;chr&amp;gt;,
## #   organization_disambiguated_organization_disambiguation_source &amp;lt;chr&amp;gt;,
## #   end_date_year_value &amp;lt;chr&amp;gt;, end_date_month_value &amp;lt;chr&amp;gt;,
## #   end_date_day_value &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One word of caution is that &lt;code&gt;group_by&lt;/code&gt; creates a grouped data.frame or tibble. It is not an issue in this case but normally any function that is applied to a grouped tibble will be applied based on the group variable. This can produce very odd results, so you will normally want to &lt;code&gt;ungroup()&lt;/code&gt; afterwards. In this case checking &lt;code&gt;class(nested)&lt;/code&gt; reveals we are good to go.&lt;/p&gt;
&lt;p&gt;To scale up let’s try fetching the employment data for the smith set and some other chunks of data. There are quite a few different chunks of data that we can call back with &lt;code&gt;rorcid&lt;/code&gt; functions. For example &lt;code&gt;orcid_person&lt;/code&gt; will retrieve basic data on the person including the country value. &lt;code&gt;orcid_bio&lt;/code&gt; will retrieve any biographical text entries and can then be text mined. Here we will just quickly run through a number of other fields:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;smith_employ &amp;lt;- rorcid::orcid_employments(smith$orcid_identifier_path) %&amp;gt;% 
  purrr::map_df(., `[[`, &amp;quot;employment-summary&amp;quot;) %&amp;gt;% 
  janitor::clean_names() %&amp;gt;% 
  group_by(source_source_orcid_path) %&amp;gt;% 
  nest(.key = &amp;quot;employment&amp;quot;)

nrow(smith_employ)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 14&lt;/code&gt;&lt;/pre&gt;
&lt;!---For fun, it turns out that some of the John Smith entries are blank because they are spam entries. Can you figure out out which ones are spam?---&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;smith_education &amp;lt;- rorcid::orcid_educations(smith$source_source_orcid_path) %&amp;gt;%
  purrr::map_df(., `[[`, &amp;quot;education-summary&amp;quot;) %&amp;gt;%
  janitor::clean_names() %&amp;gt;% 
  group_by(source_source_orcid_path) %&amp;gt;% 
  nest(.key = &amp;quot;education&amp;quot;)

nrow(smith_education)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 15&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;smith_keywords &amp;lt;- rorcid::orcid_keywords(smith$source_source_orcid_path) %&amp;gt;%
  purrr::map_df(., `[[`, &amp;quot;keyword&amp;quot;) %&amp;gt;%
  janitor::clean_names() %&amp;gt;% 
  group_by(source_source_orcid_path) %&amp;gt;% 
  nest(.key = &amp;quot;keyword&amp;quot;)

nrow(smith_keywords)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that these calls are becoming repetitive. We are calling a specific &lt;code&gt;rorcid&lt;/code&gt; function and then extracting a specific field into a data frame, suggesting we could start thinking about a helper function. We won’t go there now but a quick initial sketch for that might be.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- function(fun, id, field){
  res &amp;lt;- fun(id) %&amp;gt;% 
    purrr::map_df(., `[[`, field) %&amp;gt;% 
    janitor::clean_names() %&amp;gt;% 
    group_by(source_source_orcid_path) %&amp;gt;% 
    nest(.key = field)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;joining-profile-data-together&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Joining Profile Data Together&lt;/h2&gt;
&lt;p&gt;We now have a bunch of chunks of data. Note how our original john smith data contained 81 unique ORCID ids but we are pulling back data frames with different numbers of rows (and of different lengths). So at the moment we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;input orcids 81&lt;/li&gt;
&lt;li&gt;employ 14&lt;/li&gt;
&lt;li&gt;education 15&lt;/li&gt;
&lt;li&gt;keywords 10&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will often see this with ORCID and in some cases fields may be dominated by NULLs. For example, according to ORCID only about 2% of the 3 million ORCID ids in 2017 included a public email address.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Researchers, including this one, are sick of endless academic spam and so will often choose not make their emails public.&lt;/p&gt;
&lt;p&gt;To deal with the data frames with different numbers of rows we will start by creating a table of unique ids from ORCID path. One thing to watch out for here is that a seemingly valid input ORCID may return an NA. In the case of the john smith data there was one case of this which seemed to be a record flagged for removal. We can handle this with &lt;code&gt;tidyr::drop_na()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ids &amp;lt;- bind_rows(smith_employ, smith_education, smith_keywords) %&amp;gt;% 
  mutate(duplicated = duplicated(source_source_orcid_path)) %&amp;gt;% 
  filter(duplicated == FALSE) %&amp;gt;% 
  select(source_source_orcid_path) %&amp;gt;% 
  drop_na()
ids&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 23 x 1
##    source_source_orcid_path
##    &amp;lt;chr&amp;gt;                   
##  1 0000-0002-3335-9488     
##  2 0000-0001-7793-0079     
##  3 0000-0003-0910-8475     
##  4 0000-0002-8384-3964     
##  5 0000-0002-1963-4092     
##  6 0000-0003-3628-444X     
##  7 0000-0003-0079-9695     
##  8 0000-0002-4216-1107     
##  9 0000-0001-9684-8847     
## 10 0000-0002-0888-1274     
## # ... with 13 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we have the unique ids we can use &lt;code&gt;dplyr::left_join()&lt;/code&gt; to create a single data frame. If table joins are new to you in R try &lt;a href=&#34;http://r4ds.had.co.nz/relational-data.html&#34;&gt;this chapter of R for Data Science&lt;/a&gt;. Here we place our ids on the left hand side and the other tables on the right hand side will be joined where there is a shared &lt;code&gt;source_source_orcid_path&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;result &amp;lt;- left_join(ids, smith_employ, by = &amp;quot;source_source_orcid_path&amp;quot;) %&amp;gt;%
  left_join(., smith_education, by = &amp;quot;source_source_orcid_path&amp;quot;) %&amp;gt;%
  left_join(., smith_keywords, by = &amp;quot;source_source_orcid_path&amp;quot;)
result&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 23 x 4
##    source_source_orcid_path employment        education         keyword   
##    &amp;lt;chr&amp;gt;                    &amp;lt;list&amp;gt;            &amp;lt;list&amp;gt;            &amp;lt;list&amp;gt;    
##  1 0000-0002-3335-9488      &amp;lt;tibble [1 × 33]&amp;gt; &amp;lt;NULL&amp;gt;            &amp;lt;tibble […
##  2 0000-0001-7793-0079      &amp;lt;tibble [3 × 33]&amp;gt; &amp;lt;tibble [1 × 30]&amp;gt; &amp;lt;NULL&amp;gt;    
##  3 0000-0003-0910-8475      &amp;lt;tibble [1 × 33]&amp;gt; &amp;lt;tibble [2 × 30]&amp;gt; &amp;lt;NULL&amp;gt;    
##  4 0000-0002-8384-3964      &amp;lt;tibble [1 × 33]&amp;gt; &amp;lt;tibble [1 × 30]&amp;gt; &amp;lt;NULL&amp;gt;    
##  5 0000-0002-1963-4092      &amp;lt;tibble [1 × 33]&amp;gt; &amp;lt;tibble [1 × 30]&amp;gt; &amp;lt;tibble […
##  6 0000-0003-3628-444X      &amp;lt;tibble [1 × 33]&amp;gt; &amp;lt;tibble [1 × 30]&amp;gt; &amp;lt;NULL&amp;gt;    
##  7 0000-0003-0079-9695      &amp;lt;tibble [1 × 33]&amp;gt; &amp;lt;NULL&amp;gt;            &amp;lt;tibble […
##  8 0000-0002-4216-1107      &amp;lt;tibble [1 × 33]&amp;gt; &amp;lt;tibble [4 × 30]&amp;gt; &amp;lt;NULL&amp;gt;    
##  9 0000-0001-9684-8847      &amp;lt;tibble [1 × 33]&amp;gt; &amp;lt;tibble [1 × 30]&amp;gt; &amp;lt;NULL&amp;gt;    
## 10 0000-0002-0888-1274      &amp;lt;tibble [1 × 33]&amp;gt; &amp;lt;tibble [2 × 30]&amp;gt; &amp;lt;NULL&amp;gt;    
## # ... with 13 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, we now have a data.frame that consists of list columns containing tibbles. Note that we have some NULLs where there is no data for a particular category for that ID.&lt;/p&gt;
&lt;p&gt;If you are new to list columns, or &lt;code&gt;purrr&lt;/code&gt; in general, a great place to start is &lt;a href=&#34;https://jennybc.github.io/purrr-tutorial/ls13_list-columns.html&#34;&gt;Jenny Bryan’s purrr tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One of the great features of list columns is that we can search across them and add new values based on the matches. What we want to do now is to find John Smiths where the word University appears in their employment and their country is the US. We will carry out the search using &lt;code&gt;stringr::str_detect&lt;/code&gt; which will return a logical value. We use &lt;code&gt;map&lt;/code&gt; to map over the data and place this inside mutate to add a new column to the data frame. The code is a little more complicated than we might like because the use of map returns a vector of logical values. We use &lt;code&gt;map_lgl&lt;/code&gt; and &lt;code&gt;any&lt;/code&gt; to reduce this to a single TRUE/FALSE value. We then filter the data to those cases where both university and country are TRUE.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;result %&amp;gt;% 
  mutate(university = map(employment, str_detect, &amp;quot;University&amp;quot;), 
        university = map_lgl(university, any)) %&amp;gt;% 
  mutate(country = map(employment, str_detect, &amp;quot;US&amp;quot;), 
         country = map_lgl(country, any)) %&amp;gt;% 
  filter(university == TRUE &amp;amp; country == TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##   source_source_orcid_path employment education keyword university country
##   &amp;lt;chr&amp;gt;                    &amp;lt;list&amp;gt;     &amp;lt;list&amp;gt;    &amp;lt;list&amp;gt;  &amp;lt;lgl&amp;gt;      &amp;lt;lgl&amp;gt;  
## 1 0000-0003-0910-8475      &amp;lt;tibble [… &amp;lt;tibble … &amp;lt;NULL&amp;gt;  TRUE       TRUE   
## 2 0000-0002-4216-1107      &amp;lt;tibble [… &amp;lt;tibble … &amp;lt;NULL&amp;gt;  TRUE       TRUE   
## 3 0000-0001-9684-8847      &amp;lt;tibble [… &amp;lt;tibble … &amp;lt;NULL&amp;gt;  TRUE       TRUE   
## 4 0000-0002-0888-1274      &amp;lt;tibble [… &amp;lt;tibble … &amp;lt;NULL&amp;gt;  TRUE       TRUE   
## 5 0000-0003-1545-5078      &amp;lt;tibble [… &amp;lt;tibble … &amp;lt;NULL&amp;gt;  TRUE       TRUE   
## 6 0000-0003-1149-0562      &amp;lt;tibble [… &amp;lt;tibble … &amp;lt;tibbl… TRUE       TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, we now have a data frame where we know that the John Smiths have University somewhere in their employment and that US also appears. Note that the &lt;code&gt;any&lt;/code&gt; function can take &lt;code&gt;na.rm = TRUE&lt;/code&gt; as an argument. We are getting closer.&lt;/p&gt;
&lt;p&gt;We can do the above without using map at all because &lt;code&gt;str_detect&lt;/code&gt; will coerce columns consisting of a list of tibbles to vectors if it can. However, this will generate a warning that &lt;code&gt;argument is not an atomic vector; coercing&lt;/code&gt; so expect to see that a lot. This code is easier to read than that above but suggests a need for some more work.&lt;/p&gt;
&lt;p&gt;In this case we will also narrow down the data by searching for a keyword associated with an author.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;out &amp;lt;- result %&amp;gt;% 
  mutate(university = str_detect(employment, &amp;quot;University&amp;quot;)) %&amp;gt;% 
  mutate(country = str_detect(employment, &amp;quot;US&amp;quot;)) %&amp;gt;% 
  mutate(term = str_detect(keyword, &amp;quot;Infrared transmission&amp;quot;)) %&amp;gt;% 
  filter(university == TRUE &amp;amp; country == TRUE &amp;amp; term == TRUE)
 out&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 7
##   source_source_or… employment  education keyword university country term 
##   &amp;lt;chr&amp;gt;             &amp;lt;list&amp;gt;      &amp;lt;list&amp;gt;    &amp;lt;list&amp;gt;  &amp;lt;lgl&amp;gt;      &amp;lt;lgl&amp;gt;   &amp;lt;lgl&amp;gt;
## 1 0000-0003-1149-0… &amp;lt;tibble [5… &amp;lt;tibble … &amp;lt;tibbl… TRUE       TRUE    TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have now reduced our original 81 john smiths to 1 who has a record of being at a University in the US who is interested in Infrared Transmission. If we wished to we can unnest the columns to inspect as we go.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;out %&amp;gt;% unnest(employment)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 37
##   source_source_or… university country term  department_name role_title   
##   &amp;lt;chr&amp;gt;             &amp;lt;lgl&amp;gt;      &amp;lt;lgl&amp;gt;   &amp;lt;lgl&amp;gt; &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;        
## 1 0000-0003-1149-0… TRUE       TRUE    TRUE  Physics         Course Coord…
## 2 0000-0003-1149-0… TRUE       TRUE    TRUE  &amp;lt;NA&amp;gt;            President    
## 3 0000-0003-1149-0… TRUE       TRUE    TRUE  &amp;lt;NA&amp;gt;            Chief Techno…
## 4 0000-0003-1149-0… TRUE       TRUE    TRUE  &amp;lt;NA&amp;gt;            Director of …
## 5 0000-0003-1149-0… TRUE       TRUE    TRUE  &amp;lt;NA&amp;gt;            Senior Scien…
## # ... with 31 more variables: start_date &amp;lt;lgl&amp;gt;, end_date &amp;lt;lgl&amp;gt;,
## #   visibility &amp;lt;chr&amp;gt;, put_code &amp;lt;int&amp;gt;, path &amp;lt;chr&amp;gt;,
## #   created_date_value &amp;lt;dbl&amp;gt;, last_modified_date_value &amp;lt;dbl&amp;gt;,
## #   source_source_client_id &amp;lt;lgl&amp;gt;, source_source_orcid_uri &amp;lt;chr&amp;gt;,
## #   source_source_orcid_host &amp;lt;chr&amp;gt;, source_source_name_value &amp;lt;chr&amp;gt;,
## #   organization_name &amp;lt;chr&amp;gt;, organization_address_city &amp;lt;chr&amp;gt;,
## #   organization_address_region &amp;lt;chr&amp;gt;, organization_address_country &amp;lt;chr&amp;gt;,
## #   organization_disambiguated_organization_disambiguated_organization_identifier &amp;lt;chr&amp;gt;,
## #   organization_disambiguated_organization_disambiguation_source &amp;lt;chr&amp;gt;,
## #   start_date_day &amp;lt;lgl&amp;gt;, start_date_year_value &amp;lt;chr&amp;gt;,
## #   start_date_month_value &amp;lt;chr&amp;gt;, end_date_month &amp;lt;lgl&amp;gt;,
## #   end_date_day &amp;lt;lgl&amp;gt;, end_date_year_value &amp;lt;chr&amp;gt;,
## #   start_date_day_value &amp;lt;chr&amp;gt;,
## #   organization_disambiguated_organization &amp;lt;lgl&amp;gt;,
## #   source_source_orcid &amp;lt;lgl&amp;gt;, source_source_client_id_uri &amp;lt;chr&amp;gt;,
## #   source_source_client_id_path &amp;lt;chr&amp;gt;,
## #   source_source_client_id_host &amp;lt;chr&amp;gt;, end_date_month_value &amp;lt;chr&amp;gt;,
## #   end_date_day_value &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One gotcha to be aware of is that if we try and unnest a column along with the rest of the columns we will get &lt;code&gt;Error: All nested columns must have the same number of elements&lt;/code&gt;. In addition if we try and unnest a column containing NA or NULL or a literal NULL we get an error. The solution is to use &lt;code&gt;tidyr::drop_na()&lt;/code&gt;.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;result %&amp;gt;% 
  select(source_source_orcid_path, employment) %&amp;gt;% 
  drop_na(employment) %&amp;gt;%
  unnest() %&amp;gt;% 
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 34
##   source_source_orc… department_name   role_title      start_date end_date
##   &amp;lt;chr&amp;gt;              &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;           &amp;lt;lgl&amp;gt;      &amp;lt;lgl&amp;gt;   
## 1 0000-0002-3335-94… Civil Engineering Associate Prof… NA         NA      
## 2 0000-0001-7793-00… &amp;lt;NA&amp;gt;              Research Offic… NA         NA      
## 3 0000-0001-7793-00… &amp;lt;NA&amp;gt;              Regional Devel… NA         NA      
## 4 0000-0001-7793-00… &amp;lt;NA&amp;gt;              Barham Distric… NA         NA      
## 5 0000-0003-0910-84… Division of Plan… Graduate Resea… NA         NA      
## 6 0000-0002-8384-39… &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;            NA         NA      
## # ... with 29 more variables: visibility &amp;lt;chr&amp;gt;, put_code &amp;lt;int&amp;gt;,
## #   path &amp;lt;chr&amp;gt;, created_date_value &amp;lt;dbl&amp;gt;, last_modified_date_value &amp;lt;dbl&amp;gt;,
## #   source_source_client_id &amp;lt;lgl&amp;gt;, source_source_orcid_uri &amp;lt;chr&amp;gt;,
## #   source_source_orcid_host &amp;lt;chr&amp;gt;, source_source_name_value &amp;lt;chr&amp;gt;,
## #   organization_name &amp;lt;chr&amp;gt;, organization_address_city &amp;lt;chr&amp;gt;,
## #   organization_address_region &amp;lt;chr&amp;gt;, organization_address_country &amp;lt;chr&amp;gt;,
## #   organization_disambiguated_organization_disambiguated_organization_identifier &amp;lt;chr&amp;gt;,
## #   organization_disambiguated_organization_disambiguation_source &amp;lt;chr&amp;gt;,
## #   start_date_day &amp;lt;lgl&amp;gt;, start_date_year_value &amp;lt;chr&amp;gt;,
## #   start_date_month_value &amp;lt;chr&amp;gt;, end_date_month &amp;lt;lgl&amp;gt;,
## #   end_date_day &amp;lt;lgl&amp;gt;, end_date_year_value &amp;lt;chr&amp;gt;,
## #   start_date_day_value &amp;lt;chr&amp;gt;,
## #   organization_disambiguated_organization &amp;lt;lgl&amp;gt;,
## #   source_source_orcid &amp;lt;lgl&amp;gt;, source_source_client_id_uri &amp;lt;chr&amp;gt;,
## #   source_source_client_id_path &amp;lt;chr&amp;gt;,
## #   source_source_client_id_host &amp;lt;chr&amp;gt;, end_date_month_value &amp;lt;chr&amp;gt;,
## #   end_date_day_value &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So list columns containing data frames are great for creating uniform data frames and for purposes such as searching across columns. They are also, more commonly, good for running models as described &lt;a href=&#34;http://ijlyttle.github.io/isugg_purrr/presentation.html#(1)&#34;&gt;here&lt;/a&gt;. However, they can take a bit of getting used to.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;retrieving-publication-meta-data-with-rcrossref&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Retrieving Publication Meta data with rcrossref&lt;/h2&gt;
&lt;p&gt;When we have identified the ORCID IDs that we want the logical next step is to retrieve publications. This is a big issue for a project I am working on in Kenya where we are working on the national research permit system. The idea we have is that we can use ORCID to pull back publications from researchers who at one time or another have received a permit for biodiversity related research. That should allow us to start building up an electronic repository of publications about biodiversity research in Kenya that can be made available to the public. Because ORCID profiles can be automatically updated (through services such as Crossref) we should be able to automate updating research publications without bothering researchers for copies of their publications.&lt;/p&gt;
&lt;p&gt;We will work with a sample of ORCID IDs for 61 researchers who have worked in Kenya at some point or another. What we want to do is to retrieve their publications using the &lt;code&gt;rorcid::works&lt;/code&gt;. We will use &lt;code&gt;map&lt;/code&gt; again to send each ORCID id to the call to works. We will also add names to the list that is returned using &lt;code&gt;set_names&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kenya_works &amp;lt;- map(kenya_orcid$orcid_identifier_path, rorcid::works) %&amp;gt;% 
  set_names(., nm = kenya_orcid$orcid_identifier_path) 

names(kenya_works[1:5])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;0000-0001-5861-023X&amp;quot; &amp;quot;0000-0001-6916-0000&amp;quot; &amp;quot;0000-0002-4640-8760&amp;quot;
## [4] &amp;quot;0000-0003-0576-8935&amp;quot; &amp;quot;0000-0002-3077-7422&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What we get back from this is a named list containing data frames where the input ORCID identifier is the name for each list item. We can see this from a quick look using &lt;code&gt;str()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(kenya_works[1:5], max.level = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 5
##  $ 0000-0001-5861-023X:Classes &amp;#39;tbl_df&amp;#39;, &amp;#39;tbl&amp;#39;, &amp;#39;works&amp;#39; and &amp;#39;data.frame&amp;#39;:    20 obs. of  21 variables:
##   ..- attr(*, &amp;quot;orcid&amp;quot;)= chr &amp;quot;0000-0001-5861-023X&amp;quot;
##  $ 0000-0001-6916-0000:Classes &amp;#39;tbl_df&amp;#39;, &amp;#39;tbl&amp;#39;, &amp;#39;works&amp;#39; and &amp;#39;data.frame&amp;#39;:    103 obs. of  23 variables:
##   ..- attr(*, &amp;quot;orcid&amp;quot;)= chr &amp;quot;0000-0001-6916-0000&amp;quot;
##  $ 0000-0002-4640-8760:Classes &amp;#39;tbl_df&amp;#39;, &amp;#39;tbl&amp;#39;, &amp;#39;works&amp;#39; and &amp;#39;data.frame&amp;#39;:    0 obs. of  0 variables
##   ..- attr(*, &amp;quot;orcid&amp;quot;)= chr &amp;quot;0000-0002-4640-8760&amp;quot;
##  $ 0000-0003-0576-8935:Classes &amp;#39;tbl_df&amp;#39;, &amp;#39;tbl&amp;#39;, &amp;#39;works&amp;#39; and &amp;#39;data.frame&amp;#39;:    0 obs. of  0 variables
##   ..- attr(*, &amp;quot;orcid&amp;quot;)= chr &amp;quot;0000-0003-0576-8935&amp;quot;
##  $ 0000-0002-3077-7422:Classes &amp;#39;tbl_df&amp;#39;, &amp;#39;tbl&amp;#39;, &amp;#39;works&amp;#39; and &amp;#39;data.frame&amp;#39;:    14 obs. of  21 variables:
##   ..- attr(*, &amp;quot;orcid&amp;quot;)= chr &amp;quot;0000-0002-3077-7422&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now want to convert the list of data frames to a single data frame but in doing so we want to pass the input ORCID ID from the name of the list into a column in the output. The reason for this is that the return from ORCID does not contain the ORCID ID we sent to the API but a range of ORCIDs that are the source for the works record. We need to add the ORCID ID for the person at the same time as we convert to one data frame. One way to do this is to use &lt;code&gt;map2_df&lt;/code&gt; from &lt;code&gt;purrr&lt;/code&gt;. This will map over &lt;code&gt;kenya_works&lt;/code&gt; and the names at the same time. Mutate then adds a column containing the names (.y) as orcid_id.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pubs &amp;lt;- kenya_works %&amp;gt;%
  map2_df(., names(kenya_works), ~ mutate(.x, orcid_id = .y)) %&amp;gt;% 
  janitor::clean_names()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to do the same thing with less typing is to use the newer &lt;code&gt;purrr::imap&lt;/code&gt; function which is a shorthand for &lt;code&gt;map2_df(x, names(x), ...)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pubs &amp;lt;- kenya_works %&amp;gt;% 
  imap_dfr(~mutate(.x, orcid_id = .y)) %&amp;gt;% 
  janitor::clean_names()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a single data frame with the publications that keeps the orcid_id as the key. Let’s take a look at who has the most works.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pubs %&amp;gt;% count(orcid_id, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 36 x 2
##    orcid_id                n
##    &amp;lt;chr&amp;gt;               &amp;lt;int&amp;gt;
##  1 0000-0001-7513-0887   125
##  2 0000-0001-6916-0000   103
##  3 0000-0002-2146-5726   100
##  4 0000-0002-7793-8625   100
##  5 0000-0002-3958-0343    64
##  6 0000-0002-1921-0724    51
##  7 0000-0002-7486-4763    44
##  8 0000-0003-4024-0976    44
##  9 0000-0003-4864-5150    42
## 10 0000-0002-0123-8497    30
## # ... with 26 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can take a look at the top result based on the count of records for the ORCID ID. Note that the count above is a count of entries linked to the ORCID ID and does not necessarily add up to a count of publications (it actually over counts). One of the top researchers is Daniel Masiga who has been working on Leishmaniasis in Baringo and Nakuru countries in Kenya. Let’s take a look at his public profile with &lt;code&gt;browse&lt;/code&gt; or by opening the permanent link to the profile at &lt;a href=&#34;https://orcid.org/0000-0001-7513-0887&#34; class=&#34;uri&#34;&gt;https://orcid.org/0000-0001-7513-0887&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;browse(&amp;quot;0000-0001-7513-0887&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the titles of works that include reference to Kenya.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pubs %&amp;gt;% mutate(kenya = str_detect(title_title_value, pattern = &amp;quot;Kenya&amp;quot;)) %&amp;gt;% 
  filter(kenya == TRUE) %&amp;gt;% 
  select(title_title_value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 47 x 1
##    title_title_value                                                      
##    &amp;lt;chr&amp;gt;                                                                  
##  1 Size-dependent distribution and feeding habits of Terebralia palustris…
##  2 Spatial diversity of nematode and copepod genera of the coral degradat…
##  3 Nematode community structure along the continental slope off the Kenya…
##  4 New Desmodoridae (Nematoda: Desmodoroidea): three new species from Cer…
##  5 Papillonema danieli gen. et sp. n. and Papillonema clavatum (Gerlach, …
##  6 Unraveling Host-Vector-Arbovirus Interactions by Two-Gene High Resolut…
##  7 Unraveling host-vector-arbovirus interactions by two-gene high resolut…
##  8 Blood meal analysis and virus detection in blood-fed mosquitoes collec…
##  9 Blood meal analysis and virus detection in blood-fed mosquitoes collec…
## 10 High-resolution melting analysis reveals low Plasmodium parasitaemia i…
## # ... with 37 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note here that we have some duplicated entries (possibly coming into the profile from different sources?) that may need investigating.&lt;/p&gt;
&lt;p&gt;What we will normally want from this table will be the dois where available. We can then pass the dois to other services such as Crossref using &lt;code&gt;rcrossref&lt;/code&gt; to retrieve publication information.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;accessing-the-doi-field.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Accessing the DOI field.&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;doi &amp;lt;- pubs %&amp;gt;%
  select(external_ids_external_id)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we take a look at the doi field we will see that we have a lot of list() and NULL items as well as data.frames.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(doi[15:25,])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Classes &amp;#39;tbl_df&amp;#39;, &amp;#39;tbl&amp;#39; and &amp;#39;data.frame&amp;#39;:    11 obs. of  1 variable:
##  $ external_ids_external_id:List of 11
##   ..$ : list()
##   ..$ : list()
##   ..$ : list()
##   ..$ : list()
##   ..$ : list()
##   ..$ : list()
##   ..$ :&amp;#39;data.frame&amp;#39;: 1 obs. of  4 variables:
##   .. ..$ external-id-type        : chr &amp;quot;doi&amp;quot;
##   .. ..$ external-id-value       : chr &amp;quot;10.1242/jeb.176537&amp;quot;
##   .. ..$ external-id-relationship: chr &amp;quot;SELF&amp;quot;
##   .. ..$ external-id-url.value   : chr &amp;quot;https://doi.org/10.1242/jeb.176537&amp;quot;
##   ..$ :&amp;#39;data.frame&amp;#39;: 1 obs. of  4 variables:
##   .. ..$ external-id-type        : chr &amp;quot;doi&amp;quot;
##   .. ..$ external-id-value       : chr &amp;quot;10.7554/eLife.29053&amp;quot;
##   .. ..$ external-id-relationship: chr &amp;quot;SELF&amp;quot;
##   .. ..$ external-id-url.value   : chr &amp;quot;https://doi.org/10.7554/eLife.29053&amp;quot;
##   ..$ :&amp;#39;data.frame&amp;#39;: 1 obs. of  4 variables:
##   .. ..$ external-id-type        : chr &amp;quot;doi&amp;quot;
##   .. ..$ external-id-value       : chr &amp;quot;10.1242/jeb.171926&amp;quot;
##   .. ..$ external-id-relationship: chr &amp;quot;SELF&amp;quot;
##   .. ..$ external-id-url.value   : chr &amp;quot;https://doi.org/10.1242/jeb.171926&amp;quot;
##   ..$ :&amp;#39;data.frame&amp;#39;: 1 obs. of  4 variables:
##   .. ..$ external-id-type        : chr &amp;quot;doi&amp;quot;
##   .. ..$ external-id-value       : chr &amp;quot;10.1515/9783110548877-003&amp;quot;
##   .. ..$ external-id-url         : logi NA
##   .. ..$ external-id-relationship: chr &amp;quot;SELF&amp;quot;
##   ..$ :&amp;#39;data.frame&amp;#39;: 1 obs. of  4 variables:
##   .. ..$ external-id-type        : chr &amp;quot;doi&amp;quot;
##   .. ..$ external-id-value       : chr &amp;quot;10.1109/biocas.2016.7833768&amp;quot;
##   .. ..$ external-id-url         : logi NA
##   .. ..$ external-id-relationship: chr &amp;quot;SELF&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first thing we need to do is to get rid of the NULL and the empty list() entries. Normally we can use &lt;code&gt;purrr::compact()&lt;/code&gt; directly to do this but in this case the NULLs are inside the list objects, so we call compact inside map, we then bind the list of data frames using &lt;code&gt;map_df&lt;/code&gt; and a call to &lt;code&gt;bind_rows&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;doi &amp;lt;- doi %&amp;gt;%
  map(., compact) %&amp;gt;% 
  map_df(bind_rows) %&amp;gt;% 
  janitor::clean_names()

doi[1:5,] %&amp;gt;%
  select(1:3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   external_id_type           external_id_value external_id_relationship
## 1              doi          10.1242/jeb.176537                     SELF
## 2              doi         10.7554/eLife.29053                     SELF
## 3              doi          10.1242/jeb.171926                     SELF
## 4              doi   10.1515/9783110548877-003                     SELF
## 5              doi 10.1109/biocas.2016.7833768                     SELF&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is worth noting that the external_id_type column contains a variety of different kind of identifiers that we might want to explore (such as isbn and issn etc).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;doi %&amp;gt;% 
  group_by(external_id_type) %&amp;gt;%
  count()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
## # Groups:   external_id_type [10]
##    external_id_type     n
##    &amp;lt;chr&amp;gt;            &amp;lt;int&amp;gt;
##  1 arxiv               47
##  2 doi                690
##  3 eid                283
##  4 isbn                 1
##  5 issn                74
##  6 other-id            82
##  7 pmc                 57
##  8 pmid               140
##  9 source-work-id      41
## 10 wosuid              96&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will just filter the data to the dois and then pass them on to rcrossref to retrieve the publication meta data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;doi &amp;lt;- doi %&amp;gt;% filter(external_id_type == &amp;quot;doi&amp;quot;) %&amp;gt;% 
  select(external_id_value)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will need to &lt;code&gt;install.packages(&amp;quot;rcrossref&amp;quot;)&lt;/code&gt; and load the library to generate the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rcrossref)
crossref_kenya &amp;lt;- rcrossref::cr_works(doi$external_id_value)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As the function churns through the 690 dois warning messages will pop up with things like &lt;code&gt;404: Resource not found. - (115.001086)404&lt;/code&gt;. I got 9 of these on this test. We can easily access this data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;crossref_kenya &amp;lt;- crossref_kenya$data %&amp;gt;% 
  janitor::clean_names()
crossref_kenya&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 681 x 34
##    alternative_id  container_title   created deposited doi   indexed issn 
##    &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;
##  1 10.1242/jeb.17… The Journal of E… 2018-0… 2018-07-… 10.1… 2018-0… 0022…
##  2 10.7554/eLife.… eLife             2018-0… 2018-04-… 10.7… 2018-0… 2050…
##  3 10.1242/jeb.17… The Journal of E… 2017-1… 2018-02-… 10.1… 2018-0… 0022…
##  4 &amp;lt;NA&amp;gt;            Bild - Ton - Rhy… 2017-0… 2017-04-… 10.1… 2018-0… &amp;lt;NA&amp;gt; 
##  5 &amp;lt;NA&amp;gt;            2016 IEEE Biomed… 2017-0… 2017-12-… 10.1… 2018-0… &amp;lt;NA&amp;gt; 
##  6 &amp;lt;NA&amp;gt;            PLOS Biology      2016-0… 2017-06-… 10.1… 2018-0… 1545…
##  7 &amp;lt;NA&amp;gt;            Frontiers in Beh… 2016-0… 2017-06-… 10.3… 2018-0… 1662…
##  8 &amp;lt;NA&amp;gt;            Bat Bioacoustics… 2016-0… 2017-06-… 10.1… 2018-0… 0947…
##  9 &amp;lt;NA&amp;gt;            Frontiers in Phy… 2014-0… 2015-02-… 10.3… 2018-0… 1664…
## 10 &amp;lt;NA&amp;gt;            Frontiers in Psy… 2014-0… 2017-06-… 10.3… 2018-0… 1664…
## # ... with 671 more rows, and 27 more variables: issued &amp;lt;chr&amp;gt;,
## #   license_date &amp;lt;chr&amp;gt;, license_url &amp;lt;chr&amp;gt;, license_delay_in_days &amp;lt;chr&amp;gt;,
## #   license_content_version &amp;lt;chr&amp;gt;, member &amp;lt;chr&amp;gt;, page &amp;lt;chr&amp;gt;, prefix &amp;lt;chr&amp;gt;,
## #   publisher &amp;lt;chr&amp;gt;, reference_count &amp;lt;chr&amp;gt;, score &amp;lt;chr&amp;gt;, source &amp;lt;chr&amp;gt;,
## #   subject &amp;lt;chr&amp;gt;, title &amp;lt;chr&amp;gt;, type &amp;lt;chr&amp;gt;, url &amp;lt;chr&amp;gt;, author &amp;lt;list&amp;gt;,
## #   funder &amp;lt;list&amp;gt;, link &amp;lt;list&amp;gt;, archive &amp;lt;chr&amp;gt;, volume &amp;lt;chr&amp;gt;,
## #   abstract &amp;lt;chr&amp;gt;, issue &amp;lt;chr&amp;gt;, isbn &amp;lt;chr&amp;gt;, update_policy &amp;lt;chr&amp;gt;,
## #   assertion &amp;lt;list&amp;gt;, subtitle &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the journals&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;crossref_kenya %&amp;gt;% 
  count(container_title, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 293 x 2
##    container_title                                                       n
##    &amp;lt;chr&amp;gt;                                                             &amp;lt;int&amp;gt;
##  1 PLoS ONE                                                             22
##  2 Physical Review D                                                    21
##  3 The Journal of the Acoustical Society of America                     16
##  4 Transportation Research Record: Journal of the Transportation Re…    15
##  5 PLoS Neglected Tropical Diseases                                     14
##  6 PLOS ONE                                                             10
##  7 Leukemia                                                              9
##  8 The Journal of Immunology                                             9
##  9 Journal of Comparative Physiology A                                   8
## 10 Accident Analysis &amp;amp; Prevention                                        7
## # ... with 283 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are quite a number of things that we could do from here such as attempting to retrieve full text links, abstracts, citations or text mining the available data. For example we could retrieve full text data from PLOS using packages such as &lt;code&gt;rplos&lt;/code&gt;. For the moment, we have covered a lot of ground in using rorcid and bridging across to other data sources such as rcrossref.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round Up&lt;/h2&gt;
&lt;p&gt;ORCID is an increasingly important data service for research funding organisations, university administrators, publishers and researchers interested in understanding trends in science and technology. The &lt;code&gt;rorcid&lt;/code&gt; package provides a straightforward and easy way to access ORCID data in R while Python users can try &lt;a href=&#34;https://github.com/ORCID/python-orcid&#34;&gt;python-orcid&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This article has walked through the basics of searching using rorcid and approaches to filtering the data. As we have seen one reality of working with names is dealing with homonyms or distinct persons who share the same name (also known as lumps). One challenge with the data returned by ORCID is that the completeness of different data fields can vary wildly. We addressed this problem by creating a single data frame consisting of list columns containing data frames and then searching across them. While there is room for improvement in this approach it
illustrates the power of list columns.&lt;/p&gt;
&lt;p&gt;We finished off by retrieving publication data from a sample of researchers profiles for biodiversity research in Kenya. We then bridged across to the rcrossref package to pull back the publication data.&lt;/p&gt;
&lt;p&gt;Many thanks to Scott Chamberlain for his hard work on the rorcid package! As always corrections or suggestions are welcome.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-R-rorcid&#34;&gt;
&lt;p&gt;Chamberlain, Scott. 2018. &lt;em&gt;Rorcid: Interface to the ’Orcid.org’ ’Api’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=rorcid&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=rorcid&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-rcrossref&#34;&gt;
&lt;p&gt;Chamberlain, Scott, Carl Boettiger, Ted Hart, and Karthik Ram. 2018. &lt;em&gt;Rcrossref: Client for Various ’Crossref’ ’Apis’&lt;/em&gt;. &lt;a href=&#34;https://github.com/ropensci/rcrossref&#34; class=&#34;uri&#34;&gt;https://github.com/ropensci/rcrossref&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Fegley_2013&#34;&gt;
&lt;p&gt;Fegley, Brent D., and Vetle I. Torvik. 2013. “Has Large-Scale Named-Entity Network Analysis Been Resting on a Flawed Assumption?” Edited by Marco Tomassini. &lt;em&gt;PLoS ONE&lt;/em&gt; 8 (7). Public Library of Science (PLoS): e70299. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0070299&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0070299&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-janitor&#34;&gt;
&lt;p&gt;Firke, Sam. 2018. &lt;em&gt;Janitor: Simple Tools for Examining and Cleaning Dirty Data&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=janitor&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=janitor&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Haak_2012&#34;&gt;
&lt;p&gt;Haak, Laurel L., Martin Fenner, Laura Paglione, Ed Pentz, and Howard Ratner. 2012. “ORCID: A System to Uniquely Identify Researchers.” &lt;em&gt;Learned Publishing&lt;/em&gt; 25 (4). Wiley: 259–64. &lt;a href=&#34;https://doi.org/10.1087/20120404&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1087/20120404&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-purrr&#34;&gt;
&lt;p&gt;Henry, Lionel, and Hadley Wickham. 2017. &lt;em&gt;Purrr: Functional Programming Tools&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=purrr&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=purrr&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Meadows_2016&#34;&gt;
&lt;p&gt;Meadows, Alice. 2016. “Everything You Ever Wanted Know About ORCID: . . . But Were Afraid to Ask.” &lt;em&gt;College &amp;amp; Research Libraries News&lt;/em&gt; 77 (1). American Library Association: 23–30. &lt;a href=&#34;https://doi.org/10.5860/crln.77.1.9428&#34; class=&#34;uri&#34;&gt;https://doi.org/10.5860/crln.77.1.9428&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-usethis&#34;&gt;
&lt;p&gt;Wickham, Hadley, and Jennifer Bryan. 2018. &lt;em&gt;Usethis: Automate Package and Project Setup&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=usethis&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=usethis&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Youtie_2017&#34;&gt;
&lt;p&gt;Youtie, Jan, Stephen Carley, Alan L. Porter, and Philip Shapira. 2017. “Tracking Researchers and Their Outputs: New Insights from ORCIDs.” &lt;em&gt;Scientometrics&lt;/em&gt; 113 (1). Springer Nature: 437–53. &lt;a href=&#34;https://doi.org/10.1007/s11192-017-2473-0&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1007/s11192-017-2473-0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://members.orcid.org/api/tutorial/search-orcid-registry&#34; class=&#34;uri&#34;&gt;https://members.orcid.org/api/tutorial/search-orcid-registry&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt; This example applies code in &lt;a href=&#34;https://jennybc.github.io/purrr-tutorial/ls13_list-columns.html&#34;&gt;Jenny Bryans purrr tutorial&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Unnest does take an argument &lt;code&gt;.drop&lt;/code&gt; but I have failed to persuade that to work as I had hoped.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Separating and Trimming Messy Data the Tidy Way</title>
      <link>/dealing-with-concatenated-data-fields-in-r/</link>
      <pubDate>Mon, 18 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/dealing-with-concatenated-data-fields-in-r/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;When working with table data from the scientific or patent literature, it is extremely common to find that columns contain concatenated data. That is, they contain multiple entries with a semicolon as a separator. Data of this type is not tidy &lt;span class=&#34;citation&#34;&gt;(Wickham 2014)&lt;/span&gt;. What we commonly want to do is to separate the data out as the basis for counting. However, extra white space can have a major impact on any counts of this data if it is not recognised and dealt with. We will go through this step by step using a simple example and then scale up to a real world example.&lt;/p&gt;
&lt;p&gt;Here is a simple example of a table containing a column with concatenated data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
messy &amp;lt;- tibble::tibble(messy = c(&amp;quot;this is not the; messiest&amp;quot;, 
                                  &amp;quot;messy data column; in the world&amp;quot;, 
                                  &amp;quot;it&amp;#39;s just; a; tribute&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we try to count this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;messy %&amp;gt;% 
  count(messy) %&amp;gt;% 
  kable() %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
messy
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
it’s just; a; tribute
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
messy data column; in the world
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
this is not the; messiest
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We get three results when of course actually what we want is a count of the concatenated data points in the column. We can handle this easily with the &lt;code&gt;tidyr::separate_rows&lt;/code&gt; function in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;messy %&amp;gt;% 
  separate_rows(messy, sep = &amp;quot;;&amp;quot;) %&amp;gt;% 
  count(messy) %&amp;gt;% 
  kable() %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
messy
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
a
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
in the world
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
messiest
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
tribute
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
it’s just
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
messy data column
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
this is not the
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Simples… as our friendly local meerkat might say. But let’s look at what happens if we double up our entries and imagine that two different people had written out the same thing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;messy &amp;lt;- tibble::tibble(messy = c(&amp;quot;this is not the; messiest&amp;quot;, 
                                  &amp;quot;messy data column; in the world&amp;quot;, 
                                  &amp;quot;it&amp;#39;s just; a; tribute&amp;quot;, 
                                  &amp;quot; this is not the; messiest&amp;quot;, 
                                  &amp;quot; messy data column;  in the world&amp;quot;, 
                                  &amp;quot;it&amp;#39;s just; a; tribute&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s try again. We are innocently expecting a count of 2 for the repeated words and phrases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;messy %&amp;gt;% 
  separate_rows(messy, sep = &amp;quot;;&amp;quot;) %&amp;gt;% 
  count(messy) %&amp;gt;% 
  kable() %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
messy
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
in the world
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
a
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
in the world
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
messiest
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
messy data column
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
this is not the
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
tribute
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
it’s just
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
messy data column
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
this is not the
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This is not adding up correctly because while the two versions appear to be identical there are subtle differences… involving spaces introduced by our mysterious second person. This is a simple case so maybe you spotted them. When using R you will often want to try using &lt;code&gt;str()&lt;/code&gt;, the equivalent in a language such as Python, or stare very hard at the screen in Excel.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(messy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Classes &amp;#39;tbl_df&amp;#39;, &amp;#39;tbl&amp;#39; and &amp;#39;data.frame&amp;#39;:    6 obs. of  1 variable:
##  $ messy: chr  &amp;quot;this is not the; messiest&amp;quot; &amp;quot;messy data column; in the world&amp;quot; &amp;quot;it&amp;#39;s just; a; tribute&amp;quot; &amp;quot; this is not the; messiest&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s hard to read but we can see a white space at the start of &lt;code&gt;&amp;quot; this is not the messiest&amp;quot;&lt;/code&gt;. This though helps makes the point that when dealing with thousands of data points extra white space can be really hard to spot in R, Excel or anything else.&lt;/p&gt;
&lt;p&gt;We can understand this more clearly by using a quick logical test in R to test whether two strings are identical or not.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;quot;this is messy&amp;quot; == &amp;quot; this is messy&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The reason for this is that in testing whether the strings are identical R (and anything else) will match all characters, including white space.&lt;/p&gt;
&lt;p&gt;If you work with text based data extra white space appears in the data all the time after separation, mainly in the form of leading white space as we will see below.&lt;/p&gt;
&lt;p&gt;The solution is simple, we trim the white space on both sides. In R we can do this using either the &lt;code&gt;stringr&lt;/code&gt; function &lt;code&gt;str_trim&lt;/code&gt; or the base R function &lt;code&gt;trimws&lt;/code&gt;. We’ll use &lt;code&gt;stringr&lt;/code&gt; here because it is a reminder of how useful this &lt;code&gt;tidyverse&lt;/code&gt; package is.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;messy %&amp;gt;% separate_rows(messy, sep = &amp;quot;;&amp;quot;) %&amp;gt;%
  mutate(messy = str_trim(messy, side = &amp;quot;both&amp;quot;)) %&amp;gt;% 
  count(messy) %&amp;gt;% 
  kable() %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
messy
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
a
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
in the world
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
it’s just
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
messiest
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
messy data column
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
this is not the
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
tribute
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We use a call to &lt;code&gt;dplyr::mutate&lt;/code&gt; and then a call to &lt;code&gt;stringr str_trim&lt;/code&gt; to trim the white space on both sides of the separated strings and then overwrite the column in place.&lt;/p&gt;
&lt;p&gt;Both &lt;code&gt;str_trim&lt;/code&gt; and the base R &lt;code&gt;trimws&lt;/code&gt; have arguments for where to trim white space. In the case of &lt;code&gt;stringr&lt;/code&gt; it is &lt;code&gt;side =&lt;/code&gt; and with &lt;code&gt;trimws&lt;/code&gt; it is &lt;code&gt;which =&lt;/code&gt;. As these are general functions there may be situations where you will want to trim either the leading (left) or the trailing (right) spaces. If you are working with metadata from the scientific literature (such as Web of Science or Crossref) or with patent data my recommendation is to always trim on both sides unless you have a good reason not to.&lt;/p&gt;
&lt;p&gt;We now have a piece of code that will work for just about anything where white space is left over. We normally want to turn that into a function that we can use over and over again. One reason the &lt;code&gt;tidyverse&lt;/code&gt; set of packages are so popular is that they are so easy to use. But, if we try and put the code above into a function it won’t work.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fun &amp;lt;- function(df, col, sep){
  df %&amp;gt;% tidyr::separate_rows(col, sep = sep) %&amp;gt;% 
    dplyr::mutate(col = stringr::str_trim(col, side = &amp;quot;both&amp;quot;)) %&amp;gt;% 
    dplyr::count(col)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fun(messy, messy, sep = &amp;quot;;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will get a message that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Error: &lt;code&gt;col&lt;/code&gt; must evaluate to column positions or names, not a list&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If we try quoting the “messy” col it appears to work but instead counts the number of entries. We can go around the houses… and go slightly bananas in the process… trying to fix this only to run into mysterious problem after problem. The reason for this is that &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;tidyr&lt;/code&gt; use non standard evaluation (tidy evaluation) with the result that R does not know how to evaluate it. We need to start getting to grips with tidy evaluation to get our code to work in a function. A whole bunch of very useful resources on that have been compiled by Mara Averick &lt;a href=&#34;https://maraaverick.rbind.io/2017/08/tidyeval-resource-roundup/&#34;&gt;here&lt;/a&gt;. One solution, bearing in mind that there may well be a better one, is this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;separate_rows_trim &amp;lt;- function(df, col, sep){
  col &amp;lt;- enquo(col)
  df %&amp;gt;% tidyr::separate_rows(!!col, sep = sep) %&amp;gt;% 
    dplyr::mutate(!!col := stringr::str_trim(!!col, side = &amp;quot;both&amp;quot;)) %&amp;gt;% 
    dplyr::count(!!col := !!col, sort = TRUE) %&amp;gt;% 
    tidyr::drop_na(!!col)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case we use bang bang &lt;code&gt;!!&lt;/code&gt; to tell R when to evaluate col with a bit of help from &lt;code&gt;:=&lt;/code&gt; from &lt;code&gt;rlang&lt;/code&gt;. To actually get to grips with tidy evaluation I recommend Mara Avericks compilation &lt;a href=&#34;https://maraaverick.rbind.io/2017/08/tidyeval-resource-roundup/&#34;&gt;here&lt;/a&gt;. For a much deeper dive and highly illuminating read try the &lt;a href=&#34;https://adv-r.hadley.nz/meta.html&#34;&gt;metaprogramming chapter of Hadley Wickham’s forthcoming 2nd edition of Advanced R&lt;/a&gt;. For the moment we can move on.&lt;/p&gt;
&lt;p&gt;Let’s try again.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;separate_rows_trim(messy, messy, sep = &amp;quot;;&amp;quot;) %&amp;gt;% 
  kable() %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
messy
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
a
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
in the world
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
it’s just
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
messiest
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
messy data column
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
this is not the
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
tribute
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We now have a reusable function.&lt;/p&gt;
&lt;p&gt;This toy example introduces the importance of trimming white space when working with data that has been separated out. Otherwise bad things will happen when you start to count. To finish off let’s use some real world data from a patent dataset to illustrate this.&lt;/p&gt;
&lt;div id=&#34;scaling-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Scaling Up&lt;/h3&gt;
&lt;p&gt;This article is part of work in progress on the &lt;a href=&#34;https://github.com/wipo-analytics&#34;&gt;WIPO Patent Analytics Handbook&lt;/a&gt;. Patent data is simultaneously really well organised and really messy… with many concatenated columns containing data of varying lengths. In addition a single data set will often compile records from different patent databases. This leads to the same problem we encountered above where a mysterious second person types exactly the same thing in a slightly different way. This is really common with names such as applicants or inventors.&lt;/p&gt;
&lt;p&gt;Here we will use the &lt;a href=&#34;https://poldham.github.io/drones/&#34;&gt;drones dataset&lt;/a&gt;, a new work in progress dataset of patent data involving drone technology. As it’s a big dataset we will just use the applicants field with 18,970 rows.&lt;/p&gt;
&lt;p&gt;If you would like to explore the drones dataset try this. First make sure you have &lt;code&gt;devtools&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;devtools&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then install from github with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;poldham/drones&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will use the applicants table. This contains a column called applicants_cleaned that I have previously mainly cleaned up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(drones)
applicants %&amp;gt;% 
  select(applicants_cleaned)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 18,970 x 1
##    applicants_cleaned                         
##    &amp;lt;chr&amp;gt;                                      
##  1 SHENZHEN HUBSAN TECHNOLOGY CO. LTD.        
##  2 Intel Corporation                          
##  3 YOKOGAWA ELECTRIC CORPORATION              
##  4 NETWORK PERFORMANCE RESEARCH GROUP LLC     
##  5 NETWORK PERFORMANCE RESEARCH GROUP LLC     
##  6 Choi Hae-Yong                              
##  7 Ziva Corporation                           
##  8 WiTricity Corporation                      
##  9 Thales                                     
## 10 International Business Machines Corporation
## # ... with 18,960 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can only see one case of a semi colon in this case but we can quickly get an idea of how many there are with &lt;code&gt;str_count&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;applicants %&amp;gt;% 
  select(applicants_cleaned) %&amp;gt;% 
  str_count(., pattern = &amp;quot;;&amp;quot;) %&amp;gt;% 
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
x
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5916
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Creating a count of the number of separators per record reveals that the maximum number of semicolons is 20.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;applicants %&amp;gt;% 
  select(applicants_cleaned) %&amp;gt;% 
  mutate(sepcount = str_count(applicants_cleaned, &amp;quot;;&amp;quot;)) %&amp;gt;% 
  drop_na(applicants_cleaned) %&amp;gt;% 
  filter(sepcount == max(sepcount)) %&amp;gt;% 
  select(sepcount) %&amp;gt;% 
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
sepcount
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let’s try counting the data up both ways to join them together. We’ll limit this to the top ten.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(drones)
df1 &amp;lt;- applicants %&amp;gt;% 
  separate_rows(applicants_cleaned, sep = &amp;quot;;&amp;quot;) %&amp;gt;%
  drop_na(applicants_cleaned) %&amp;gt;% 
  count(applicants_cleaned, sort = TRUE) %&amp;gt;% 
  rename(messy = n) %&amp;gt;% 
  .[1:10,] %&amp;gt;% 
  mutate(applicants_cleaned = str_trim(applicants_cleaned, side = &amp;quot;both&amp;quot;))

df2 &amp;lt;- separate_rows_trim(applicants, applicants_cleaned, sep = &amp;quot;;&amp;quot;) %&amp;gt;% 
  drop_na(applicants_cleaned) %&amp;gt;% 
  rename(tidy = n) %&amp;gt;% 
  .[1:10,]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we join these two tables together we will be able to calculate the differences between them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df3 &amp;lt;- merge(df1, df2, by = &amp;quot;applicants_cleaned&amp;quot;) %&amp;gt;% 
  arrange(desc(tidy)) %&amp;gt;%
  mutate(percent = (tidy - messy) / tidy * 100) %&amp;gt;% 
  mutate(percent = formatC(percent, digits = 2))

df3 %&amp;gt;% 
  kable(&amp;quot;html&amp;quot;, escape = F) %&amp;gt;% 
  kable_styling(position = &amp;quot;center&amp;quot;) %&amp;gt;% 
  column_spec(4, width = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
applicants_cleaned
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
messy
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
tidy
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
percent
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
QUALCOMM Incorporated
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
483
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
498
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 8; &#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Thales
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
322
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
382
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 8; &#34;&gt;
16
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
HON HAI PRECISION INDUSTRY CO LTD
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
345
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
345
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 8; &#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
QINGHUA UNIV
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
343
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
343
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 8; &#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Samsung Electronics Co. Ltd.
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
207
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
213
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 8; &#34;&gt;
2.8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
International Business Machines Corporation
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
184
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
193
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 8; &#34;&gt;
4.7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
THE BOEING COMPANY
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
177
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
181
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 8; &#34;&gt;
2.2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
GOOGLE INC.
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
165
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
167
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 8; &#34;&gt;
1.2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Elwha LLC
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
161
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
166
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 8; &#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SONY CORPORATION
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
144
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
148
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 8; &#34;&gt;
2.7
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can see from the percentage scores that there is significant variance in the counts, with a maximum of 16% variance in the case of Thales. The reason this matters whether using patent data or data from the scientific literature is that any counts that do not recognise the white space problem will be wrong… and generally quite seriously wrong. Typically with patent data the most observable change is movement in the top rankings. But where precision in counting is important, such as capturing all documents linked to a company in a highly competitive field, that can really really matter.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Round Up&lt;/h3&gt;
&lt;p&gt;When working with data from the scientific literature or patent data in spreadsheets or data frames we will always want to separate out the data in order to count it, whether in R, Python or using tools such as &lt;a href=&#34;https://wipo-analytics.github.io/open-refine.html&#34;&gt;Open Refine&lt;/a&gt;. The act of separating data onto new rows is however only one step with trimming white space a key step to arrive at accurate counts.&lt;/p&gt;
&lt;p&gt;So ends this episode of “fun with white space and semicolons”. Thanks for reading!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3 unnumbered&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-R-formattable&#34;&gt;
&lt;p&gt;Ren, Kun, and Kenton Russell. 2016. &lt;em&gt;Formattable: Create ’Formattable’ Data Structures&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=formattable&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=formattable&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Wickham_2014&#34;&gt;
&lt;p&gt;Wickham, Hadley. 2014. “Tidy Data.” &lt;em&gt;Journal of Statistical Software&lt;/em&gt; 59 (10). Foundation for Open Access Statistic. &lt;a href=&#34;https://doi.org/10.18637/jss.v059.i10&#34; class=&#34;uri&#34;&gt;https://doi.org/10.18637/jss.v059.i10&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34;&gt;
&lt;p&gt;———. 2017. &lt;em&gt;Tidyverse: Easily Install and Load the ’Tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-stringr&#34;&gt;
&lt;p&gt;———. 2018. &lt;em&gt;Stringr: Simple, Consistent Wrappers for Common String Operations&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=stringr&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=stringr&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-dplyr&#34;&gt;
&lt;p&gt;Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2018. &lt;em&gt;Dplyr: A Grammar of Data Manipulation&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=dplyr&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=dplyr&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyr&#34;&gt;
&lt;p&gt;Wickham, Hadley, and Lionel Henry. 2018. &lt;em&gt;Tidyr: Easily Tidy Data with ’Spread()’ and ’Gather()’ Functions&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyr&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=tidyr&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-kableExtra&#34;&gt;
&lt;p&gt;Zhu, Hao. 2018. &lt;em&gt;KableExtra: Construct Complex Table with ’Kable’ and Pipe Syntax&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=kableExtra&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=kableExtra&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>API Resources for the Scientific Literature in R and Python</title>
      <link>/api-resources-for-scientific-literature/</link>
      <pubDate>Mon, 11 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/api-resources-for-scientific-literature/</guid>
      <description>&lt;p&gt;This short post provides details on some of the main APIs (web services) that can be used to monitor and retrieve data from the scientific literature in either R or Python. We are using these packages and libraries as part of a GIZ supported project with the authorities in Kenya who are responsible for providing research permits. Kenya is famous for its biodiversity and the diversity of its communities. However, there is no single repository of publications arising from research in Kenya. We are looking to use APIs to automate retrieval of publications about Kenya and its biodiversity. Hopefully this should allow us to build an open access virtual repository of publications on Kenya to serve the needs of researchers and the wider community.&lt;/p&gt;
&lt;p&gt;We plan to use three main APIs for the Kenya project. There are many APIs out there but we will focus on those that aggregate data from different sources. I’ll add a few more that are interesting mainly for biodiversity topics.&lt;/p&gt;
&lt;div id=&#34;main-apis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Main APIs&lt;/h2&gt;
&lt;div id=&#34;crossref&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.crossref.org/&#34;&gt;Crossref&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Crossref provides access to metadata on over 96 million scientific publications. It is not a full text search engine although abstracts are increasingly available as are links to full text versions of articles (which may well be paywalled).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The Crossref API: &lt;a href=&#34;https://github.com/CrossRef/rest-api-doc&#34; class=&#34;uri&#34;&gt;https://github.com/CrossRef/rest-api-doc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Rcrossref: &lt;a href=&#34;https://github.com/ropensci/rcrossref&#34; class=&#34;uri&#34;&gt;https://github.com/ropensci/rcrossref&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Python: &lt;a href=&#34;https://pypi.org/project/habanero/&#34; class=&#34;uri&#34;&gt;https://pypi.org/project/habanero/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;For lovers of all things Ruby try the Serrano Ruby gem &lt;a href=&#34;https://github.com/sckott/serrano&#34; class=&#34;uri&#34;&gt;https://github.com/sckott/serrano&lt;/a&gt; and &lt;a href=&#34;https://www.rubydoc.info/gems/serrano&#34;&gt;rubydoc version&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The rcrossref, python and ruby wrappers were all created by Scott Chamberlain and collaborators at the fantastic &lt;a href=&#34;https://ropensci.org/&#34;&gt;ROpenSci&lt;/a&gt;. Note that searching on crossref is rather limited and so cannot really be used for statistical purposes (the search searches what they have available and that may be quite mixed) BUT crossref is still really useful. In particular it can be used to search for the names of researchers and to retrieve publication details or to enter a list of DOIs.&lt;/p&gt;
&lt;p&gt;A walkthrough on using rcrossref to access the scientific literature for Kenya is available &lt;a href=&#34;https://poldham.github.io/abs/crossref.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For text retrieval and text mining, the &lt;a href=&#34;https://github.com/ropensci/crminer&#34;&gt;crminer&lt;/a&gt; package by Scott Chamberlain is intended to facilitate access to full texts for text mining purposes from Crossref. You will also very probably want to check out Scott’s &lt;a href=&#34;https://github.com/ropensci/fulltext&#34;&gt;fulltext package&lt;/a&gt; for text retrieval from a range of different APIs including some of those listed here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;orcid&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://orcid.org/&#34;&gt;ORCID&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;ORCID provides persistent unique identifiers for researchers and access to their public profiles. Where a researcher publishes an article with a DOI that is covered by Crossref, that DOI should automatically (with luck) be added to the researcher’s public profile. Note that you can only access the parts of an ORCID profile that a researcher chooses to make public.&lt;/p&gt;
&lt;p&gt;An example of an ORCID public profile is mine: &lt;a href=&#34;https://orcid.org/0000-0002-1013-4390&#34; class=&#34;uri&#34;&gt;https://orcid.org/0000-0002-1013-4390&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lists of publications can be retrieved using the API and can therefore be used to automate the creation of a repository of publications for a country without needing to chase the researcher through email.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;ORCID API home page for creating an app: &lt;a href=&#34;https://orcid.org/organizations/integrators/API&#34; class=&#34;uri&#34;&gt;https://orcid.org/organizations/integrators/API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ORCID Python library: &lt;a href=&#34;https://github.com/ORCID/python-orcid&#34; class=&#34;uri&#34;&gt;https://github.com/ORCID/python-orcid&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ORCID R Package: &lt;a href=&#34;https://github.com/ropensci/rorcid&#34; class=&#34;uri&#34;&gt;https://github.com/ropensci/rorcid&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note that when using a remote server the OAuth process (using the rorcid package) can be difficult because the API triggers a browser login. A way around this needs to be found.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;core.ac.uk&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://core.ac.uk/&#34;&gt;core.ac.uk/&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Core is a full text database that aggregates scientific publications in open access repositories. It can be difficult to find due to the name. But it provides access to over 131 million open access articles. Taking Kenya as an example, a quick search for Kenya reveals 103,310 publications that contain Kenya somewhere in the text. The &lt;a href=&#34;https://core.ac.uk/services&#34;&gt;services page&lt;/a&gt; provides details of the web service, what you can do and how to get started. You will need a free API key from &lt;a href=&#34;https://core.ac.uk/api-keys/register&#34;&gt;here&lt;/a&gt;. Note the quotas and throttle accordingly.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Python notebook with examples: &lt;a href=&#34;https://github.com/oacore/or2016-api-demo&#34; class=&#34;uri&#34;&gt;https://github.com/oacore/or2016-api-demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;R Package rcoreaoa: &lt;a href=&#34;https://github.com/ropensci/rcoreoa&#34; class=&#34;uri&#34;&gt;https://github.com/ropensci/rcoreoa&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;other-apis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other APIs&lt;/h2&gt;
&lt;p&gt;The resources above should capture a lot. But here are some other major APIs that you may want to use.&lt;/p&gt;
&lt;div id=&#34;springer-biomed-central-api&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.biomedcentral.com/getpublished/indexing-archiving-and-access-to-data&#34;&gt;Springer BioMed Central API&lt;/a&gt;&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;BMC R package &lt;a href=&#34;https://github.com/ropensci/bmc&#34; class=&#34;uri&#34;&gt;https://github.com/ropensci/bmc&lt;/a&gt;. This package is not on CRAN. To install it use:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;devtools&amp;quot;)
devtools::install_github(&amp;quot;ropensci/bmc&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I couldn’t easily identify a Python library or gist. If you know of one please add to the comments below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ncbi-pubmed&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/&#34;&gt;NCBI PubMed&lt;/a&gt;&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The &lt;a href=&#34;https://github.com/ropensci/rentrez&#34;&gt;rentrez&lt;/a&gt; package and &lt;a href=&#34;https://ropensci.org/tutorials/rentrez_tutorial/&#34;&gt;walkthrough&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://github.com/cran/easyPubMed&#34;&gt;easyPubMed&lt;/a&gt; package in R:
See the walkthrough by &lt;a href=&#34;https://cran.r-project.org/web/packages/easyPubMed/vignettes/easyPM_vignette_html.html&#34;&gt;Daniel Fantini&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;For Python there is &lt;a href=&#34;https://pypi.org/project/pubmed-lookup/&#34;&gt;pubmed-lookup&lt;/a&gt; and a gist for searching PubMed with Biopython is &lt;a href=&#34;https://pypi.org/project/pubmed-lookup/&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;public-library-of-science&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.plos.org/&#34;&gt;Public Library of Science&lt;/a&gt;&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Rplos package &lt;a href=&#34;https://github.com/ropensci/rplos&#34; class=&#34;uri&#34;&gt;https://github.com/ropensci/rplos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;For Python a &lt;a href=&#34;https://gist.github.com/drewbuschhorn/1077318&#34;&gt;gist&lt;/a&gt; is available providing examples of the use of the sunburnt library&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One of my walkthroughs, now a bit old but still working, for rplos is available &lt;a href=&#34;https://www.pauloldham.net/rplos-walkthrough/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;biorxiv&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.biorxiv.org/&#34;&gt;bioRxiv&lt;/a&gt;&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;For R the &lt;a href=&#34;https://github.com/ropensci/fulltext&#34;&gt;fulltext package&lt;/a&gt; provides access to the texts of bioRxiv which has an RSS feed but does not appear to have an API.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I wasn’t able to spot anything for Python and maybe its a matter of wrangling the RSS feed, so if you know of anything please add a comment.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.biorxiv.org/alertsrss&#34;&gt;Alerts/RSS&lt;/a&gt; page provides details of the most recent 30 posts across categories and there is a Twitter feed by subject that people have tried to do interesting things with by creating a twitter bots.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round Up&lt;/h2&gt;
&lt;p&gt;I hope you found this quick list useful. If you know of any other good resources in either R or Python please feel welcome to add a comment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-R-crminer&#34;&gt;
&lt;p&gt;Chamberlain, Scott. 2017a. &lt;em&gt;Crminer: Fetch ’Scholary’ Full Text from ’Crossref’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=crminer&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=crminer&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-rcoreoa&#34;&gt;
&lt;p&gt;———. 2017b. &lt;em&gt;Rcoreoa: Client for the Core Api&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=rcoreoa&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=rcoreoa&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-fulltext&#34;&gt;
&lt;p&gt;———. 2018a. &lt;em&gt;Fulltext: Full Text of ’Scholarly’ Articles Across Many Data Sources&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=fulltext&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=fulltext&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-rorcid&#34;&gt;
&lt;p&gt;———. 2018b. &lt;em&gt;Rorcid: Interface to the ’Orcid.org’ ’Api’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=rorcid&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=rorcid&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-rcrossref&#34;&gt;
&lt;p&gt;Chamberlain, Scott, Carl Boettiger, Ted Hart, and Karthik Ram. 2018. &lt;em&gt;Rcrossref: Client for Various ’Crossref’ ’Apis’&lt;/em&gt;. &lt;a href=&#34;https://github.com/ropensci/rcrossref&#34; class=&#34;uri&#34;&gt;https://github.com/ropensci/rcrossref&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-rplos&#34;&gt;
&lt;p&gt;Chamberlain, Scott, Carl Boettiger, and Karthik Ram. 2017. &lt;em&gt;Rplos: Interface to the Search ’Api’ for ’Plos’ Journals&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=rplos&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=rplos&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-easyPubMed&#34;&gt;
&lt;p&gt;Fantini, Damiano. 2018. &lt;em&gt;EasyPubMed: Search and Retrieve Scientific Publication Records from Pubmed&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=easyPubMed&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=easyPubMed&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-rentrez&#34;&gt;
&lt;p&gt;Winter, David. 2018. &lt;em&gt;Rentrez: ’Entrez’ in R&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=rentrez&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=rentrez&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Creating A Bibliography with rcrossref</title>
      <link>/creating-a-bibliography-with-rcrossref/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/creating-a-bibliography-with-rcrossref/</guid>
      <description>&lt;p&gt;In this article we will look at how to create a bibliography using the &lt;code&gt;rcrossref&lt;/code&gt; package in R with RStudio. To help us get started we will also look at how to reference any R packages that you use in your work with &lt;code&gt;knitr&lt;/code&gt;. The article is part of work in progress for the WIPO Handbook on Patent Analytics. Comments and suggestions are welcome.&lt;/p&gt;
&lt;p&gt;Many of us will have spent an unreasonable amount of our time struggling to create bibliographies for a thesis, publication or reports. Software such as Endnote (Clarivate Analytics), Mendeley (Elsevier) and Zotero provide options for creating and managing bibliographic data. From experience, this generally involves masses of hours fiddling around with reference formats and so on before and after submission for publication.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.crossref.org/&#34;&gt;Crossref&lt;/a&gt; is a non-profit organisation that provides metadata on over 96 million publications in the form of information on the authors, titles, organisations, funding and so on. It is not a full text database but is increasingly providing access to abstracts and links to full texts.&lt;/p&gt;
&lt;p&gt;Crossref can be accessed through an &lt;a href=&#34;https://github.com/CrossRef/rest-api-doc&#34;&gt;API&lt;/a&gt;. For R users, Scott Chamberlain, Carl Boettiger, Ted Hart and Karthik Ram have developed the &lt;a href=&#34;https://github.com/ropensci/rcrossref&#34;&gt;rcrossref&lt;/a&gt; package as part of the ever growing suite of &lt;a href=&#34;https://ropensci.org/&#34;&gt;ROpenSci&lt;/a&gt; packages.&lt;/p&gt;
&lt;p&gt;To get started we will install the packages we are going to need and then look at how to quickly create a bibliography file for packages we are going to use. Once we have a grip on that we will use &lt;code&gt;rcrossref&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Packages&lt;/h3&gt;
&lt;p&gt;If you are new to R and RStudio then first we need to get set up. To install R for your operating system choose the appropriate option &lt;a href=&#34;http://cran.rstudio.com/&#34;&gt;here&lt;/a&gt; and install R. Then download the free RStudio desktop for your system &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/#download&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You should already have knitr installed with RStudio but in the wildly unlikely event that you don’t then run this line.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;knitr&amp;quot;)
install.packages(&amp;quot;devtools&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will use the development version of rcrossref for reasons we will explain below. You will need devtools (above) installed to do that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;ropensci/rcrossref&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(knitr)
library(rcrossref)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-a-bibliography-file&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating A Bibliography File&lt;/h3&gt;
&lt;p&gt;One of the puzzling things about getting started with creating bibliographies in RStudio… is how to create a bibliography file. So, as a warm up let’s do that.&lt;/p&gt;
&lt;p&gt;Yihui Xie has made it incredibly easy to create a bibliography in &lt;a href=&#34;http://www.bibtex.org/&#34;&gt;bibtex&lt;/a&gt; format with the &lt;a href=&#34;https://yihui.name/knitr/&#34;&gt;knitr&lt;/a&gt; package. Let’s create a file that contains packages mentioned in this article. The &lt;code&gt;write_bib()&lt;/code&gt; function allows us to add in a vector of package names, and to specify the path and name for the file using &lt;code&gt;file =&lt;/code&gt;. In this case I am creating a file called packages.bib in the content section of my blogdown site. I’ll break it up for visibility.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::write_bib(c(&amp;quot;devtools&amp;quot;, &amp;quot;base&amp;quot;, &amp;quot;rcrossref&amp;quot;, &amp;quot;blogdown&amp;quot;, &amp;quot;bookdown&amp;quot;, &amp;quot;knitr&amp;quot;,
                   &amp;quot;rmarkdown&amp;quot;, &amp;quot;citr&amp;quot;, &amp;quot;bibtex&amp;quot;), width = 60, 
                 file = &amp;quot;/Users/pauloldham17inch/blog/content/post/packages.bib&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This takes citation information from the description section of the packages. When you run this chunk it is normal to see a bunch of warning messages such as &lt;code&gt;DESCRIPTION file of package ‘ggmap’could not determine year for ‘ggmap’ from package&lt;/code&gt; etc.&lt;/p&gt;
&lt;p&gt;It is that easy. All we need to do is identify where we want to save the file and then try to remember the packages that we use regularly.&lt;/p&gt;
&lt;p&gt;If we take a quick look at the content we will see the following.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;@Manual{R-rcrossref,
  title = {rcrossref: Client for Various &amp;#39;CrossRef&amp;#39; &amp;#39;APIs&amp;#39;},
  author = {Scott Chamberlain and Carl Boettiger and Ted Hart
    and Karthik Ram},
  note = {R package version 0.8.1.9513},
  url = {https://github.com/ropensci/rcrossref},
  year = {2018},
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So now we have an easy way to create a bibliography file, hurrah! Let’s move on to rcrossref.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-rcrossref-addin&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The rcrossref Addin&lt;/h3&gt;
&lt;p&gt;RStudio includes a button at the top of the screen called Addins. It’s here:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/rcrossref/addin.gif&#34; width=&#34;400px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The standard version of rcrossref includes an addin that allows you to search for references using dois (document identifiers). I’ll come back to that. However, the add in for the development version does more thanks to great work by &lt;a href=&#34;https://github.com/haozhu233&#34;&gt;Hao Zhu&lt;/a&gt; of &lt;a href=&#34;https://github.com/haozhu233/giphyr&#34;&gt;giphyr&lt;/a&gt; fame. In response to a request on github &lt;a href=&#34;https://github.com/ropensci/rcrossref/issues/148&#34;&gt;here&lt;/a&gt;, Hao Zhu created a new version of the rcrossref addin that we will use now.&lt;/p&gt;
&lt;p&gt;From the Addins menu select Add Rcrossref Citations and a panel will open up. In the development version we can type in the name of an article and then Add to My Citations and Done when we are finished as we see in this example. I’ll look up one I can remember.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/rcrossref/addarticle.gif&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also use document identifiers. Here is one of mine on the completely bonkers subject of global climate engineering or geoengineering with doi: 10.1098/rsta.2014.0065.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/rcrossref/doi.gif&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The rcrossref add in creates a new file in your RStudio project directory (assuming you are using projects which is a very good thing), called crossref.bib. Once this file is created any time that you use the Addin it will add references rather than overwrite the file. So, this can be a very good way to build up a bibliography using rcrossref.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-citations-in-rmarkdown&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding citations in Rmarkdown&lt;/h3&gt;
&lt;p&gt;To add a citation, using the example above, we simply use &lt;code&gt;[@R-rcrossref]&lt;/code&gt;, as explained by Yihui &lt;a href=&#34;https://bookdown.org/yihui/bookdown/citations.html&#34;&gt;here&lt;/a&gt;. Note when inserting the reference we skip the type (e.g. Manual, Article etc) at the start of the crossref.bib entry. The result will look something like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;rcrossref &lt;span class=&#34;citation&#34;&gt;(Chamberlain et al. 2018)&lt;/span&gt; is a great package for accessing data for a bibliography or for larger scale bibliometrices. In 2014 Paul Oldham and his co-authors published an article on the completely mad subject of global climate engineering that succeeded in exciting consipiracy theorists using the #contrails and #chemtrails hashtags on twitter &lt;span class=&#34;citation&#34;&gt;(Oldham et al. 2014)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is ridiculously easy to do.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-the-bibliography&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding the Bibliography&lt;/h3&gt;
&lt;p&gt;We now have two bibliographies, one called packages.bib and one called crossref.bib. To get them to render when we knit the document (or build for blogdown or bookdown users… no knitting please) we need to put them in the header or YAML of the rmarkdown document. In all cases that looks like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bibliography: [packages.bib, crossref.bib]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we knit or build the document the citations will show up in the text itself and at the bottom of the document. To mark the start of the bibliography simply create a header such as &lt;code&gt;#References&lt;/code&gt; as the final line of your document.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;going-further&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Going Further&lt;/h3&gt;
&lt;p&gt;Thanks to the team behind &lt;code&gt;rcrossref&lt;/code&gt; you now have over 96 million references at your finger tips. Many thanks are also due to Yihui Xie and the team behind &lt;code&gt;knitr&lt;/code&gt;, &lt;code&gt;bookdown&lt;/code&gt; and &lt;code&gt;blogdown&lt;/code&gt; for their really great work&lt;/p&gt;
&lt;p&gt;You can read more about bibliographies on the RStudio site &lt;a href=&#34;https://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html&#34;&gt;here&lt;/a&gt;. Yihui Xie’s &lt;a href=&#34;https://yihui.name/knitr/&#34;&gt;knitr website&lt;/a&gt; and &lt;a href=&#34;https://www.amazon.com/dp/1498716962/ref=cm_sw_su_dp&#34;&gt;book&lt;/a&gt; are great resources &lt;span class=&#34;citation&#34;&gt;(Xie 2016a)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Bookdown users can find more information in the electronic version of the bookdown book right &lt;a href=&#34;https://bookdown.org/yihui/bookdown/citations.html&#34;&gt;here&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(Xie 2016b)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For blogdown users: &lt;a href=&#34;https://github.com/apreshill&#34;&gt;Alison Presmanes Hill&lt;/a&gt;, a co-author of blogdown, has written a very useful guide to citations in the minimal blogdown demo &lt;a href=&#34;https://github.com/rbind/blogdown-demo/blob/master/content/post/2017-08-28-adding-citations-to-posts.Rmd&#34;&gt;here&lt;/a&gt; that I used to write this article. It covers subjects such as using &lt;code&gt;nocite:&lt;/code&gt; in your YAML to avoid academic style citations for packages in the text. In the references below you will see that there all the packages used are included but without inline citation. You will very probably want to read the excellent blogdown book &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;here&lt;/a&gt; as well &lt;span class=&#34;citation&#34;&gt;(Xie, Thomas, and Hill 2017)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Packages to help you navigate citations include &lt;a href=&#34;https://github.com/ropensci/RefManageR&#34;&gt;&lt;code&gt;RefManageR&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/crsh/citr&#34;&gt;&lt;code&gt;citr&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/romainfrancois/bibtex&#34;&gt;&lt;code&gt;bibtex&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/ropensci/bib2df&#34;&gt;`bib2df&lt;/a&gt; you may well want to check them out. bib2df may be useful for cleaning up bibliography entries as it provides functions to convert a .bib file to a data.frame and back again.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; This list is not comprehensive and so if you know of any more please feel welcome to mention them below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-R-rcrossref&#34;&gt;
&lt;p&gt;Chamberlain, Scott, Carl Boettiger, Ted Hart, and Karthik Ram. 2018. &lt;em&gt;Rcrossref: Client for Various ’Crossref’ ’Apis’&lt;/em&gt;. &lt;a href=&#34;https://github.com/ropensci/rcrossref&#34; class=&#34;uri&#34;&gt;https://github.com/ropensci/rcrossref&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Oldham_2014&#34;&gt;
&lt;p&gt;Oldham, P., B. Szerszynski, J. Stilgoe, C. Brown, B. Eacott, and A. Yuille. 2014. “Mapping the Landscape of Climate Engineering.” &lt;em&gt;Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences&lt;/em&gt; 372 (2031). The Royal Society: 20140065–5. &lt;a href=&#34;https://doi.org/10.1098/rsta.2014.0065&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1098/rsta.2014.0065&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Dynamic_2016&#34;&gt;
&lt;p&gt;Xie, Yihui. 2016a. &lt;em&gt;Dynamic Documents with R and Knitr&lt;/em&gt;. Chapman; Hall/CRC. &lt;a href=&#34;https://doi.org/10.1201/b15166&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1201/b15166&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Xie_2016&#34;&gt;
&lt;p&gt;———. 2016b. &lt;em&gt;Bookdown&lt;/em&gt;. CRC Press. &lt;a href=&#34;https://doi.org/10.1201/9781315204963&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1201/9781315204963&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-blogdown_2017&#34;&gt;
&lt;p&gt;Xie, Yihui, Amber Thomas, and Alison Presmanes Hill. 2017. &lt;em&gt;Blogdown&lt;/em&gt;. Chapman; Hall/CRC. &lt;a href=&#34;https://doi.org/10.1201/9781351108195&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1201/9781351108195&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;At the time of writing install using devtools::install_github(“ottlngr/bib2df”)&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exploring Geocoding Scientific Literature with R</title>
      <link>/geocoding-scientific-literature-with-r/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/geocoding-scientific-literature-with-r/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/leaflet/leaflet.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/leaflet/leaflet.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/leafletfix/leafletfix.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/Proj4Leaflet/proj4-compressed.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/Proj4Leaflet/proj4leaflet.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/rstudio_leaflet/rstudio_leaflet.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/leaflet-binding/leaflet.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getting-set-up-with-the-google-maps-api&#34;&gt;Getting set up with the Google Maps API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#using-the-api&#34;&gt;Using the API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-source-data&#34;&gt;The Source Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lookup-the-records&#34;&gt;Lookup the Records&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#using-placement&#34;&gt;Using placement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#using-ggmap&#34;&gt;Using ggmap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#using-googleway&#34;&gt;Using Googleway&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reviewing-initial-results&#34;&gt;Reviewing Initial Results&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tackling-abbreviations&#34;&gt;Tackling Abbreviations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lookup-edited-names&#34;&gt;Lookup edited names&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bringing-the-data-together&#34;&gt;Bringing the data together&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#assessing-the-quality-of-geocoding&#34;&gt;Assessing the Quality of Geocoding&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#preprocess-the-data-and-rerun-the-query&#34;&gt;Preprocess the Data and Rerun the Query&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#duplicated-affiliation-names&#34;&gt;Duplicated Affiliation Names&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quickly-mapping-the-data&#34;&gt;Quickly Mapping the Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#round-up&#34;&gt;Round Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;In this article we will explore geocoding using the Google Maps API and the &lt;code&gt;placement&lt;/code&gt;, &lt;code&gt;ggmap&lt;/code&gt;, and &lt;code&gt;googleway&lt;/code&gt; packages in R. We will work with some raw data from Clarivate Analytics &lt;a href=&#34;http://wokinfo.com/&#34;&gt;&lt;code&gt;Web of Science&lt;/code&gt;&lt;/a&gt; database of scientific literature. Many universities have access to Web of Science and it is a very important tool in fields such as bibliometrics/scientometrics. This article is part of work in progress for the WIPO Patent Analytics Handbook. The aim of the article is to explore geocoding issues involving the scientific literature in depth and move closer to a solution using R.&lt;/p&gt;
&lt;p&gt;Geocoding is the process of taking a name and address and looking up the geographic coordinates expressed in latitude and longitude. This is normally done using a web service. There are plenty of example walkthroughs on how to do this. However, many of them start with data that is already clean. We will be working with data that is really rather messy.&lt;/p&gt;
&lt;p&gt;What we are attempting to do is to obtain the addresses and coordinates from the author affiliations field in Web of Science records. Our dataset is from a set of queries for scientific literature for South East Asia (ASEAN) countries that involve marine organisms. We have a table with 5,206 author affiliation details containing the names of organisations, the city and the country. This data is not clean and contains multiple minor variations of the same organisation name. The data also contains variations in geographic locations such as references to a district within a city rather than the name of the city itself. To follow the walk through you can download the data from Github &lt;a href=&#34;https://github.com/wipo-analytics/data-handbook/raw/master/affiliation_records.csv&#34;&gt;here&lt;/a&gt;. It simply contains the author affiliation name and a count of the number of records.&lt;/p&gt;
&lt;p&gt;One of the issues with Web of Science data is that the names of organisations are abbreviated/stemmed (so that University becomes Univ, Institute becomes Inst and so on and so on). Until recently this made geocoding a significant headache. However, as we will see below the Google Maps API now seems to do a very good job of handling these issues but considerable care is needed when interpreting the results.&lt;/p&gt;
&lt;p&gt;In this article we will go step by step through the process of geocoding and deal with the issues we encounter along the way. At the end of the article we will pull the code together to identify a more efficient way to deal with geocoding Web of Science and similar data.&lt;/p&gt;
&lt;p&gt;By the end of this article you will be familiar with what geocoding is and how to carry out geocoding using the &lt;code&gt;placement&lt;/code&gt;, &lt;code&gt;ggmap&lt;/code&gt; and &lt;code&gt;googleway&lt;/code&gt; packages in R with RStudio. You will also be familiar with the Google Maps API and be able to identify and retrieve missing data using packages from the tidyverse. We will take what we learned and combine it into more efficient code for solving the problem and finish off with a quick map of the results.&lt;/p&gt;
&lt;div id=&#34;getting-started&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Getting Started&lt;/h3&gt;
&lt;p&gt;If you are new to R and RStudio then first we need to get set up. To install R for your operating system choose the appropriate option &lt;a href=&#34;http://cran.rstudio.com/&#34;&gt;here&lt;/a&gt; and install R. Then download the free RStudio desktop for your system &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/#download&#34;&gt;here&lt;/a&gt;. We will be using a suite of packages called the &lt;code&gt;tidyverse&lt;/code&gt; that make it easy to work with data. When you have installed and opened RStudio run these lines in your console to install the packages that we will be using.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;tidyverse&amp;quot;)
install.packages(&amp;quot;placement&amp;quot;)
install.packages(&amp;quot;devtools&amp;quot;)
install.packages(&amp;quot;usethis&amp;quot;)
install.packages(&amp;quot;googleway&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For ggmap we will load the latest version 2.7 that includes &lt;code&gt;register_google()&lt;/code&gt; for authentication and install it from github as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;dkahle/ggmap&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next load the libraries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(ggmap)
library(placement)
library(usethis)
library(googleway)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will now see a bunch of messages as the packages are loaded. You should now be good to go.&lt;/p&gt;
&lt;p&gt;If you would like to learn more about R then try the excellent &lt;a href=&#34;http://www.wipo.int/treaties/en/ip/paris/summary_paris.html&#34;&gt;DataCamp&lt;/a&gt; online courses or read Garrett Grolemund and Hadley Wickham’s &lt;a href=&#34;http://r4ds.had.co.nz/&#34;&gt;R for Data Science&lt;/a&gt;. Learning to do things in R will make a huge difference to your ability to work with patent and other data and to enjoy the support of the R community in addressing new challenges. There is never a better time to start learning to do things in R than right now.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;placement&lt;/code&gt;, &lt;code&gt;ggmap&lt;/code&gt; and recent &lt;code&gt;googleway&lt;/code&gt; packages all provide functions for geocoding with the Google Maps API. The &lt;code&gt;placement&lt;/code&gt; package by &lt;a href=&#34;http://derekyves.github.io/&#34;&gt;Derek Darves&lt;/a&gt; was created in 2016 and provides straightforward access to the Google Maps API and additional tools for address cleaning, calculating distances and driving times. As Derek explains &lt;a href=&#34;http://derekyves.github.io/2016/07/24/placement-pkg.html&#34;&gt;here&lt;/a&gt;. I found it remarkably easy to use and it does not require any complicated code. The function we will be using is &lt;code&gt;geocode_url()&lt;/code&gt; and &lt;code&gt;geocode_pull()&lt;/code&gt;. That is basically it.&lt;/p&gt;
&lt;p&gt;While &lt;code&gt;placement&lt;/code&gt; mainly focuses on geocoding, &lt;a href=&#34;https://github.com/dkahle/ggmap&#34;&gt;&lt;code&gt;ggmap&lt;/code&gt;&lt;/a&gt; is a bigger package for mapping in R that includes geocoding. The package is a complement to ggplot2 and a Data Camp course by Charlotte Wickham &lt;a href=&#34;https://www.datacamp.com/courses/working-with-geospatial-data-in-r&#34;&gt;Working with Geospatial Data in R&lt;/a&gt; will get you started in no time with ggmap and other mapping packages. As we will see below, I ran in to some tricky issues when trying to geocode with ggmap and you may also want to give &lt;code&gt;googleway&lt;/code&gt; a try.&lt;/p&gt;
&lt;p&gt;We will mainly use the &lt;code&gt;placement&lt;/code&gt; package because I like the simplicity of the package, but which you use will depend on your purpose and you will probably want to experiment with the wider functionality of &lt;code&gt;ggmap&lt;/code&gt; or the more recent &lt;code&gt;googleway&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-set-up-with-the-google-maps-api&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Getting set up with the Google Maps API&lt;/h3&gt;
&lt;p&gt;To use the Google Maps API you will need to:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Sign in to a Google account&lt;/li&gt;
&lt;li&gt;Get a free API key from &lt;a href=&#34;https://developers.google.com/maps/documentation/javascript/get-api-key#get-an-api-key&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This involves pressing the &lt;code&gt;Get a Key&lt;/code&gt; button and creating a project (app) that you will query by following these steps.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/geocoding/getkey.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Create a new project and wait a short while while Google spins it up.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/geocoding/create_project.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You will then see your API key. Note that you will see a link to restrict access to your API. It is a good idea to follow this and use your IP address to limit access to your IP address under Application restrictions. This will prevent other people from using the account if they discover the API key. We will not go down that route right now.&lt;/p&gt;
&lt;p&gt;Take a copy of your API key (say into a text file in R Studio). What you do next is up to you.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Save the text file somewhere sensible and copy it into the functions below when needed.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With &lt;code&gt;usethis&lt;/code&gt; either:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;usethis::edit_r_environ()&lt;/code&gt; to open your local environment file and enter something like GOOGLE_MAPS_KEY=“yourkey” and then restart R. You will be able to access the key using &lt;code&gt;Sys.getenv(&amp;quot;GOOGLE_MAPS_KEY&amp;quot;)&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;usethis::edit_r_profile()&lt;/code&gt; and enter google_maps_key=“your key”, inside the existing options() chunk, save and restart R. Call the key with &lt;code&gt;getOption(&amp;quot;google_maps_key&amp;quot;)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For discussion on the above try reading the R startup section of &lt;a href=&#34;https://csgillespie.github.io/efficientR/&#34;&gt;Efficient R Programming&lt;/a&gt; or follow the very useful &lt;a href=&#34;https://github.com/ropensci/rOpenSci/wiki/Use-of-API-keys&#34;&gt;ROpenSci instructions&lt;/a&gt;. &lt;code&gt;usethis&lt;/code&gt; makes life much easier because it knows where the files are!&lt;/p&gt;
&lt;p&gt;We will go with the &lt;code&gt;usethis::edit_r_environ()&lt;/code&gt; environment option, so let’s store the key in our working environment for the moment using the imaginatively named key.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;key &amp;lt;- Sys.getenv(&amp;quot;GOOGLE_MAPS_KEY&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-api&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using the API&lt;/h3&gt;
&lt;p&gt;Note that API queries are limited to a free 2500 per day. It costs 50 cents per 1000 queries after that. As this would not break the bank we simply signed up for a billing account to run the full list. As we will see below signing up for an API key is a good idea to avoid problems with the return resulting from pressure on the free service. When you sign up for the API key you still get the 2500 results but make sure you put your API key somewhere safe and do not make it public.&lt;/p&gt;
&lt;p&gt;Below we will briefly show how to use the &lt;code&gt;placement&lt;/code&gt;, &lt;code&gt;ggmap&lt;/code&gt; and newer &lt;code&gt;googleway&lt;/code&gt; packages to retrieve geocode data. Unfortunately the return from the Google API with placement also includes a column called &lt;code&gt;input_url&lt;/code&gt;. I say unfortunate because the &lt;code&gt;input_url&lt;/code&gt; includes your private API key! So, if you are planning to make any of this data public you should exclude the &lt;code&gt;input_url&lt;/code&gt; column.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-source-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Source Data&lt;/h3&gt;
&lt;p&gt;Next let’s take a quick look at the source data. When we send the addresses to the Google Maps API with &lt;code&gt;placement&lt;/code&gt; it will return the original search terms in a column called &lt;code&gt;locations&lt;/code&gt;. To make our life easier we renamed the original column in our source dataset. Note that the records field refers to the number of publications associated with an address and will allow us to size dots on any map we produce with the results. We can import the data directly from Github.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;affiliation_records &amp;lt;- read_csv(&amp;quot;https://github.com/wipo-analytics/data-handbook/raw/master/affiliation_records.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(affiliation_records)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   records locations                                id
##     &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                                 &amp;lt;int&amp;gt;
## 1       1 AAHL, Vic, Australia                      1
## 2       1 AAHRI, Bangkok, Thailand                  2
## 3       1 Aarhus Univ Biosci, Roskilde, Denmark     3
## 4       1 Aarhus Univ Hosp, Aarhus, Denmark         4
## 5      13 Aarhus Univ, Aarhus C, Denmark            5
## 6       3 Aarhus Univ, Aarhus, Denmark              6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;lookup-the-records&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lookup the Records&lt;/h2&gt;
&lt;p&gt;In this section we will look up some of the records with each of the three packages to show how easy it is. Purely from personal preference we will use &lt;code&gt;placement&lt;/code&gt; for the rest of the work.&lt;/p&gt;
&lt;div id=&#34;using-placement&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using placement&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;placement&lt;/code&gt; package can do more than we will attempt here. For example, you can attempt address cleaning or calculating driving distances with &lt;code&gt;placement&lt;/code&gt;. For our purposes the main event is the &lt;code&gt;geocode_url()&lt;/code&gt; function, We pass the data in the locations column to the function along with the authentication route and the private key. The &lt;code&gt;clean = TRUE&lt;/code&gt; argument applies the &lt;code&gt;address_cleaner&lt;/code&gt; function before encoding the URL to send to the API. The default is set to TRUE and you may want to experiment with setting this value to FALSE. We also add the date of search as it is always useful to know when we carried out the search and we set verbose to TRUE to receive more information. Note that other arguments such as &lt;code&gt;dryrun&lt;/code&gt; can be useful for debugging problem addresses.&lt;/p&gt;
&lt;p&gt;Note that the key can be entered directly into &lt;code&gt;geocode_url()&lt;/code&gt; as &lt;code&gt;privkey = Sys.getenv(&amp;quot;GOOGLE_MAPS_KEY&amp;quot;)&lt;/code&gt;. However, I found that this sometimes returned an error message on long runs. For that reason we might copy it into our local environment (and be careful not to expose it).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;key &amp;lt;- Sys.getenv(&amp;quot;GOOGLE_MAPS_KEY&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(placement)
coordaffil &amp;lt;- 
  geocode_url(affiliation_records$locations, auth = &amp;quot;standard_api&amp;quot;, privkey = key, clean = TRUE, add_date = &amp;#39;today&amp;#39;, verbose = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;using-ggmap&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using ggmap&lt;/h3&gt;
&lt;p&gt;We can perform the same lookup using &lt;code&gt;ggmap&lt;/code&gt; and the &lt;code&gt;geocode()&lt;/code&gt; function. Note that the function defaults to the free allocation of 2500 queries. There are options to return “latlon” and “latlona”&amp;quot; or “more” or “all”. In the case of “all” this returns a list with entries of differing lengths that you will need to wrangle. In general use &lt;code&gt;latlon&lt;/code&gt;, &lt;code&gt;latlona&lt;/code&gt; or &lt;code&gt;more&lt;/code&gt; as this will return a data frame. Here we will just test 100 records. &lt;code&gt;geocode()&lt;/code&gt; does not return the input URL with our private key (which is good).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggmap)
coord_ggmap &amp;lt;- 
  geocode(location = affiliation_records$locations[1:100], 
output = &amp;quot;more&amp;quot;, source = &amp;quot;google&amp;quot;, 
messaging = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When using &lt;code&gt;ggmap&lt;/code&gt; I encountered a significant number of &lt;code&gt;OVER_QUERY_LIMIT&lt;/code&gt; entries in the return. Why is something of a mystery although as discussed &lt;a href=&#34;https://stackoverflow.com/questions/36175529/getting-over-query-limit-after-one-request-with-geocode&#34;&gt;here&lt;/a&gt; this may because we are sharing the call to the free service with others. It is therefore better to get a key if you are going to be using this service. To authenticate using &lt;code&gt;ggmap&lt;/code&gt; (2.7 only) create a key based on the key in your environment file. Pass it to &lt;code&gt;register_google()&lt;/code&gt; and then you are ready to make the call.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;key &amp;lt;- Sys.getenv(&amp;quot;GOOGLE_MAPS_KEY&amp;quot;)
register_google(key = key)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It will now work smoothly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggmap)
ggmap1 &amp;lt;- 
  geocode(location = affiliation_records$locations[201:300],
output = &amp;quot;more&amp;quot;, source = &amp;quot;google&amp;quot;,
messaging = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This overcame the limitation and returned a data.frame with 100 entries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggmap1 %&amp;gt;% 
  select(1:4) %&amp;gt;% 
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          lon      lat                        type          loctype
## 1 -93.631913 42.03078                    locality      approximate
## 2   3.707011 51.05376               establishment          rooftop
## 3  -1.386919 50.90853               establishment geometric_center
## 4 142.384141 43.72986               establishment          rooftop
## 5 142.384141 43.72986               establishment          rooftop
## 6 127.680932 26.21240 administrative_area_level_1      approximate&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;using-googleway&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using Googleway&lt;/h3&gt;
&lt;p&gt;An alternative to &lt;code&gt;placement&lt;/code&gt; or &lt;code&gt;ggmap&lt;/code&gt; is also available using the &lt;code&gt;googleway&lt;/code&gt; package. &lt;code&gt;googleway&lt;/code&gt; includes access to the Google APIs for directions, distance, elevation, timezones, places, geocoding and reverse geocoding and so has a wider set of uses. However, &lt;code&gt;googleway&lt;/code&gt; is expecting an address field of length 1 (meaning it takes one address at a time) whereas placement and ggmap are vectorised. The return from googleway returns a list object containing a data frame with the results and the status of the return. Here is one quick example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(googleway)
googleway &amp;lt;- 
  google_geocode(address = &amp;quot;Aarhus Univ Biosci, Roskilde, Denmark&amp;quot;, key = Sys.getenv(&amp;quot;GOOGLE_MAPS_KEY&amp;quot;), simplify = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For long lists we would therefore need to use an approach such as &lt;code&gt;lapply()&lt;/code&gt; or &lt;code&gt;purrr::map()&lt;/code&gt; to make the call as a set and then look at ways to bind the results together.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;googleway2 &amp;lt;- 
  purrr::map(affiliation_records$locations[1:2], google_geocode, key = Sys.getenv(&amp;quot;GOOGLE_MAPS_KEY&amp;quot;), simplify = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;!--- identify a solution to bind into a df, as I have forgotten, again!----&gt;
&lt;p&gt;As this makes clear, you have at least three choices for geocoding and which you prefer will depend on your needs. I found &lt;code&gt;ggmap&lt;/code&gt; rather awkward because the existing CRAN version (2.6) does not provide the &lt;code&gt;register_google()&lt;/code&gt; function in the long standing 2.7 development version. While this is a bit awkward &lt;code&gt;ggmap&lt;/code&gt; provides some very powerful features that you will want to use. On the other hand &lt;code&gt;googleway&lt;/code&gt; would involve some more work to vectorise over the list as we started exploring above. &lt;code&gt;placement&lt;/code&gt; on the other hand is fine with the only disadvantage being the return of the API key in the input URL that we have to remember.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;reviewing-initial-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reviewing Initial Results&lt;/h2&gt;
&lt;p&gt;When we originally started working with the Google API in 2017 the API returned 3,937 results from the 5,206 names. This then required a lot of additional work to retrieve the remaining numbers by cleaning up abbreviations and country names. However, the Google Maps API seems to have improved rather radically in the meantime.&lt;/p&gt;
&lt;p&gt;Let’s take a look at the issues that can arise with the return from the Google Maps API. For the moment we will focus on the completeness of the data revealed in status and error messages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coordaffil %&amp;gt;%
  select(location_type, status, error_message) %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      location_type status error_message
## 1          ROOFTOP     OK              
## 2 GEOMETRIC_CENTER     OK              
## 3          ROOFTOP     OK              
## 4 GEOMETRIC_CENTER     OK              
## 5          ROOFTOP     OK              
## 6          ROOFTOP     OK&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The return from placement is a data.frame that is exactly the same length as our input. What we need to watch out for are the entries in the status column and the error message column. Here we need to be cautious because most of the time the API returns either “OK” or “ZERO_RESULTS”. However, there are additional status codes listed &lt;a href=&#34;https://developers.google.com/maps/documentation/geocoding/intro&#34;&gt;here&lt;/a&gt; and they are also listed in the documentation for &lt;code&gt;geocode_url()&lt;/code&gt;. They are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“OK”&lt;/li&gt;
&lt;li&gt;“ZERO_RESULTS”&lt;/li&gt;
&lt;li&gt;“OVER_QUERY_LIMIT”&lt;/li&gt;
&lt;li&gt;“REQUEST_DENIED”&lt;/li&gt;
&lt;li&gt;“INVALID_REQUEST”&lt;/li&gt;
&lt;li&gt;“UNKNOWN_ERROR”&lt;/li&gt;
&lt;li&gt;“CONNECTION_ERROR” (added)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When running a long set of addresses the CONNECTION_ERROR can creep into the data, so be aware of this.&lt;/p&gt;
&lt;p&gt;We can now join our data sets together. We will use &lt;code&gt;left_join()&lt;/code&gt; for convenience and specify the column to join on as the shared &lt;code&gt;locations&lt;/code&gt; column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results &amp;lt;- 
  dplyr::left_join(affiliation_records, coordaffil, by = &amp;quot;locations&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can identify the results found so far by filtering on the status field which will show “OK” where there is a return and “ZERO_RESULTS” where the geocoding did not work:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results %&amp;gt;%
  filter(., status == &amp;quot;OK&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5,187 x 10
##    records locations     id   lat     lng location_type formatted_address 
##      &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;      &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;             
##  1       1 AAHL, Vic…     1 -38.2  144.   ROOFTOP       5 Portarlington R…
##  2       1 AAHRI, Ba…     2  13.8  101.   GEOMETRIC_CE… 50, กรมประมง, ถนน…
##  3       1 Aarhus Un…     3  56.2   10.2  ROOFTOP       Nordre Ringgade 1…
##  4       1 Aarhus Un…     4  56.2   10.2  GEOMETRIC_CE… Nørrebrogade, 800…
##  5      13 Aarhus Un…     5  56.2   10.2  ROOFTOP       Nordre Ringgade 1…
##  6       3 Aarhus Un…     6  56.2   10.2  ROOFTOP       Nordre Ringgade 1…
##  7       1 Abasyn Un…     7  34.0   71.6  GEOMETRIC_CE… Ring Road, Charsa…
##  8       1 Abdul Wal…     8  34.2   72.0  GEOMETRIC_CE… Nowshera Mardan R…
##  9       1 Abertay U…     9  56.5   -2.97 GEOMETRIC_CE… Bell St, Dundee D…
## 10       1 Aberystwy…    10  52.4   -4.07 GEOMETRIC_CE… Penglais Campus, …
## # ... with 5,177 more rows, and 3 more variables: status &amp;lt;chr&amp;gt;,
## #   error_message &amp;lt;chr&amp;gt;, geocode_dt &amp;lt;date&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the results that were not found it is safest not to simply filter for ZERO RESULTS but instead to filter for anything that is not OK using &lt;code&gt;!=&lt;/code&gt;. This can save on endless hours of confusion where you have multiple messages in the status column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lookup &amp;lt;- results %&amp;gt;%
  filter(., status != &amp;quot;OK&amp;quot;)
nrow(lookup)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 19&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, we have 19 records with no results. That is pretty good from just over 5000 results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lookup %&amp;gt;%
  select(-id)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 19 x 9
##    records locations       lat   lng location_type formatted_addre… status
##      &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt; 
##  1       2 Aomori Prefe…    NA    NA &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;             ZERO_…
##  2       1 FOOD CROPS R…    NA    NA &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;             ZERO_…
##  3       2 Hunan Agr Un…    NA    NA &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;             ZERO_…
##  4       1 Hunan Fisher…    NA    NA &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;             ZERO_…
##  5       1 Hunan Univ C…    NA    NA &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;             ZERO_…
##  6       2 Indonesian I…    NA    NA &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;             ZERO_…
##  7       1 Inst Oceanog…    NA    NA &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;             ZERO_…
##  8       1 Inst Oceanog…    NA    NA &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;             ZERO_…
##  9       6 Inst Oceanog…    NA    NA &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;             ZERO_…
## 10       2 ISME, Okinaw…    NA    NA &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;             ZERO_…
## 11       1 Kitasato Uni…    NA    NA &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;             ZERO_…
## 12       1 Main Off Edu…    NA    NA &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;             ZERO_…
## 13       1 Nha Trang In…    NA    NA &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;             ZERO_…
## 14       1 Okinawa Pref…    NA    NA &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;             ZERO_…
## 15       1 Ryukoku Univ…    NA    NA &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;             ZERO_…
## 16       1 UNIV WESTMIN…    NA    NA &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;             ZERO_…
## 17       6 Vietnam Acad…    NA    NA &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;             ZERO_…
## 18       1 VNIO, Nha Tr…    NA    NA &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;             ZERO_…
## 19       1 Xi Consultan…    NA    NA &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;             ZERO_…
## # ... with 2 more variables: error_message &amp;lt;chr&amp;gt;, geocode_dt &amp;lt;date&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When dealing with thousands of records it is often a good idea to add a cut off threshold. For example we can see above that with two exceptions the entries are all for 1 or 2 records. As these will be barely visible on a map you may want to set a cut off point to focus in on the more important records.&lt;/p&gt;
&lt;p&gt;However, the lookup table highlights an issue that the Google Maps API previously struggled to deal with: abbreviations. When working with scientific literature abbreviations in author affiliations along with acronyms are common. So, lets look at how to deal with that.&lt;/p&gt;
&lt;div id=&#34;tackling-abbreviations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tackling Abbreviations&lt;/h3&gt;
&lt;p&gt;Here we have created a simple file containing some of the major Web of Science organisation abbreviations and their matches. It is probably not complete but is a good start. Next we added a column with word boundaries that we will use to find and replace the abbreviations. You can download the the file directly from Github.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wos_abbreviations &amp;lt;- read_csv(&amp;quot;https://github.com/wipo-analytics/data-handbook/raw/master/wos_abbreviations.csv&amp;quot;, 
  col_types = cols(abbreviation = col_character(), 
    text = col_character()))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A simple word boundary regular expression was added to assist with matching.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wos_abbreviations$regex &amp;lt;- 
  paste0(&amp;quot;\\b&amp;quot;, wos_abbreviations$abbreviation, &amp;quot;\\b&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   abbreviation text       regex       
##   &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;       
## 1 Univ         University &amp;quot;\\bUniv\\b&amp;quot;
## 2 Natl         National   &amp;quot;\\bNatl\\b&amp;quot;
## 3 Inst         Institute  &amp;quot;\\bInst\\b&amp;quot;
## 4 Sci          Science    &amp;quot;\\bSci\\b&amp;quot; 
## 5 Ctr          Centre     &amp;quot;\\bCtr\\b&amp;quot; 
## 6 Res          Research   &amp;quot;\\bRes\\b&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To replace the abbreviations we will want to temporarily separate out the city and the country names in the locations column. This helps us to avoid transforming them by accident. We will bring the edited version back together later. Web of Science data uses a comma to separate out the entities and so we use that in a call to separate. We also keep the original column by specifying &lt;code&gt;remove = FALSE&lt;/code&gt; as the default removes the input column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lookup &amp;lt;- 
  lookup %&amp;gt;% 
  separate(., locations, c(&amp;quot;organisation&amp;quot;, &amp;quot;city&amp;quot;, &amp;quot;country&amp;quot;), sep = &amp;quot;,&amp;quot;, remove = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##   records locations             organisation        city   country      id
##     &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;               &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;
## 1       2 Aomori Prefectural A… Aomori Prefectural… &amp;quot; Aom… &amp;quot; Japan&amp;quot;    161
## 2       1 FOOD CROPS RES INST,… FOOD CROPS RES INST &amp;quot; HAI… &amp;quot; VIETNA…  1215
## 3       2 Hunan Agr Univ, Huna… Hunan Agr Univ      &amp;quot; Hun… &amp;quot; People…  1521
## 4       1 Hunan Fisheries Sci … Hunan Fisheries Sc… &amp;quot; Hun… &amp;quot; People…  1522
## 5       1 Hunan Univ Chinese M… Hunan Univ Chinese… &amp;quot; Hun… &amp;quot; People…  1523
## 6       2 Indonesian Inst Sci,… Indonesian Inst Sci &amp;quot; Amb… &amp;quot; Indone…  1597&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we want to iterate over the list of our organisation names and replace the abbreviations. There are a variety of ways to do that such as the &lt;code&gt;qdap&lt;/code&gt; package function &lt;code&gt;multigsub()&lt;/code&gt; or &lt;code&gt;mgsub()&lt;/code&gt;. We like &lt;code&gt;qdap&lt;/code&gt; a lot but installation of the package can be a bit awkward due to a dependency on &lt;code&gt;rJava&lt;/code&gt;.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Instead we are going to use a simple for loop (although a &lt;code&gt;purrr&lt;/code&gt; solution would be an improvement).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;replaceabbr &amp;lt;- function(pattern, replacement, var) {
  replacement &amp;lt;- rep(replacement, length(pattern)) 
    for (i in seq_along(pattern)) {
        var &amp;lt;- gsub(pattern[i], replacement[i], var)
        }
  var
} &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One issue with cleaning names is capitalisation. For example, in our wos abbreviations file we have used &lt;code&gt;Univ&lt;/code&gt; as the most common abbreviation for University. However, this will not match UNIV and so we will be better off regularising the text. A common convention is to convert everything to lower case using &lt;code&gt;tolower()&lt;/code&gt; at the start of working with the data. Here we don’t want to do that. We will use the extremely useful &lt;code&gt;stringr&lt;/code&gt; package to convert the organisation name to to title case in a new field that we will call organisation_edited. The reason that we are not editing our original column is that at some point we will want to join the table back on to our original dataset…so we don’t want to touch our original column. We will do this using &lt;code&gt;mutate()&lt;/code&gt; from &lt;code&gt;dplyr()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lookup &amp;lt;- lookup %&amp;gt;%  
  mutate(organisation_edited = str_to_title(.$organisation))
lookup %&amp;gt;% select(organisation_edited)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 19 x 1
##    organisation_edited                          
##    &amp;lt;chr&amp;gt;                                        
##  1 Aomori Prefectural Agr &amp;amp; Forestry Res Ctr    
##  2 Food Crops Res Inst                          
##  3 Hunan Agr Univ                               
##  4 Hunan Fisheries Sci Inst                     
##  5 Hunan Univ Chinese Med                       
##  6 Indonesian Inst Sci                          
##  7 Inst Oceanog Vast                            
##  8 Inst Oceanog                                 
##  9 Inst Oceanog                                 
## 10 Isme                                         
## 11 Kitasato Univ                                
## 12 Main Off Educ &amp;amp; Teaching Area                
## 13 Nha Trang Inst Oceanog                       
## 14 Okinawa Prefectural Fisheries &amp;amp; Ocean Res Ctr
## 15 Ryukoku Univ                                 
## 16 Univ Westminster                             
## 17 Vietnam Acad Sci &amp;amp; Technol                   
## 18 Vnio                                         
## 19 Xi Consultancy&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we transform the abbreviations using replaceabbr.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lookup$organisation_edited &amp;lt;- 
  replaceabbr(wos_abbreviations$regex, wos_abbreviations$text, lookup$organisation_edited)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lookup %&amp;gt;% 
  select(organisation_edited)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 19 x 1
##    organisation_edited                                      
##    &amp;lt;chr&amp;gt;                                                    
##  1 Aomori Prefectural Agriculture &amp;amp; Forestry Research Centre
##  2 Food Crops Research Institute                            
##  3 Hunan Agriculture University                             
##  4 Hunan Fisheries Science Institute                        
##  5 Hunan University Chinese Medical                         
##  6 Indonesian Institute Science                             
##  7 Institute Oceanography Vast                              
##  8 Institute Oceanography                                   
##  9 Institute Oceanography                                   
## 10 Isme                                                     
## 11 Kitasato University                                      
## 12 Main Office Education &amp;amp; Teaching Area                    
## 13 Nha Trang Institute Oceanography                         
## 14 Okinawa Prefectural Fisheries &amp;amp; Ocean Research Centre    
## 15 Ryukoku University                                       
## 16 University Westminster                                   
## 17 Vietnam Academy Science &amp;amp; Technology                     
## 18 Vnio                                                     
## 19 Xi Consultancy&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is not perfect, for example we encounter issues with Agriculture and Agricultural and so on. We also encounter issues of capitalisation in the city and the country field that we are presently ignoring. However, it is good enough for the time being. Rather than focus on resolving a small number of remaining items the next step is to reunite the fields we separated into a field we will call locations edited using the &lt;code&gt;tidyr&lt;/code&gt; &lt;code&gt;unite&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lookup &amp;lt;- lookup %&amp;gt;%
  unite(., locations_edited, c(organisation_edited, city, country), sep = &amp;quot;,&amp;quot;, remove = FALSE)

lookup %&amp;gt;%
  select(organisation, city, country, locations_edited)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 19 x 4
##    organisation          city     country   locations_edited              
##    &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;                         
##  1 Aomori Prefectural A… &amp;quot; Aomor… &amp;quot; Japan&amp;quot;  Aomori Prefectural Agricultur…
##  2 FOOD CROPS RES INST   &amp;quot; HAI H… &amp;quot; VIETNA… Food Crops Research Institute…
##  3 Hunan Agr Univ        &amp;quot; Hunan&amp;quot; &amp;quot; People… Hunan Agriculture University,…
##  4 Hunan Fisheries Sci … &amp;quot; Hunan&amp;quot; &amp;quot; People… Hunan Fisheries Science Insti…
##  5 Hunan Univ Chinese M… &amp;quot; Hunan&amp;quot; &amp;quot; People… Hunan University Chinese Medi…
##  6 Indonesian Inst Sci   &amp;quot; Ambon&amp;quot; &amp;quot; Indone… Indonesian Institute Science,…
##  7 Inst Oceanog VAST     &amp;quot; Nha T… &amp;quot; Vietna… Institute Oceanography Vast, …
##  8 Inst Oceanog          &amp;quot; Nha T… &amp;quot; Vietna… Institute Oceanography, Nha T…
##  9 Inst Oceanog          &amp;quot; Nha T… &amp;quot; Vietna… Institute Oceanography, Nha T…
## 10 ISME                  &amp;quot; Okina… &amp;quot; Japan&amp;quot;  Isme, Okinawa, Japan          
## 11 Kitasato Univ         &amp;quot; Aomor… &amp;quot; Japan&amp;quot;  Kitasato University, Aomori, …
## 12 Main Off Educ &amp;amp; Teac… &amp;quot; Tehra… &amp;quot; Iran&amp;quot;   Main Office Education &amp;amp; Teach…
## 13 Nha Trang Inst Ocean… &amp;quot; Khanh… &amp;quot; Vietna… Nha Trang Institute Oceanogra…
## 14 Okinawa Prefectural … &amp;quot; Okina… &amp;quot; Japan&amp;quot;  Okinawa Prefectural Fisheries…
## 15 Ryukoku Univ          &amp;quot; Okina… &amp;quot; Japan&amp;quot;  Ryukoku University, Okinawa, …
## 16 UNIV WESTMINSTER      &amp;quot; LONDO… &amp;quot; ENGLAN… University Westminster, LONDO…
## 17 Vietnam Acad Sci &amp;amp; T… &amp;quot; Nha T… &amp;quot; Vietna… Vietnam Academy Science &amp;amp; Tec…
## 18 VNIO                  &amp;quot; Nha T… &amp;quot; Vietna… Vnio, Nha Trang, Vietnam      
## 19 Xi Consultancy        &amp;quot; Delft&amp;quot; &amp;quot; Nether… Xi Consultancy, Delft, Nether…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that rather than creating a separate character vector we made life easier by simply adding &lt;code&gt;locations_edited&lt;/code&gt; to our lookup data.frame (because the vectors are of the same length) using &lt;code&gt;unite()&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lookup-edited-names&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Lookup edited names&lt;/h3&gt;
&lt;p&gt;We now send the cleaned up version off to the Google API.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(placement)
coordlookup &amp;lt;- geocode_url(lookup$locations_edited, auth = &amp;quot;standard_api&amp;quot;, privkey = key, clean = TRUE, add_date = &amp;#39;today&amp;#39;, verbose = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;!--- save and then load the lookup---&gt;
&lt;p&gt;Let’s take a look.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coordlookup %&amp;gt;% 
  select(locations, status)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                                                   locations
## 1  Aomori Prefectural Agriculture &amp;amp; Forestry Research Centre, Aomori, Japan
## 2                          Food Crops Research Institute, HAI HUNG, VIETNAM
## 3                      Hunan Agriculture University, Hunan, Peoples R China
## 4                 Hunan Fisheries Science Institute, Hunan, Peoples R China
## 5                  Hunan University Chinese Medical, Hunan, Peoples R China
## 6                            Indonesian Institute Science, Ambon, Indonesia
## 7                           Institute Oceanography Vast, Nha Trang, Vietnam
## 8                           Institute Oceanography, Nha Trang City, Vietnam
## 9                                Institute Oceanography, Nha Trang, Vietnam
## 10                                                     Isme, Okinawa, Japan
## 11                                       Kitasato University, Aomori, Japan
## 12                      Main Office Education &amp;amp; Teaching Area, Tehran, Iran
## 13                Nha Trang Institute Oceanography, Khanh Hoa Prov, Vietnam
## 14    Okinawa Prefectural Fisheries &amp;amp; Ocean Research Centre, Okinawa, Japan
## 15                                       Ryukoku University, Okinawa, Japan
## 16                          University Westminster, LONDON W1M 8JS, ENGLAND
## 17                 Vietnam Academy Science &amp;amp; Technology, Nha Trang, Vietnam
## 18                                                 Vnio, Nha Trang, Vietnam
## 19                                       Xi Consultancy, Delft, Netherlands
##          status
## 1  ZERO_RESULTS
## 2  ZERO_RESULTS
## 3            OK
## 4            OK
## 5            OK
## 6            OK
## 7  ZERO_RESULTS
## 8            OK
## 9            OK
## 10 ZERO_RESULTS
## 11 ZERO_RESULTS
## 12           OK
## 13 ZERO_RESULTS
## 14           OK
## 15           OK
## 16           OK
## 17           OK
## 18 ZERO_RESULTS
## 19 ZERO_RESULTS&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, 8 of our revised names have failed to produce a return. In some cases this is a little surprising. For example the private Kitasato University would be expected to come up, but the reference to Aomori seems to have confused the mapper (as the University is listed as located in Minato). In the case of the Institute Oceanography Vast we can see that there is duplication (Vast refers to the Vietnam Academy of Science and Technology as the parent organisation of the institute) with the second and third entries being recognised. Other variants such as &lt;code&gt;Nha Trang Institute Oceanography, Khanh Hoa Prov, Vietnam&lt;/code&gt; and the acronym &lt;code&gt;Vnio, Nha Trang, Vietnam&lt;/code&gt; are also missed. How far you want to push with fixing addresses is up to you and will depend on your purposes. As mentioned above, to avoid a long tail of unresolved addresses for low frequency data you may want to use a cut off on the number of records.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bringing-the-data-together&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bringing the data together&lt;/h3&gt;
&lt;p&gt;To join the data back together we need to do some tidying up on the lookup and coordlookup table first. Recall that we sent edited names to Google and those were returned as &lt;code&gt;locations&lt;/code&gt;. This means that they will not match with the names in our original table. We also created some additional columns. To create tables that will match the original table we need to tidy up by:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;selecting the original columns in lookup plus locations_edited (our join field)&lt;/li&gt;
&lt;li&gt;renaming locations to locations_edited in the lookup results (the join field)&lt;/li&gt;
&lt;li&gt;join the tables&lt;/li&gt;
&lt;li&gt;drop the locations-edited column&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lookup &amp;lt;- lookup %&amp;gt;% 
  select(records, locations, locations_edited, id) 

coordlookup &amp;lt;- coordlookup %&amp;gt;%
  rename(locations_edited = locations)

res &amp;lt;- left_join(lookup, coordlookup, by = &amp;quot;locations_edited&amp;quot;) %&amp;gt;% 
  select(-locations_edited)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To join the data back together we now need to do two things. First we filter the results from the original search to those that are &lt;code&gt;status == &amp;quot;OK&amp;quot;&lt;/code&gt; and then bind the &lt;code&gt;res&lt;/code&gt; table to the end.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results_complete &amp;lt;- results %&amp;gt;%
  filter(., status == &amp;quot;OK&amp;quot;) %&amp;gt;% 
  bind_rows(., res)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will write the results to an Excel and csv file that we can use in other programmes such as Tableau for mapping (we will briefly look at mapping with R below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writexl::write_xlsx(results_complete, path = &amp;quot;asean_geocode_complete.xlsx&amp;quot;)
write_csv(results_complete, path = &amp;quot;asean_geocode_complete.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a complete set of geocoded results with 5,198 locations from 5,206. That is pretty good. However, having obtained the geocoded data and joined it onto our original data.frame we now need to look at the quality of the return.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;assessing-the-quality-of-geocoding&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Assessing the Quality of Geocoding&lt;/h3&gt;
&lt;p&gt;So far we have focused on getting geocoded data without really looking at it. To assess the quality of the data that has been returned we should take a look at the location type field. The API documentation for these entries can be found &lt;a href=&#34;https://developers.google.com/maps/documentation/geocoding/intro&#34;&gt;here&lt;/a&gt; and in the &lt;code&gt;geocode_url()&lt;/code&gt; documentation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results_complete %&amp;gt;%
  drop_na(location_type) %&amp;gt;% 
  count(location_type, sort = TRUE) %&amp;gt;% 
  mutate(prop = prop.table(n))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##   location_type        n  prop
##   &amp;lt;chr&amp;gt;            &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
## 1 ROOFTOP           2155 0.415
## 2 GEOMETRIC_CENTER  1848 0.356
## 3 APPROXIMATE       1195 0.230&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The API documentation fills us in on what is going on here.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;quot;location_type stores additional data about the specified location. The following values are currently supported:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“ROOFTOP” indicates that the returned result is a precise geocode for which we have location information accurate down to street address precision.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“RANGE_INTERPOLATED” indicates that the returned result reflects an approximation (usually on a road) interpolated between two precise points (such as intersections). Interpolated results are generally returned when rooftop geocodes are unavailable for a street address.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“GEOMETRIC_CENTER” indicates that the returned result is the geometric center of a result such as a polyline (for example, a street) or polygon (region).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“APPROXIMATE” indicates that the returned result is approximate.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What this tells us is that Google believes it has reached rooftop accuracy for 2155 records but has selected the geometric centre or an approximate value for around 58% of the entries. Lets take a closer look at the geometric center data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results_complete %&amp;gt;% 
  filter(location_type == &amp;quot;GEOMETRIC_CENTER&amp;quot;) %&amp;gt;% 
  select(locations, lat, lng, formatted_address)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,848 x 4
##    locations                                 lat     lng formatted_address
##    &amp;lt;chr&amp;gt;                                   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;            
##  1 AAHRI, Bangkok, Thailand                 13.8  101.   50, กรมประมง, ถน…
##  2 Aarhus Univ Hosp, Aarhus, Denmark        56.2   10.2  Nørrebrogade, 80…
##  3 Abasyn Univ, Peshawar, Pakistan          34.0   71.6  Ring Road, Chars…
##  4 Abdul Wali Khan Univ, Mardan, Pakistan   34.2   72.0  Nowshera Mardan …
##  5 Abertay Univ, Dundee DD1 1HG, Scotland   56.5   -2.97 Bell St, Dundee …
##  6 Aberystwyth Univ, Ceredigion, Wales      52.4   -4.07 Penglais Campus,…
##  7 Aberystwyth Univ, Dyfed, Wales           52.4   -4.07 Penglais Campus,…
##  8 ABRII, Karaj, Iran                       35.8   51.0  Karaj, Alborz Pr…
##  9 Absyn Univ Peshawar, Peshawar, Pakistan  34.0   71.6  Ring Road, Chars…
## 10 Acad Ciencias Cuba, C Habana, Cuba       23.1  -82.4  Havana, Cuba     
## # ... with 1,838 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A review of these results suggests that the geometric center data is pretty good. In the past we might have ended up in a different country. But what about the approximate results?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results_complete %&amp;gt;% 
  filter(location_type == &amp;quot;APPROXIMATE&amp;quot;) %&amp;gt;% 
  select(locations, lat, lng, formatted_address)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,195 x 4
##    locations                           lat     lng formatted_address      
##    &amp;lt;chr&amp;gt;                             &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                  
##  1 Acad Sci Czech Republic, Brno, …  49.2   16.6   Brno, Czechia          
##  2 Acad Sci Czech Republic, Ceske …  49.0   14.5   Ceske Budejovice, Czec…
##  3 Acad Sinica, Beijing, Peoples R…  39.9  116.    Beijing, China         
##  4 Achva Acad Coll, Mobile Post Sh…  31.7   34.6   Shikmim, Ashkelon, Isr…
##  5 ADAS UK Ltd, Cambs, England       52.2    0.122 Cambridgeshire, UK     
##  6 Adv Choice Econ Pty Ltd, Batema… -25.3  134.    Australia              
##  7 AFRIMS Entomol Lab, Kamphaeng P…  16.5   99.5   Kamphaeng Phet, Thaila…
##  8 Agcy Consultat &amp;amp; Res Oceanog, L…  45.2    1.97  19320 La Roche-Canilla…
##  9 Agcy Marine &amp;amp; Fisheries Res Ind…  -6.18 107.    Jakarta, Indonesia     
## 10 Agcy Marine &amp;amp; Fisheries Res, Ja…  -6.18 107.    Jakarta, Indonesia     
## # ... with 1,185 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The approximate results are a mixed bag, in some cases the coordinates focus on a city or town. In other cases such as &lt;code&gt;Adv Choice Econ Pty Ltd, Bateman, Australia&lt;/code&gt; the coordinate is for a country and so on.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;preprocess-the-data-and-rerun-the-query&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preprocess the Data and Rerun the Query&lt;/h2&gt;
&lt;p&gt;This suggests to me at least that while the geocoding is OK the prevalence of geometric centre and approximate results suggests that we might want to run this again but this time edit the location names first to see if we can improve the accuracy of the results. We now know that we can geocode pretty much all of this data. What we are interested in now is whether we can improve the accuracy of the geocoding.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# import data and separate out the organisation country and city into new columns
affiliation2 &amp;lt;- read_csv(&amp;quot;https://github.com/wipo-analytics/data-handbook/raw/master/affiliation_records.csv&amp;quot;) %&amp;gt;%
  separate(locations, c(&amp;quot;organisation&amp;quot;, &amp;quot;city&amp;quot;, &amp;quot;country&amp;quot;), sep = &amp;quot;,&amp;quot;, remove = FALSE) 

# import abbreviations
wos_abbreviations &amp;lt;- read_csv(&amp;quot;https://github.com/wipo-analytics/data-handbook/raw/master/wos_abbreviations.csv&amp;quot;, 
  col_types = cols(abbreviation = col_character(), 
    text = col_character()))

# function to replace the abbreviations
replaceabbr &amp;lt;- function(pattern, replacement, var) {
  replacement &amp;lt;- rep(replacement, length(pattern)) 
    for (i in seq_along(pattern)) {
        var &amp;lt;- gsub(pattern[i], replacement[i], var)
        }
  var
} 

# regularise organisation names

affiliation2 &amp;lt;- affiliation2 %&amp;gt;%  
  mutate(organisation_edited = str_to_title(.$organisation)) %&amp;gt;% 
  mutate(city = str_to_title(.$city)) %&amp;gt;% # added
  mutate(country = str_to_title(.$country)) #added

# fix abbreviations

affiliation2$organisation_edited &amp;lt;- 
  replaceabbr(wos_abbreviations$regex, wos_abbreviations$text, affiliation2$organisation_edited)

# unite cleaned up fields

affiliation2 &amp;lt;- affiliation2 %&amp;gt;% 
  unite(., locations_edited, c(organisation_edited, city, country), sep = &amp;quot;,&amp;quot;, remove = FALSE)

# run the search 
 
run1 &amp;lt;- placement::geocode_url(affiliation2$locations_edited, auth = &amp;quot;standard_api&amp;quot;, privkey = key, clean = TRUE, add_date = &amp;#39;today&amp;#39;, verbose = TRUE)

# drop the input-url and rename for join
run1 &amp;lt;- run1 %&amp;gt;%
  select(-8) %&amp;gt;% 
  rename(locations_edited = locations)

# join to the input table

res_complete &amp;lt;- left_join(affiliation2, run1, by = &amp;quot;locations_edited&amp;quot;) 

res_complete &amp;lt;- res_complete %&amp;gt;% 
  mutate(duplicate_id = duplicated(id)) %&amp;gt;% 
  filter(duplicate_id == &amp;quot;FALSE&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we join the two tables together we discover that we arrive at 5232 rather than 5,206 results. The reason for this is that the name harmonisation has created duplicated names from formerly distinct names. The Google API returns duplicate entries in these cases. These duplicate entries have been filtered out above. We will come on to other forms of duplication below.
&lt;!---investigate this further. The Ids are distinct in affiliation 2 but then x entries are duplicated in the return from the API ---&gt;&lt;/p&gt;
&lt;p&gt;Ok let’s take a look at our results to assess whether this is an improvement.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;run1 %&amp;gt;%
  drop_na(location_type) %&amp;gt;% 
  count(location_type, sort = TRUE) %&amp;gt;% 
  mutate(prop = prop.table(n))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##   location_type        n  prop
##   &amp;lt;chr&amp;gt;            &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
## 1 ROOFTOP           2280 0.439
## 2 GEOMETRIC_CENTER  1927 0.371
## 3 APPROXIMATE        981 0.189&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What this has done is improved the rooftop resolution by a couple of percentage points and improved the geometric centre results by about the same. The approximate score has dropped to 19% from 23% so this is definitely progress. In total 214 records have moved up from the approximate to the rooftop or geometric centre location_types. As this suggests, improving the quality of geocoding matters and it is therefore worth putting the effort into improving the resolution of the results.&lt;/p&gt;
&lt;div id=&#34;duplicated-affiliation-names&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Duplicated Affiliation Names&lt;/h3&gt;
&lt;p&gt;It will not have escaped your attention that in reality our original input data contained a significant amount of duplication on organisation names. This becomes obvious when we review the organisation edited field. We can rapidly see multiple entries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;affiliation2 %&amp;gt;% 
  count(organisation_edited, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4,042 x 2
##    organisation_edited              n
##    &amp;lt;chr&amp;gt;                        &amp;lt;int&amp;gt;
##  1 University Putra Malaysia       19
##  2 Chinese Academy Science         15
##  3 Mahidol University              15
##  4 Prince Songkla University       14
##  5 University Philippines          14
##  6 Cnrs                            12
##  7 Department Fisheries            12
##  8 Fisheries Research Agency       12
##  9 Indonesian Institute Science    12
## 10 Ministry Health                 12
## # ... with 4,032 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are a number of reasons for this. In some cases researchers may list different departments or institutes along with the name of their organisation. In other cases an organisation (such as the Chinese Academy of Science or CNRS) may have multiple offices within or outside a particular country. In still other cases, such as Department Fisheries or Ministry Health we are lumping together organisations that share the same name but are distinct entities.&lt;/p&gt;
&lt;p&gt;Lets take a closer look at this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;affiliation2 %&amp;gt;% 
  select(locations, organisation_edited) %&amp;gt;% 
  head(20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 20 x 2
##    locations                                 organisation_edited          
##    &amp;lt;chr&amp;gt;                                     &amp;lt;chr&amp;gt;                        
##  1 AAHL, Vic, Australia                      Aahl                         
##  2 AAHRI, Bangkok, Thailand                  Aahri                        
##  3 Aarhus Univ Biosci, Roskilde, Denmark     Aarhus University Bioscience 
##  4 Aarhus Univ Hosp, Aarhus, Denmark         Aarhus University Hospital   
##  5 Aarhus Univ, Aarhus C, Denmark            Aarhus University            
##  6 Aarhus Univ, Aarhus, Denmark              Aarhus University            
##  7 Abasyn Univ, Peshawar, Pakistan           Abasyn University            
##  8 Abdul Wali Khan Univ, Mardan, Pakistan    Abdul Wali Khan University   
##  9 Abertay Univ, Dundee DD1 1HG, Scotland    Abertay University           
## 10 Aberystwyth Univ, Ceredigion, Wales       Aberystwyth University       
## 11 Aberystwyth Univ, Dyfed, Wales            Aberystwyth University       
## 12 Abo Akad Univ, Turku, Finland             Abo Akad University          
## 13 ABRII, Karaj, Iran                        Abrii                        
## 14 Absyn Univ Peshawar, Peshawar, Pakistan   Absyn University Peshawar    
## 15 Acad Ciencias Cuba, C Habana, Cuba        Academy Ciencias Cuba        
## 16 Acad Nat Sci Philadelphia, Philadelphia,… Academy Natural Science Phil…
## 17 Acad Sci Czech Republ, Ceske Budejovice,… Academy Science Czech Republ 
## 18 Acad Sci Czech Republic, Brno, Czech Rep… Academy Science Czech Republ…
## 19 Acad Sci Czech Republic, Ceske Budejovic… Academy Science Czech Republ…
## 20 Acad Sci Czech Republic, Prague, Czech R… Academy Science Czech Republ…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the case of Aarhus University, we can see that we have Aarhus University Bioscience, Aarhus University Hospital and an Aarhus University. In some cases the entities belong to the organisation but might otherwise be regarded as distinct (Aarhus University Hospital) while in another the Bioscience reference refers to a department (but gives the impression that it may be a separate University as for Agricultural cases). To add to this we note that there are locations in Aarhus and Roskilde and a minor variant (Aarhus C) in the address field.&lt;/p&gt;
&lt;p&gt;As this makes clear address field data in scientific names is pretty messy because authors choose how to denote their affiliations, and are perhaps rebelling against the tyranny of performance indicators and endless research assessment exercises.&lt;/p&gt;
&lt;p&gt;Cleaning up author affiliation and author names is generally a painful process (and we will come back to this in a future article). One challenge with name cleaning is the availability of criteria to determine if a name can be merged. For example, we could comfortably merge some of the Aarhus University references above but we might want to keep distinct locations distinct (for example Aarhus is around 150km by road from Roskilde). The availability of georeferenced data, bearing in mind the approximates issue, could provide us with additional information for informed decision making during name cleaning. Let’s take a quick look at the formatted address field in our results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res_complete %&amp;gt;% 
  select(formatted_address, organisation_edited) %&amp;gt;% 
  head(20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 20 x 2
##    formatted_address                                organisation_edited   
##    &amp;lt;chr&amp;gt;                                            &amp;lt;chr&amp;gt;                 
##  1 5 Portarlington Road, Newcomb VIC 3219, Austral… Aahl                  
##  2 50, กรมประมง, ถนนพหลโยธิน, ลาดยาว จตุจักร Bangk…    Aahri                 
##  3 Aarhus University, 150, Frederiksborgvej 399, 4… Aarhus University Bio…
##  4 Nørrebrogade, 8000 Aarhus, Denmark               Aarhus University Hos…
##  5 Langelandsgade 140, 8000 Aarhus, Denmark         Aarhus University     
##  6 Langelandsgade 140, 8000 Aarhus, Denmark         Aarhus University     
##  7 Ring Road, Charsadda Link، Near Patang Chowk، A… Abasyn University     
##  8 Nowshera Mardan Rd, Muslimabad, Mardan, Khyber … Abdul Wali Khan Unive…
##  9 Bell St, Dundee DD1 1HG, UK                      Abertay University    
## 10 Penglais Campus, Penglais, Aberystwyth SY23 3FL… Aberystwyth University
## 11 Penglais Campus, Penglais, Aberystwyth SY23 3FL… Aberystwyth University
## 12 Domkyrkotorget 3, 20500 Åbo, Finland             Abo Akad University   
## 13 Karaj, Alborz Province, Iran                     Abrii                 
## 14 Ring Road, Charsadda Link، Near Patang Chowk، A… Absyn University Pesh…
## 15 Havana, Cuba                                     Academy Ciencias Cuba 
## 16 1900 Benjamin Franklin Pkwy, Philadelphia, PA 1… Academy Natural Scien…
## 17 Branišovská 1645/31A, České Budějovice 2, 370 0… Academy Science Czech…
## 18 Palackého tř. 1946/1, 612 42 Brno-Královo Pole,… Academy Science Czech…
## 19 Branišovská 1645/31A, České Budějovice 2, 370 0… Academy Science Czech…
## 20 Žitná 609/25, 110 00 Praha-Nové Město, Czechia   Academy Science Czech…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we can see that the Google data suggests that some of these entities share an address. Based on this we may want (with appropriate attention to the location type field as a guide) to merge or not merge names in our list. If we take a look at the counts for shared addresses it becomes clear that we may want to use a step wise approach depending on the level of confidence in the location type field.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res_complete %&amp;gt;%
  filter(location_type == &amp;quot;ROOFTOP&amp;quot;) %&amp;gt;% 
  count(formatted_address, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,653 x 2
##    formatted_address                                                     n
##    &amp;lt;chr&amp;gt;                                                             &amp;lt;int&amp;gt;
##  1 113 Soi Klong Luang 17, Tambon Khlong Nung, Amphoe Khlong Luang,…    16
##  2 18 Hoàng Quốc Việt, Nghĩa Đô, Cầu Giấy, Hà Nội, Vietnam              14
##  3 169 Long Had Bangsaen Rd, Tambon Saen Suk, อำเภอ เมืองชลบุรี Cha…       11
##  4 15 Karnjanavanit Soi 7 Rd, Kho Hong, Amphoe Hat Yai, Chang Wat S…     9
##  5 999 Phutthamonthon Sai 4 Rd, Tambon Salaya, Amphoe Phutthamontho…     9
##  6 Jl. Pasir Putih Raya No.1, RT.8/RW.10, Kota Tua, Pademangan Tim.…     9
##  7 New Administration Building, Miagao, 5023 Iloilo, Philippines         9
##  8 02 Nguyễn Đình Chiểu, Vĩnh Thọ, Thành phố Nha Trang, Vĩnh Thọ Th…     8
##  9 Nørregade 10, 1165 København, Denmark                                 8
## 10 Pesthuislaan 7, 2333 BA Leiden, Netherlands                           8
## # ... with 1,643 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;quickly-mapping-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Quickly Mapping the Data&lt;/h3&gt;
&lt;p&gt;To finish off lets quickly map the data. We will focus on mapping in more detail in other articles in the Handbook. For the moment we will use the leaflet package for this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;leaflet&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(leaflet)
mapdata &amp;lt;- res_complete %&amp;gt;% 
  filter(., status == &amp;quot;OK&amp;quot;)
mapdata &amp;lt;- leaflet(mapdata) %&amp;gt;%
  addTiles() %&amp;gt;%
  addCircleMarkers(~lng, ~lat, popup = .$locations_edited, radius = mapdata$records / 20, weight = 0.1, opacity = 0.2, fill= TRUE, fillOpacity = 0.2) 
mapdata&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:800px;height:480px;&#34; class=&#34;leaflet html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;options&#34;:{&#34;crs&#34;:{&#34;crsClass&#34;:&#34;L.CRS.EPSG3857&#34;,&#34;code&#34;:null,&#34;proj4def&#34;:null,&#34;projectedBounds&#34;:null,&#34;options&#34;:{}}},&#34;calls&#34;:[{&#34;method&#34;:&#34;addTiles&#34;,&#34;args&#34;:[&#34;//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&#34;,null,null,{&#34;minZoom&#34;:0,&#34;maxZoom&#34;:18,&#34;tileSize&#34;:256,&#34;subdomains&#34;:&#34;abc&#34;,&#34;errorTileUrl&#34;:&#34;&#34;,&#34;tms&#34;:false,&#34;noWrap&#34;:false,&#34;zoomOffset&#34;:0,&#34;zoomReverse&#34;:false,&#34;opacity&#34;:1,&#34;zIndex&#34;:1,&#34;detectRetina&#34;:false,&#34;attribution&#34;:&#34;&amp;copy; &lt;a href=\&#34;http://openstreetmap.org\&#34;&gt;OpenStreetMap&lt;\/a&gt; contributors, &lt;a href=\&#34;http://creativecommons.org/licenses/by-sa/2.0/\&#34;&gt;CC-BY-SA&lt;\/a&gt;&#34;}]},{&#34;method&#34;:&#34;addCircleMarkers&#34;,&#34;args&#34;:[[-38.1521037,13.8427152,55.6955174,56.1704229,56.1682371,56.1682371,34.03155,34.1897214,56.463307,52.418073,52.418073,60.4509869,35.8034776,34.03155,23.1367346,39.9570904,48.9773099,49.2177859,48.9773099,50.0775517,50.0001644,39.948367,25.0421852,44.6962067,16.3763357,31.744109,31.653523,40.6349348,44.4102211,52.4083974,52.2052973,21.107642,37.8552535,39.180949,-32.059446,14.2193401,23.0960979,50.830909,-1.3409189,16.4827798,-6.1845778,45.195437,-6.17511,-6.17511,1.2996113,1.288119,-22.7389178,51.804213,13.7179357,37.3248859,45.364652,47.6007732,35.8034776,29.1416432,1.333013,1.333013,-25.6143832,30.021554,6.0372967,35.7960737,31.9927222,34.792932,23.577282,2.991686,53.4364995,1.333013,1.333013,1.333013,54.5508824,1.333013,1.333013,-2.1633469,48.1135487,48.8397733,46.3987338,39.1436913,35.161123,35.3563473,35.1930576,35.1801883,5.721828,5.721828,30.075807,-7.264084,-7.2709281,36.0614634,14.0761416,14.0761416,43.5248171,43.293621,35.4475073,37.2821251,26.199204,26.199204,39.8008185,39.8008185,39.6266996,39.7269733,62.6737399,62.6737399,27.1888822,19.9943808,33.2790141,25.1347093,35.768276,31.832817,10.0786931,58.2761,56.8111578,47.1739612,53.5332554,53.5332554,55.02096,27.9135016,22.5753178,14.0208391,43.4623057,31.832817,34.746644,34.7337339,45.5415526,45.5415526,44.2227398,44.2227398,44.067634,32.8031004,45.4498521,35.6688048,40.7813241,-14.270972,33.8929063,30.018923,-6.17511,51.4855766,15.8639822,15.8639822,55.7114837,55.7114837,55.7114837,21.0277644,27.5767138,-0.914559,-0.914559,-0.914559,7.867633,7.8291229,17.7287213,15.1456612,1.3023151,31.860048,31.284916,32.633459,31.820591,17.8680356,25.1719805,13.0110816,11.4914533,11.3917829,-18.914032,40.822072,-8.4095178,-22.6953281,-22.9659181,-23.5008933,10.6706389,60.4360587,12.9895003,-30.0002315,-41.4545196,36.7212737,9.1341949,13.8427152,40.8020712,36.1788032,43.318334,-25.4518387,-33.9487268,-28.5305539,-25.730024,-33.9252988,37.6905882,44.2334469,39.103706,40.6308283,40.6308283,33.4242399,13.7664451,24.8773785,13.7563309,44.6365107,39.0254895,34.242861,26.1216652,39.6685765,40.7097557,30.8340774,36.6719519,33.4333856,38.8867675,42.0284465,51.0537649,50.9085325,43.729855,43.729855,26.2124013,46.148411,13.064273,13.064273,5.720366,14.0761416,14.0803142,14.0761416,14.0761416,14.0761416,14.0761416,14.0761416,31.491169,26.7247986,27.181348,35.4475073,21.4022222,13.7535342,1.3030314,1.2996113,-22.2710727,14.6394505,14.6394505,14.6394505,46.1607584,46.1607584,38.2788534,-8.6892043,-8.1639797,10.607936,-8.6892043,-31.9828221,-8.6738315,-31.9828221,23.7309322,36.204824,32.5933574,-37.0024756,-36.8532194,12.2374439,-27.6728168,-27.428,-20.9175738,-33.8373959,-19.2677,-31.9815143,-33.8743446,-33.8743446,-35.2776999,48.2691806,-35.243192,13.3809761,7.2046668,24.8253962,35.576338,15.4584166,15.4580526,9.2940027,-6.1847254,11.5743107,-0.6559126,-5.0031118,-6.5828594,54.570464,-8.6663188,-6.5971469,40.2061028,54.179855,9.8499911,13.8535109,13.8533828,25.2677203,-6.89148,-6.89148,23.7947944,24.7196252,23.962443,22.845641,24.7471492,22.6416005,24.721368,23.9903009,53.2265132,53.2295205,13.7329635,-10.1719444,12.6112485,48.2522461,48.164891,48.164891,48.3986881,51.0094974,44.6830036,39.9041999,39.9245081,39.9041999,22.543096,39.9041999,39.9050945,39.9619537,39.970954,29.899044,13.1919261,31.262218,47.6099738,32.909526,-31.9923799,19.2979079,32.3704512,53.546752,21.3329195,21.3329195,15.7322833,16.0861081,14.6082518,25.2488712,11.0390221,10.6779085,10.6779085,-20.9175738,13.1482156,13.1482156,13.363609,43.8597347,23.7830064,14.4765477,13.7673903,32.2376263,-20.9175738,1.3165196,1.4748305,13.7563309,50.116073,1.3033444,54.18286,21.6420138,33.0249301,35.6050574,-6.17511,56.0254321,-37.8136276,65.8241937,-27.495222,-20.9175738,14.0783982,14.0208391,13.716512,14.0783982,14.0783982,3.1238369,42.8765528,42.8765528,43.6462328,41.87834,3.0738379,21.3329195,19.7229948,44.49651,26.0755026,7.9188955,35.6894875,67.2892218,49.4874592,-6.5540836,-6.5540836,-6.5540836,-6.5540836,-6.5971469,48.2365448,14.5667539,1.53344,13.8429969,22.5797135,42.3504997,43.2239726,-33.8641859,41.3797788,7.1897659,7.1897659,-6.1848621,23.7801679,-7.7050532,-7.150975,-7.5360639,24.228476,52.2714048,-7.952194,-7.952194,-7.952194,-6.4024844,29.2366172,-27.0534914,40.2518435,40.3440522,52.212991,49.2618515,16.7694617,52.8787126,51.496715,42.3628858,43.1175731,41.8267718,23.9903009,35.4475073,34.0389055,34.7911399,-6.7155252,40.9546869,42.6944431,13.746166,51.165691,14.6571214,14.0208391,15.7352928,15.7322833,16.0861081,15.7337901,14.631019,14.653664,23.3790333,-27.507458,12.56482,13.2859842,12.598683,13.2859842,12.598683,13.2859842,13.2859842,13.2859842,13.2859842,13.2859842,13.2859842,13.2859842,13.2859842,13.2859842,-0.789275,52.0102445,14.9891321,28.9795018,51.0481705,12.9088657,12.9088657,12.879721,35.7518326,51.4199883,36.9885993,-16.872011,30.0274734,30.0274734,37.7698646,39.4255722,38.4801914,34.0575651,36.8124451,33.7838235,36.6516548,34.1376576,42.9295084,11.4759335,10.0451618,10.0309641,10.0309641,45.412709,10.0309641,10.0309641,10.0309641,10.0309641,51.4866271,51.4891209,51.4866271,13.7522,49.3052575,53.146873,53.146873,45.3875812,40.4428081,35.384742,41.509959,36.5658729,36.5658729,36.5658729,37.5155257,-6.17511,37.5859218,50.8779545,50.6696875,50.8779545,50.6696875,-41.2715278,14.716677,52.0896802,48.7126467,48.7126467,-29.9145993,50.6052632,50.6052632,43.6388234,38.6106481,43.6455357,11.6060191,23.9035781,45.4506875,52.0896802,26.9467067,22.780868,13.0205174,19.1398223,19.1398223,20.1838505,20.1838505,26.8200458,15.7304612,15.7304612,15.7325266,15.7304612,15.7293533,15.7304612,13.0205174,14.8017738,17.71718,20.9039984,12.9141417,9.988304,43.5819086,-5.3822904,-23.319648,35.8781526,24.173051,1.428593,10.607936,40.5667611,43.8079675,2.9330912,13.8821965,13.6853625,13.6904194,18.8059685,15.6061081,15.6061081,57.6888144,13.8779717,12.5850571,12.5427803,13.7232303,-12.3717852,-12.3717852,-33.8468792,50.0871106,13.5133962,15.870032,15.870032,13.733445,13.5475216,13.733445,13.733445,13.5475216,13.5475216,13.8361545,13.7334713,33.4514464,33.4514464,5.2649115,22.9988084,22.9209971,18.7421777,18.766833,18.8059685,18.8059685,18.8059685,35.7015356,35.6277374,41.9267847,18.8059685,11.3542837,33.5134851,33.5138712,40.408141,39.939,30.318764,30.274084,36.061255,31.2288759,32.072145,39.9041999,40.408141,35.7518326,36.067108,23.3790333,30.7378118,36.067108,31.2303904,31.491169,24.4752847,24.880095,24.4752847,39.9041999,39.911431,31.2303904,40.408141,23.3790333,35.86166,23.12911,19.983035,30.7378118,33.1401715,24.880095,36.155285,36.055253,31.2921072,31.1928774,22.543096,30.592849,24.4752847,40.004202,31.2303904,22.4162632,22.4162632,22.4710021,22.4710021,35.8468291,29.612289,35.1759333,35.1759333,34.7752533,54.3460549,12.9246021,43.6323305,34.3852029,34.399407,13.8821965,13.879302,18.7903062,13.7338328,13.2786737,13.7341969,13.7341969,10.4340465,37.504147,24.957537,36.6282989,36.6282989,36.6282989,36.3699872,36.3699872,36.3699872,25.045532,24.1390751,20.1838505,19.1384518,19.4326077,41.1526015,41.1526015,35.6894875,43.610769,21.0311643,43.646212,-21.3222914,4.0520216,25.234479,22.3028474,22.3371397,22.3371397,34.6760942,33.3674223,15.7322833,32.8572718,13.0205174,46.148411,45.4805862,43.7974988,37.6538968,40.83508,-18.9330468,29.1416432,49.182863,48.6947322,48.8442935,-22.2710727,42.480214,48.7046138,48.8476037,46.192722,45.787406,43.6322742,-22.2710727,48.7070221,48.8233424,43.6117956,44.083566,43.5572646,23.74899,22.5519759,12.9649215,15.870032,15.870032,11.812367,15.870032,15.870032,15.870032,15.870032,15.870032,15.870032,7.1630315,7.1630315,13.7563309,15.870032,13.7563309,12.61134,15.870032,7.1897659,12.5850571,13.4286514,8.0264776,40.7588467,40.7588467,-31.8616038,10.0302905,10.0315389,32.7836117,21.121444,14.153601,47.8743528,-5.140086,27.503591,37.271674,30.6006773,50.9582607,39.6477653,40.573436,45.530937,40.8075355,41.0035658,48.1412879,45.8139584,13.7701299,41.413268,-27.4460245,51.5510068,33.6518263,46.8628042,-27.7012799,41.3307937,36.5167782,-19.4914108,-17.548201,-8.4095178,-8.6862895,-8.6705261,-0.8795067,38.8556199,-8.6705261,-8.6705261,-6.2772446,14.6433383,-8.6705261,47.6263898,47.6263898,50.7522773,-20.9175738,1.7941225,35.4475073,-4.0434771,42.4534492,39.0941963,22.7091849,13.733445,-27.51007,-37.831122,35.8617292,41.3853788,13.879302,46.227638,-12.412132,10.651698,39.636702,43.3897222,41.3874996,36.5321042,41.3874996,38.9848295,41.967092,39.666525,40.4411076,37.4119846,42.2258078,26.9467067,15.4563902,15.9128998,30.7126663,26.8594788,-19.3274869,-19.3274869,-38.1568828,-38.1521037,-37.898461,-19.3274869,-35.2748358,-27.495222,-20.9175738,-25.274398,-35.3063342,-35.2719452,-34.9668957,-31.9944498,-27.498082,-38.1521037,-19.3274869,-42.9062765,-42.8867572,-42.8867133,-19.3274869,-42.8867572,-42.9062765,-42.8867133,-31.9484937,-27.495222,-19.3274869,-42.8867133,-42.8867133,-19.3274869,-42.9062765,-37.9062737,-6.17511,52.3555177,3.1238369,-6.5804981,17.421021,42.0509025,33.7993178,33.7993178,33.7993178,23.7247599,33.7993178,33.7993178,33.7993178,31.861399,21.121444,50.6052632,13.7563309,51.4929292,22.1599753,41.6832069,13.7563309,13.7563309,19.9104798,13.7426245,7.0086472,7.0086472,13.7563309,13.7354378,9.1341949,-3.5106508,23.817248,23.7764022,20.1838505,3.50259,43.610769,-6.5523871,-6.5523871,-6.5523871,7.9519331,28.447433,24.1390751,24.1390751,24.1390751,42.5797871,48.9146746,46.2383985,34.2239869,13.7779649,13.7090087,1.292421,11.5947388,12.3714277,-18.9330468,39.552707,42.2405989,-3.6255302,-12.0463731,6.3702928,-17.5388435,-6.5770606,12.808369,1.5343616,35.021542,35.0206383,-16.993788,53.10802,53.10802,17.0727808,60.1698557,40.6314406,40.7685406,40.6314406,40.8200471,-32.0061951,-32.0061951,35.1801883,48.8207053,15.7322833,16.0755258,15.6711339,24.000073,35.8900521,36.9395813,-37.7224789,44.6365812,25.6730738,38.868263,38.868263,55.756381,55.756381,55.7855742,55.735202,-12.555569,7.313627,7.313627,7.313627,6.82652,22.4485685,24.000073,55.756381,26.748484,14.262641,14.5642894,52.62978,-37.8467404,14.554729,18.5117823,1.2894506,28.6287132,51.9962559,5.7913745,51.9856484,22.4809431,6.095112,6.523256,13.7595319,12.9816227,14.5713683,1.2955817,-27.468683,14.6595538,-37.8344931,28.7620739,19.3017627,35.7405436,39.7172044,13.7563309,13.8444731,5.4100664,5.957393,13.8444839,12.61134,-15.5624718,13.8444839,2.9146681,-31.9520619,7.9519331,15.870032,15.870032,13.5475216,-31.9520619,16.8750541,52.518537,13.7595319,13.880496,-31.2532183,13.873153,30.462149,13.7631804,-12.480022,13.8518586,13.3913889,-20.9175738,-27.468712,-42.884785,-19.4914108,-38.2704097,14.675787,3.139003,3.0994012,53.5730276,49.414414,52.2121995,53.5901,3.6135571,1.3550629,10.786336,1.3557324,1.330176,40.2027214,-41.4545196,47.1748359,44.4982234,44.4057165,8.5416661,-7.0511282,-7.0511282,-7.0511282,-7.0511282,41.3833529,21.0277644,22.4902043,-42.8861436,51.05337,2.926361,42.979444,36.4721174,36.4721174,-33.8665582,37.5575367,10.4204929,40.7422255,30.6696818,30.8570581,39.9596731,1.2894506,53.385062,1.281046,34.716561,36.0014258,53.98488,39.7567661,51.4458797,35.6055108,31.2284387,16.7355709,1.346597,-1.2455412,45.207359,39.4836717,12.6813957,12.6235039,12.6235039,47.4037666,52.8326932,51.0052499,41.1579438,47.3768866,36.5658729,-33.763487,-24.8743516,14.716677,43.599211,45.7298161,46.5190557,38.1753675,14.155593,55.9331214,46.194862,33.8508342,33.8416238,33.8416238,33.8416238,33.8502724,-6.1978784,35.8399836,32.8609021,19.8966277,-10.1455832,-15.7312666,-15.7312666,-16.506659,21.0277644,24.917661,24.4882873,33.7925195,23.829545,44.0838594,38.9887199,25.3462553,24.4633885,52.3229133,43.2992144,40.7385199,-38.1468862,3.143882,23.5974705,-17.5388435,40.006757,45.188529,10.4877763,19.5117707,-2.1481458,36.8299606,39.3587759,47.376313,36.332664,52.5965252,34.685728,2.9843449,6.1253969,7.004731,-34.836873,51.0533618,-5.1298363,-5.1298363,-37.7994303,48.8368329,45.8134628,15.3694451,45.826307,48.8433277,13.7661937,36.829309,50.4665484,16.4783882,50.9989161,2.975634,43.1204279,43.1173247,43.1173247,43.1173247,24.826202,10.3398167,45.070312,45.4923087,-3.0866971,4.7109886,-22.8527997,51.9691868,2.9361942,11.987572,11.987572,52.3758916,53.6764136,-1.049677,-22.94885,-1.4743965,7.3042695,12.4721857,28.2441768,21.121444,41.866261,41.866261,-18.1260333,60.1783281,-20.505517,52.518537,-6.17511,-7.0051453,13.7563309,43.6517371,14.1698969,3.0266944,44.6409897,46.0980288,49.2871067,46.5180774,45.0735774,49.3455235,11.5447032,36.3418112,34.7302829,48.006196,36.2262393,-12.470385,39.7994484,34.334253,34.2747293,40.822072,34.2747293,43.0358115,36.3418112,35.3460413,34.334253,32.8123524,37.9161924,33.2395578,34.4088756,38.33445,32.9453761,5.2867189,5.2867189,5.2867189,2.7258058,5.2867189,22.4630017,5.2867189,13.7563309,-37.934105,-31.9520619,5.2867189,3.143524,34.334253,5.2867189,69.6492047,23.600255,51.2035027,51.2190212,54.1827846,-35.0244952,-35.0244952,26.3734854,27.7623195,28.0646888,25.756576,25.7809298,30.4418778,39.9520288,-20.9175738,21.0277644,41.882989,41.882989,47.7969553,20.9909321,36.0085088,13.7563309,3.2353327,3.2353327,30.3437685,10.8230989,21.0469527,36.0085088,35.0116363,20.9111884,50.7221335,50.1175395,40.593486,38.2688373,37.9161924,48.6119298,40.0484514,47.6081576,52.3337568,52.3345709,52.4525264,50.8132068,11.9369424,21.0690314,30.592849,2.9410716,5.8119056,48.5454083,50.9084724,31.2974197,26.076378,26.084946,26.076378,24.479833,33.6266584,33.665815,36.204824,37.6843669,34.458438,-19.921083,-22.875704,-32.0311548,52.0896802,37.422325,-7.7690066,-7.7713847,-7.7713847,-7.7713847,-27.4333583,53.2784293,43.6354613,-8.7048294,37.7687678,37.869984,-30.559482,39.9420219,29.0228626,40.198339,37.4195603,43.610769,37.4138,59.912499,1.3026846,48.6236331,13.723105,1.5769747,54.330034,54.330034,54.330034,54.3271991,51.5408116,41.7283565,38.829629,38.7578708,38.8997145,38.9076089,33.7756178,32.4205489,58.3814058,51.3199845,51.3199845,14.554729,52.2688736,52.2109803,51.0465619,35.4662236,53.410432,3.0199108,15.1099752,36.3504119,15.458414,50.1270675,13.8140293,-0.789275,-0.789275,-0.789275,-0.789275,-0.789275,-0.789275,-0.789275,-8.1149145,42.5893751,36.8429327,36.8429327,36.8429327,57.6898601,57.6981719,41.6693521,11.6484876,31.416111,31.573152,17.6502599,9.556036,12.8102481,25.4242577,24.8508406,24.8508406,-5.139819,7.9381845,-31.9430555,35.7126775,38.2569097,26.465329,43.241944,47.0777872,-18.5000682,-27.9607894,-27.5550897,44.4531131,-8.4095178,50.7715119,23.146518,21.270702,21.150374,21.150374,21.150374,21.150374,21.270702,21.481291,22.8428331,23.7247599,21.481291,22.841686,22.817002,23.12911,23.784262,25.219006,25.280923,26.8429645,26.37902,26.8429645,26.37902,26.37902,36.4919,35.1805921,35.15404,35.15404,35.15404,35.15404,35.15404,13.0051448,39.931821,37.09024,42.7598957,20.863945,20.044412,19.5663947,20.044412,19.5663947,20.044412,-4.0086915,-4.0086915,-4.0086915,34.7700055,34.7108344,-7.2933372,30.305626,30.274084,36.3544548,36.3544548,21.0032473,21.0358471,21.0374287,21.082786,21.0374287,21.0691162,21.0216361,21.082786,21.0480817,20.996017,21.0056183,38.7192502,50.9089375,50.9091421,37.5572321,37.5556003,27.5303235,21.4272283,51.9244201,35.245152,10.607936,14.6243572,42.0026813,42.3356499,42.3770029,42.3770029,33.3546156,34.1060817,47.5012456,-5.1337227,-5.1337227,-5.1337227,7.0156498,6.980249,20.9050033,31.7945578,31.7945578,49.4191402,47.1216472,37.7333464,37.7333464,35.240117,36.457113,-0.789275,37.9838096,35.3255514,51.3517518,51.3517518,52.2109803,53.5332554,50.9411415,34.2904302,34.773205,5.0559546,55.908404,41.1834,40.7679759,30.8570581,34.6788461,40.5900822,30.8242142,34.370628,34.3852029,34.403123,34.403123,34.403123,35.6844154,42.3549953,43.231413,35.86166,12.5002403,10.8222075,10.773266,10.773266,20.8262289,10.8777684,10.773266,40.716792,43.2203266,43.2203266,43.0779575,42.9848542,43.0830353,43.0779575,43.0779575,41.6089711,9.0580044,22.3380838,22.396428,22.3028474,22.535933,22.3363998,22.3363998,33.2914101,44.4317226,48.8370792,27.2677141,51.441035,-5.1337227,3.1278409,20.666916,6.1247136,-41.4491766,3.242274,43.6524201,10.7528588,1.3322693,38.9226843,16.8759085,13.6130528,34.605864,32.613785,33.599411,30.475126,30.475126,30.592849,16.4582393,16.4590908,16.4590908,38.9063188,40.8752748,52.517883,27.6252995,27.6252995,28.178915,47.5012456,30.865825,35.6894875,33.3537315,49.2856525,34.6912688,38.8980299,22.7271723,41.787028,36.4012765,9.0457188,37.211053,9.0457188,9.0457188,35.7347232,-6.5971469,22.7790104,20.1741147,17.71718,25.1821462,23.817248,23.817248,41.3517302,5.4356367,14.554729,35.8934044,41.508149,42.8627231,36.506296,-20.9330043,48.520412,50.7287579,-20.9330043,46.192722,43.610769,47.24712,-22.3016637,43.5199883,49.3402396,35.6891975,35.6891975,3.2543285,3.2543285,33.6349736,10.9316884,52.9629451,51.49891,51.4987997,35.6837823,35.685175,20.8518138,28.6373709,13.1311821,13.6299103,13.0218597,22.3149274,23.7491949,41.1173345,39.7738832,12.5724675,-6.2287941,-6.228835,-6.179152,-8.5841064,-8.691707,-6.17511,-6.1254164,-6.5988243,-6.3543475,-6.228835,-0.789275,-6.3543475,-6.1254164,-0.789275,4.33939,-6.904033,-6.355067,-6.4901067,-6.1254164,-6.2288362,0.6246932,-6.2287941,-6.2287941,-6.1383768,-7.090911,-7.090911,33.7993178,39.0769826,37.449627,-38.0316646,35.248712,13.7563309,11.5563738,-42.7759122,37.4639089,37.4639089,36.8456427,47.0378741,40.806114,43.6188823,44.7876792,47.3166461,48.7640827,43.682355,43.617897,16.201667,48.1131215,46.3674684,43.6025923,44.7876792,48.7947182,48.8272783,50.9945496,10.8719808,-53.1633626,60.472024,62.6737399,60.472024,21.1023498,60.472024,63.4373698,21.1023498,10.7856405,12.2766354,12.2521546,12.2681437,12.2681437,21.1023498,1.352083,56.1681384,21.0363681,21.0481227,35.6894875,53.1434501,1.2816581,13.7656582,21.0477747,55.7114837,19.2601605,10.8230989,10.7968965,57.3617138,45.8129635,19.9104798,56.9092548,11.01087,48.8502105,17.9620778,52.4486,47.80949,39.003571,41.1579438,51.05838,51.0740298,-33.3106292,-6.17511,42.4054196,34.6937378,17.9626187,21.0277644,-0.789275,-8.4095178,44.627814,54.0919438,20.860847,20.860847,20.860847,21.0277644,60.87582,60.40004,60.3878586,53.248005,53.922992,48.640262,35.6656501,3.1689616,3.1689616,54.3273496,24.1477358,1.3030314,37.869768,38.7222524,21.0056183,37.3532156,48.6503438,43.5082016,12.2074691,12.2074691,33.1401715,30.572815,33.4941704,33.4255104,11.5804444,11.5804444,13.5115963,37.406373,-18.8831361,50.6282142,12.2521546,48.8403734,11.5804444,1.4853682,5.2867189,-6.582755,-23.4521053,51.2277411,52.2688736,49.9909791,51.0543422,49.4483389,25.5451771,21.0381111,-23.116124,-22.300872,43.296482,-22.300872,43.6451206,-18.902403,-6.17511,43.6515506,-22.300872,48.278195,-22.300872,11.1649219,50.8562001,1.280434,43.6320424,47.5602535,-37.4713077,27.4838952,-6.89148,4.9773576,-7.2823728,-7.2823728,-5.358378,-6.89148,52.33146,51.9244201,23.0356075,51.212166,49.6835534,48.006196,41.2994958,45.3485098,-6.17511,43.7325237,43.7384176,43.610769,12.5303806,23.776295,23.7770601,23.7764022,23.776295,14.5977636,14.5977636,14.5995124,14.6090537,3.0463955,5.561742,52.008996,3.8436799,3.2543285,3.2543285,3.2543285,3.2543285,3.841391,21.0104936,35.6894875,3.0597615,2.7093834,46.2316338,34.988087,14.1590548,14.1590548,14.5621101,14.557158,-4.4019765,37.2682177,29.4610229,10.8777684,10.8777684,37.464769,37.09024,11.0049836,41.1579438,29.5014255,29.5021395,51.6434028,1.427298,2.8139714,51.0316735,11.3254024,47.6996197,42.0266187,54.3232927,28.9233837,37.2682177,37.4639089,35.6891975,35.6891975,35.798572,35.6234485,43.6451206,-8.6704582,42.48343,14.731556,-8.6704582,-6.2559023,43.296482,43.6515506,43.6451206,13.5115963,-22.300872,48.8513355,38.9940214,14.167774,14.5621101,14.557158,40.627698,16.7221221,32.6431202,10.688854,32.6546275,32.7190781,23.7237244,33.9999538,27.2096603,27.2096603,28.9350686,36.1670273,34.8448526,28.5125971,31.8694943,31.8694943,37.2436448,35.3025236,35.715291,-7.6876752,23.8114667,-2.5105838,38.7122444,41.9027835,29.557669,32.7940463,44.4482151,41.9027835,-6.89148,52.2236937,59.3460909,45.070312,-6.369028,35.6894875,39.705294,39.7158682,-33.9290876,39.1034761,53.1677169,22.4985604,23.7085784,50.0609623,23.8812811,1.548639,4.3321195,-1.4779213,-19.3276927,-16.819013,-19.3276927,36.444154,35.320171,35.8617292,35.6894875,35.6894875,35.6894875,33.5903547,35.6894875,35.6731042,36.054603,26.501301,5.4163459,35.6268386,35.8010655,35.8010655,33.2395578,26.2124013,13.7410754,38.9040198,43.604652,8.7533453,33.4514464,33.4514464,33.4996213,-7.4042404,-7.4042404,-7.4042404,31.482675,33.1401715,33.1401715,31.980171,28.7247282,-0.789275,24.575969,7.6820055,23.128057,31.934065,36.054603,35.671396,-6.2859959,49.9926403,41.8675726,51.7651508,39.2979576,39.3299013,1.479931,52.574333,12.2374439,10.8230989,21.0277644,35.9330832,8.6559176,42.1626947,46.0423559,14.650296,31.1063198,34.342774,31.59584,31.5682318,31.8309938,31.571918,42.290035,39.2758183,53.0409109,-6.8013737,4.245073,35.4769011,35.449156,35.484183,36.584169,36.5459854,37.7687678,37.7687678,37.7687678,37.869984,34.981447,39.1974437,34.6937378,22.7091849,22.6473733,15.9771941,40.9898738,-0.4790996,47.0777872,49.0119199,11.7584052,17.8680356,12.8565441,17.8680356,12.8581843,59.3481484,59.3520742,13.8476452,13.8476452,13.8476452,13.8476452,13.8476452,13.8476452,14.0236026,14.0236026,14.0236026,9.9528702,50.8779545,50.6696875,22.309465,34.8686637,-6.3543475,29.1416432,-21.15246,51.9851034,3.1172854,38.2002577,-4.0552402,-4.0552402,-0.023559,-1.3353467,9.9107386,29.985295,16.4741162,16.4741162,16.4741162,17.8065374,16.4741162,30.4018802,22.8033117,33.9589698,21.5016038,22.1237711,22.309465,13.7308645,13.7298956,13.6511842,13.7298956,13.7298956,13.7298956,13.7298956,13.819025,13.819025,13.6511842,13.8189664,13.6511842,13.8189664,24.7545024,24.7252026,24.7252026,51.5114864,34.65123,34.65123,34.65123,34.65123,34.8806427,37.751853,35.6455113,39.7036194,35.5383573,35.644758,35.644758,39.7158682,-22.2710727,14.9030362,-4.0552402,13.7298956,22.6473733,52.0896802,4.2463281,34.7323338,34.7254497,34.7256185,33.5608692,33.5495088,33.5495088,5.423392,36.4689824,37.4138,37.4562557,37.5777608,40.871459,35.1795543,35.1595454,37.5940049,37.2635727,36.9910113,37.284761,37.598422,36.9930661,37.8071169,37.598422,37.3684733,36.377428,37.590799,37.8228,34.0206037,8.0264776,22.9867569,36.3504119,56.0485005,12.5146121,-0.8761629,3.8436799,32.8031004,33.836081,32.8140382,31.4360149,12.572091,25.023088,35.946032,-10.1520811,33.0249301,5.3116916,29.3497017,29.3393526,35.693517,35.010339,35.0262444,34.9911169,35.0123776,35.0262444,35.0262444,35.0262444,35.0262444,42.980398,37.5961951,35.8900521,35.8900521,33.8941523,32.8115167,33.6266584,32.8140382,10.7368699,-34.9128319,-37.7206671,51.0537649,51.0537649,50.8798438,30.2240897,43.610769,42.6886591,45.6495264,51.0121717,48.7088345,43.6841263,-22.2710727,6.4674816,14.655771,35.074246,15.870032,7.6423396,-3.297065,-5.3642974,-36.883379,41.9027835,9.8499911,43.0137835,53.0846189,37.7769044,16.9496263,49.7830083,53.10802,53.10802,53.10802,52.5200066,52.50597,52.4486,52.4486,54.3271991,54.3271991,50.9089375,50.9089375,52.50597,52.50597,52.1571485,10.316988,54.9524134,54.9524134,29.3510842,38.918088,6.428055,40.5640188,5.995982,47.658236,-34.7263443,-6.2331448,58.3978364,56.6659156,-3.6379066,-6.5988243,-6.4924457,-6.228835,53.4035223,18.0175377,51.5209007,51.5209007,51.5255215,51.5209007,47.546095,35.8755172,34.0522265,34.0522265,34.7996028,29.2389519,30.4132579,30.4514677,29.2543567,13.0633873,41.998997,-6.3487617,37.517142,55.7119483,48.624677,37.09024,-7.957302,-33.7738237,7.142572,13.085483,40.5437073,9.9403922,20.0449472,20.0449472,20.0449472,18.8983027,18.8983027,18.8983027,16.1991156,13.8422299,13.8422299,16.1991156,16.1981544,16.1991156,16.1991156,16.1991156,16.1927946,13.7945775,13.7945775,13.7927155,14.1295009,13.7945775,13.7927155,13.7945775,13.7927155,15.5798143,13.7945775,13.7945775,13.7945775,13.7927155,13.7927155,13.7945775,13.7661693,13.7945775,3.1178789,3.1178789,-7.150975,35.6891975,43.8438344,43.8438344,44.3868955,20.0656229,20.0656229,20.0656229,1.352083,34.2642189,2.9905083,2.9041161,2.9673892,3.143717,3.143717,2.9727787,2.9071234,5.3536019,3.1598299,2.9673892,55.6087954,-36.883379,53.471068,12.8170387,24.7522876,3.202778,50.7487635,14.562432,2.9792879,4.7632756,3.143717,3.073281,18.0585437,-4.276407,13.5475216,-38.2677746,50.3643942,50.3643942,41.5256351,7.8034576,39.7036194,35.6894875,51.9286443,34.6937378,35.8998815,54.3696799,5.3116916,13.7563309,-6.8933478,-6.9174639,53.248005,26.501301,39.327962,4.1708346,5.9854617,5.9854617,64.1489253,32.7488637,29.5320522,57.1377715,7.1897659,1.27371,38.9853814,49.1986299,24.4330231,24.4330231,43.3801311,42.3631763,-40.3867649,-8.5869073,10.301978,64.130359,64.130359,-20.2657903,-20.2362646,50.1742631,52.414899,48.5371893,54.1601708,51.55143,52.52514,51.49558,51.4896462,47.7654056,36.7332355,45.5047847,45.4077736,43.260879,30.1783657,38.994784,38.994784,16.8660694,32.7844632,32.7844632,35.631698,35.7797886,35.6894875,35.61281,35.1354291,-27.5948698,26.624269,12.4896877,40.764101,47.5737975,47.5737975,49.2098917,28.12327,45.764043,30.5655764,1.28961,52.3555177,34.9771201,5.7348119,2.9041161,42.701848,52.2109803,36.5671162,41.8280186,34.9651567,34.74263,45.4786455,39.8430961,8.2410931,8.2414499,7.0147061,8.4315749,8.4315749,8.4286368,5.038635,6.067245,6.067245,8.2414499,8.4286368,5.038635,8.2254698,3.1704563,14.6536185,25.087255,24.0517963,9.1738727,1.5772459,13.7595319,13.7595319,13.7595319,23.5985695,4.1783118,1.5868818,17.9708394,21.0272206,21.121444,21.0225665,36.0326679,11.5434084,36.067108,35.3939908,27.4866699,30.592849,34.272117,13.7595319,9.9981413,13.8824645,13.7563309,13.7656274,7.9506299,2.942842,23.600387,23.600387,23.780806,-6.2331448,25.3271737,11.5549388,25.3271737,23.7595315,-9.4456381,-8.5187511,-6.2331448,11.575678,2.9425517,24.2051282,-18.8791902,21.0287684,-9.4456381,-6.2303803,27.695238,-25.9691072,12.375832,11.5759538,1.2806572,17.9568646,17.9568646,11.5563738,16.8751927,-5.4389569,-6.1793419,-6.1793419,-6.1793419,13.782611,-9.4456381,1.279217,17.9568646,13.853587,17.9568646,-6.1849624,3.139003,14.0208391,9.1341949,9.1382389,3.8480325,13.7641306,15.870032,2.9356719,2.9356719,-1.8565746,33.4551742,33.4551742,42.360091,35.6894875,43.6295782,9.081999,43.6295782,43.6295782,38.2688373,36.204824,31.9076736,31.8309938,23.7329926,48.8442935,35.6894875,34.9104194,36.3260807,49.2827291,3.0648608,3.0641912,3.0648608,3.0648608,3.0648608,3.0648608,3.0648608,3.0648608,3.0648608,-37.9105238,47.8863988,-23.609932,45.6667557,40.8644792,36.802151,43.6188823,35.4475073,55.7039349,27.332846,35.3939908,3.0986443,13.4732985,12.9971694,11.385274,1.303045,8.2410931,8.2414499,45.8983184,42.2532535,37.1641139,-0.4684258,-0.4684258,14.1114189,22.4076256,-32.0688823,-32.0692218,49.6098501,50.830909,50.830909,27.8623191,-22.90576,-12.4376339,-12.4368699,-12.4376339,-12.4376339,39.9041999,46.198895,46.164809,48.205212,34.9590972,35.7163059,47.8685056,34.0169567,52.1648206,48.8442935,-41.2978122,-19.2574666,-37.803273,-37.7385822,-6.6036134,-37.803273,19.1174746,3.1541322,-3.4260905,16.8750541,35.8066704,35.781864,35.7846633,35.7846633,58.5855084,53.5184925,49.28595,13.843262,35.6948901,16.3792311,35.1814464,32.7502856,32.814234,35.138965,35.153491,31.2303904,17.6339299,17.2881183,14.9798997,14.984877,13.959391,50.466158,28.6834074,28.683529,32.060255,31.491169,32.043846,32.054943,32.077692,32.0568391,32.20541,32.006493,1.3801179,1.3483099,1.3483099,35.1479949,6.1183964,55.9331214,34.7321121,34.687905,16.748275,19.0284952,16.748275,14.0208391,14.5758259,37.0197314,10.8505159,52.1648206,12.9247966,35.1365143,46.9421412,55.6871876,50.1094508,34.0169567,48.205212,50.1175395,51.496715,51.496715,34.0169567,48.205212,57.149717,-1.3361154,51.3972589,1.3136246,1.3136246,37.2635727,14.4905913,53.92054,36.0281236,17.9757058,18.0461022,32.8031004,13.7563309,14.0783982,6.9751667,37.7912876,26.7861183,10.8505159,26.8637897,28.6300471,13.767423,24.9679966,22.9988084,24.789067,23.4687991,24.123552,24.123552,24.789067,24.7135517,37.2635727,39.9783179,35.86166,14.0783982,14.0783982,14.0783982,14.0783982,14.0783982,14.0783982,14.0783982,14.0783982,14.0783982,14.0783982,14.0783982,-35.0288147,21.0277644,12.2585098,37.99083,39.989184,21.0277644,11.5947388,20.8449115,16.993837,23.8976625,23.8976625,23.8976625,1.310471,50.7155591,55.64191,56.176362,5.4163459,5.4163459,5.4163459,5.4163459,37.4562557,34.8361679,35.1330251,40.7978787,14.637115,36.0190178,35.194634,14.637115,34.8361679,34.1859563,15.7298859,13.4591181,30.592849,16.7875423,21.0277644,13.7563309,36.0614634,43.617897,37.2635727,36.0306202,36.0306202,36.0304027,34.9494383,45.528319,22.5658475,22.5658475,7.1785253,7.1785253,-26.2041028,37.2635727,13.7687415,37.5688063,36.0500792,35.1911906,35.7481602,-6.17511,35.8835576,33.6650639,13.0108387,21.013299,23.1135925,35.7043002,21.0277644,36.0500792,21.0277644,-5.0656001,1.3217228,60.3912628,21.0135591,12.9454885,15.4589178,32.8257581,12.2074691,31.2303904,30.6834446,35.7122783,44.446957,21.0029944,51.4164431,-43.53173,-41.3018457,-36.8440895,-37.7894152,-41.3018457,-41.2768239,22.7544845,22.7246559,22.7246559,22.7868514,11.5947388,32.8704316,41.5253646,30.3664995,37.7365642,-9.4257088,51.4868364,22.0502043,22.0460173,22.0460173,25.1408887,25.1408887,35.7163059,35.7163059,35.7163059,52.1648206,52.1648206,38.8912793,52.1583,38.8912662,24.156744,14.5869371,51.4855766,52.1648206,-6.2356684,-6.2356684,23.574398,23.574398,22.6642309,1.3157045,13.849811,6.1183964,6.1183964,6.1183964,38.914003,1.2966426,45.5598415,30.7378118,30.0358447,30.0358447,31.5965535,35.3460413,34.334253,33.2395578,34.334253,35.6334828,25.1197728,34.991132,34.991132,34.2747293,35.7493331,35.3460413,13.8476452,43.2203266,18.110541,14.0786242,13.732717,14.0786242,14.0786242,14.0786242,13.732717,14.076829,14.0831346,14.0786242,14.076829,14.076829,14.076829,14.0786242,14.076829,9.1341949,35.7163059,14.0490351,35.7163059,14.0786242,14.076829,-4.5585849,6.1183964,1.3197461,-6.184888,22.6264249,14.9798997,25.023387,22.736571,25.0260598,25.1508552,25.1508552,25.1508552,25.0173405,25.0173405,18.0564881,24.7947253,24.7947253,51.8977847,21.037649,1.2922814,10.8774839,51.8921099,51.8921099,18.0373164,2.929997,3.1681251,2.9232561,3.1681251,2.9232561,1.2937278,31.26765,1.2966426,1.2933333,1.2966426,59.8229392,7.873054,25.123002,25.123002,-25.7360046,52.1648206,-8.7049816,-9.4456381,38.8206003,13.3611431,44.3646982,18.291098,26.7861183,39.435561,39.43668,25.6120984,26.2006043,26.2006043,13.4540922,52.3702157,9.3118334,10.6749005,10.6749005,-34.0081858,55.8612013,55.754382,52.1648206,52.1648206,51.987523,53.0027981,13.843262,13.7563309,37.4529598,42.7217722,10.066198,39.944052,40.7423345,39.945942,32.2787745,-32.7185853,-32.6553869,-33.8678955,40.8093746,41.0852969,40.8396716,1.2947638,54.9791871,14.1406629,-31.2532183,14.637115,37.4562557,35.1795543,14.637115,12.2681437,12.2681437,12.2681437,12.2681437,21.0469527,12.2521546,21.0277644,39.0628463,39.002793,36.0500792,-31.2532183,13.7563309,38.9951559,37.566535,21.013299,21.0277644,35.6911142,35.6911142,37.9161924,20.9922584,29.803449,38.5365101,30.0444196,37.9161924,35.7215163,33.2395578,35.6894875,35.701774,35.701774,35.701774,35.3324588,20.854996,-36.8440895,-41.3018457,-37.7909382,35.1851895,-27.9613362,46.5596032,-19.4914108,-19.4914108,34.7187161,32.8704316,32.8704316,25.7346912,42.1398577,36.6041223,36.6316744,36.9517572,47.6870035,38.892949,22.7928863,22.8238607,60.472024,59.665732,10.8719808,64.4672328,49.188971,46.8962434,-26.690468,42.3398067,11.4584903,32.049147,21.4517131,21.268443,20.8449115,34.262298,34.262298,34.262298,31.2056754,47.8528291,59.932369,63.414076,60.3912628,59.9667545,59.6662655,59.6662655,63.419499,63.419499,63.419499,59.9138688,38.7228378,26.0907066,37.8363654,10.7711697,34.8804104,9.7150631,44.6456943,1.3230188,13.7636593,14.076829,14.0807813,14.0807813,-33.3560503,16.3873557,51.7527214,1.2966426,1.2966426,-10.1544464,-0.789275,37.3942527,34.262298,34.262298,40.7295134,41.293483,42.8759981,43.6963085,43.6963085,42.6791832,42.6791832,33.6845673,35.0793761,36.061255,36.061255,6.9753413,-29.869576,-25.274398,-35.878375,55.3689827,13.8547212,-9.4723471,-6.314993,-33.8782571,16.0341215,12.2387911,17.9519877,33.3764196,27.0943662,40.0141905,40.775025,40.2964786,33.2395578,33.177372,34.6873868,26.465329,26.465329,26.465329,26.2124013,26.2156717,26.465329,36.1270236,36.8856104,41.2251159,24.2398596,40.155255,45.3384948,22.3402106,53.1632001,43.1308845,45.5139805,45.4989098,44.5637806,3.8725987,35.6050574,7.690466,-6.17511,43.6164874,34.5905808,34.7306318,34.6937378,34.6937378,34.5476897,34.8220139,53.9466536,35.6894875,51.755011,-6.1254164,40.7111197,49.2105624,16.301669,16.301669,43.2239726,14.5995124,-6.9261321,-6.9261321,-6.9261321,-6.9261321,-6.5998987,35.4475073,49.5954268,-2.2120723,7.3387808,9.9672163,14.5868095,30.7579647,30.7579647,41.119997,21.3659896,-5.2218841,47.6776384,52.17725,11.5804444,17.9626187,47.6183248,-33.8027455,-3.6543735,-23.5652103,18.7944844,52.084881,24.798775,23.0356075,39.986913,-32.042273,40.7982133,53.181242,6.4449129,21.4735329,28.9233837,28.9084384,2.9909437,-6.3003791,12.9649215,15.870032,10.7572075,40.6567664,40.6567664,59.9240802,64.5041886,10.777683,10.777683,19.0284952,16.4515999,12.9649215,39.948075,14.1749629,14.1716237,11.0049836,14.6612079,14.6605469,15.672513,14.168689,15.671205,15.671205,50.8106859,15.578375,14.3482586,13.7691994,13.7691994,7.897664,7.8034576,7.8034576,7.9096958,7.9096958,21.268443,41.9027835,16.8285909,22.6558442,-20.9175738,34.746611,21.0721358,-25.680809,-33.954662,50.3660786,52.7007132,49.9000179,52.2214125,45.4786455,43.6171442,-4.805035,-33.4411514,-30.0573733,52.3555177,35.0906222,-32.7684603,28.7620739,50.7951866,50.7951866,50.7951866,13.7563309,13.7661693,34.3965603,32.8140382,1.4018544,1.352083,1.352083,7.0062812,7.0062812,7.006915,6.8774965,7.8947617,9.0941937,7.0062812,6.8774965,7.5184906,6.8774965,7.8947617,23.69781,7.0062812,7.0058848,9.0941937,7.0062812,7.0062812,9.0941937,7.0062812,7.0062812,6.4556868,40.3439888,45.329621,28.589873,40.0613124,10.323388,26.2124013,8.4402127,-6.1216097,-4.3968094,-4.087207,14.1847394,14.3532128,22.339169,-5.246892,35.134032,35.134032,35.134032,30.3588884,40.4237054,35.2338855,35.2338855,-6.1254164,-6.1254164,-6.1254164,2.9425517,3.053361,-6.4982342,25.376743,25.376743,-27.0590026,-27.449561,36.5981032,21.979933,18.308063,51.5240671,54.5844087,44.2252795,-27.4863585,-27.498082,-24.864824,-20.7291664,-27.468712,-27.4686858,-20.9175738,-20.9175738,-27.5589156,-27.970828,-27.499063,-19.2574666,-27.4728937,-27.4728937,-23.826444,-27.477357,28.970079,-3.6446698,14.882905,51.8193148,51.8193148,4.89263,13.8188209,14.882905,16.1991156,13.7758611,7.201236,16.4282615,14.9875656,18.80968,18.3690793,7.201236,8.3496325,7.201236,7.201236,7.5296842,7.201236,13.2281338,13.7137329,13.413027,14.987727,7.201236,7.201236,7.5296842,18.8097157,13.0837496,8.5841615,24.3683508,13.766015,12.6646965,13.7644702,13.7562122,13.9668672,13.9668672,13.9668672,13.9668672,9.9528702,9.9364634,9.8753133,0.5386586,25.885066,25.885066,55.7107897,41.717677,12.6813957,34.3880178,37.2635727,16.8660694,-13.0024602,-8.6846885,43.6158299,25.0814367,5.9788398,26.820553,15.9128998,8.4324831,16.8444743,7.1897659,-8.6862895,-6.2614927,-6.2288362,-6.17511,-6.4024844,-6.5971469,-6.2614927,-6.4982342,-6.1383768,-6.17511,-6.1383768,-6.1254164,-6.1254164,51.8094154,21.1023498,10.7862408,10.7856405,21.1023498,10.7862408,21.1023498,21.1023498,21.1023498,21.1023498,20.8449115,21.0043035,10.7856405,10.7856405,12.2521546,12.2766354,21.1023498,-6.0025343,-5.0549145,-1.8479,36.4420296,-6.5971469,-7.090911,-6.5971469,-6.17511,51.9608721,35.0725448,-5.1476651,20.8449115,-6.1383768,14.409846,14.409846,14.409846,-6.5571047,-7.090911,-6.3487617,6.5211111,-6.4024844,5.745247,50.7800778,-30.0002315,41.8103694,-33.3135911,10.508928,21.121444,-5.0549145,-6.5971469,-6.2121345,30.1785555,35.7790283,-6.1892847,35.0325561,50.830909,-37.8079933,13.7563309,40.7625706,44.4470358,55.653207,55.653207,17.295028,51.927354,13.7171916,12.7078152,55.9652527,-33.8665582,-33.8641859,51.4787438,55.680234,55.680234,55.680234,13.8489843,13.8489843,51.425673,59.3498065,13.780057,4.6020525,-37.7990618,51.4888511,53.0547626,53.0547626,43.6677097,43.6677097,13.8489843,59.366468,45.4015784,55.6802303,55.679577,55.8745653,53.1206355,3.1638642,10.7862408,48.6553397,51.4456659,37.2770477,55.7107897,55.7107897,59.9438602,60.0021133,43.1141348,12.2374439,40.5008186,40.741187,40.5280759,39.4251852,40.5008186,35.1569452,34.991116,26.501301,-34.920693,-34.969355,32.751731,23.1568182,23.1568182,23.3790333,26.5637114,5.958493,6.2073591,6.7145912,45.5303465,33.2420542,10.9785423,39.4385648,39.390963,10.7599171,35.8654477,32.8873915,32.8873915,32.8873915,13.719119,47.7883758,1.4554793,1.4554793,1.4554793,13.4983477,13.5497754,13.4983477,10.3540762,32.7321106,32.7757217,37.7844713,14.6137864,15.3485208,36.6999603,37.602638,37.602638,34.41045,-23.6026682,41.9037626,43.055198,1.5558542,1.5891309,-7.3247611,26.3206308,3.0738379,-33.2277918,57.16476,13.6511842,9.9644323,-38.3901566,54.9791871,54.9791871,53.2265132,16.8220774,9.1341949,41.9632822,56.45131,56.45131,-19.8730077,34.1039348,32.8681401,32.8681401,32.8943096,55.3689827,10.672247,10.672247,10.6706389,10.6706389,14.660831,10.672247,10.672247,14.4126499,5.3116916,10.672247,30.5173159,-33.8700539,-20.9175738,37.863936,-27.9572417,10.7954612,14.4126499,10.6706389,14.660831,10.6706389,-6.6355092,26.1364992,-7.55963,31.2303904,1.492659,1.4853682,25.1721091,-22.3015408,5.9788398,10.335586,32.8123524,26.501301,26.501301,-8.6647896,50.1109221,50.1175395,51.1534754,50.1175395,37.459882,37.5777744,37.459882,37.459882,37.459882,37.5796775,43.2150187,-7.2823728,34.341574,35.7990628,35.8001144,35.8001144,37.513068,31.2660883,31.241044,31.2303904,31.201001,30.883691,31.201001,23.354091,36.082587,27.199945,35.8861237,55.647883,10.695572,22.543096,1.2895005,35.4864939,33.9545922,33.956639,36.2500574,29.6299262,12.7234456,34.975562,34.965406,9.4259684,9.4259684,9.4259684,14.0783982,30.572815,30.6730561,33.5903547,27.3172691,9.3307293,9.3132466,9.3132466,13.7529161,13.8188978,12.6499387,12.6499387,49.2780937,-31.9330833,-31.9539703,25.4264844,1.3138397,1.345034,1.2810159,1.3406833,1.298575,1.298575,1.3082507,1.3412873,1.4043485,13.8188978,31.9883198,45.316614,30.2049389,48.1548146,59.814947,42.3180957,38.8859942,38.8859942,38.8887861,8.9614983,38.8859942,8.9624308,25.7616798,8.9160067,8.9624308,2.95642,48.147518,-28.81799,-28.81799,10.8230989,37.7090577,38.793103,14.6551691,23.182586,36.067108,24.479833,48.856614,44.4197361,44.4197361,44.4197361,44.4197361,-20.963617,-7.024374,26.5637114,21.0748709,35.6894203,35.261095,-6.8522278,7.1713712,7.1897659,7.0062812,31.303257,25.0952273,36.769687,48.8484556,-33.310684,-33.310684,-34.920693,23.1568182,26.1920826,10.6706389,10.6706389,10.672247,14.4126499,-28.81799,47.2247678,47.2247678,10.3920483,10.3920483,7.2035139,10.8230989,7.0983727,14.058324,29.663574,43.2203266,12.9239176,13.7400469,13.745429,14.1064651,13.7400469,13.854492,-2.984833,-2.984833,12.825759,12.8231047,1.352083,55.8584493,20.6798761,51.5772884,12.0004108,12.0004108,38.6221129,37.8413472,14.6144514,40.5006889,8.7180549,8.7180549,8.7180549,48.8005564,49.5896744,48.8051133,21.0277644,36.6202607,37.4274745,54.9884804,39.1933213,16.9400273,-7.962774,30.274084,36.067108,24.479833,9.736255,-7.9465289,51.0525306,-5.1866992,-7.962774,-0.834368,-4.185235,-6.1657658,43.4141267,54.9884804,54.981535,42.5830342,40.8326656,59.357156,59.3621737,52.0868908,48.8051133,13.7758611,13.7766239,13.7766239,-33.0435992,30.620533,30.3636948,3.685048,23.5909657,23.5909657,23.5909657,23.5909657,23.5909657,5.395676,-9.3897722,23.0965384,23.0965384,22.270978,23.0965384,37.588227,37.588227,37.2930009,3.0662171,40.6552459,40.9123761,43.0402889,14.8817715,14.8817715,14.8817715,14.8817715,9.1341949,9.760829,14.882905,14.861765,14.861765,21.3069444,21.0277644,10.672247,32.8704316,30.6487378,29.888411,14.8817715,51.62406,59.334679,59.814947,1.5322626,-37.8221504,47.4037666,47.376313,47.556104,47.556104,47.556104,-33.8749641,-33.8382071,5.5673238,5.5709872,24.9092354,47.5930253,43.6206266,25.1914523,25.025354,23.829545,25.1276033,22.644444,22.6272784,22.632526,24.8138287,37.8612907,22.726916,22.726916,38.001167,32.8025783,38.914003,32.8031004,5.97044,11.0123261,8.7771917,13.085483,25.1753389,1.3215188,-0.059959,30.7924391,-6.7641702,-6.7792776,14.4410093,36.5789462,36.5789462,35.7232423,-32.2785846,-6.1694025,12.6506639,3.0643194,30.079579,44.1677237,13.8382489,52.2714048,55.7855742,55.7855742,55.7855742,55.7855742,51.0292734,51.0292734,48.14966,48.14966,-6.89148,3.0696089,4.371315,32.1133141,32.0458024,32.825254,1.2944389,35.6894875,1.391288,32.789522,30.618531,27.7127448,29.315514,32.709483,33.5842591,30.6137981,13.819025,21.5875322,13.7563309,14.068891,14.0761416,13.83561,7.8081585,7.8081585,7.8081585,7.1620803,13.7568378,14.073862,14.073862,14.073862,14.073862,14.073862,41.6089711,14.7997157,24.479833,13.0295139,13.7563309,54.1111238,39.1821223,39.3433574,39.3433574,39.083961,30.1533605,14.044149,-7.9183528,13.5965667,35.5710322,38.2492499,38.253834,40.822072,38.284621,40.784068,38.253834,34.9901156,35.6644464,34.0702703,35.6314761,35.6051229,35.7012897,35.693451,35.6638938,35.617166,35.6406561,35.6406561,35.6406561,35.6268386,35.6268386,35.6268386,35.6263035,31.2796856,35.5152072,34.3852029,36.6959518,34.7017563,3.1168553,7.4977066,44.3568962,53.3483383,14.6214655,29.4618444,-6.1678314,6.5243793,32.7689827,40.1865632,39.9996674,39.9996674,50.9181454,42.4074843,42.2463361,42.2467034,29.8848438,4.8715804,16.8660694,4.8715804,5.3116916,57.158569,24.200588,16.4637117,12.9141417,-34.5998875,15.2470367,15.2470367,15.117961,15.117961,15.3323436,51.5245592,50.6696875,34.0680524,38.9940439,3.079673,-8.7982727,-8.7982727,17.3974875,-21.7769499,-1.4743965,47.8419633,-22.7684765,40.4388584,56.402176,39.641222,3.200159,18.0079722,3.159497,50.8132068,54.311633,6.3702928,43.6451206,12.8858107,63.57273,63.8205484,3.5437018,43.610769,43.610769,5.4075278,5.4075278,27.717201,-9.8627946,-15.7631573,1.255179,52.008996,39.9041999,-22.390734,-22.3489822,-22.8911069,10.8987326,-6.2151397,39.0018561,-4.269928,3.0985051,56.1681384,56.1681384,56.1681384,13.5026064,57.16476,57.16476,57.16476,57.16476,57.16476,6.4157451,37.7460756,8.8912598,-34.920603,-34.920603,10.8719808,10.8728195,31.4302863,13.0766681,12.8581843,12.9141417,13.0766681,14.6060784,7.7660319,10.7003741,-7.2721514,43.2314739,41.076655,33.4358723,33.4947719,64.8557566,64.8557566,53.5232189,40.4824722,31.2105164,37.0439713,38.190742,38.3852246,52.3558182,52.3558182,43.6171442,-0.914559,-0.914559,-0.914559,12.8581843,4.9326453,6.2669533,-23.6691073,51.218701,47.4814072,33.6492326,32.2318851,36.0678324,-36.8523378,-36.2692321,-39.8061639,21.8874193,41.386608,40.5466983,25.726406,20.5915185,20.96965,40.6311763,30.1656721,12.615191,-2.0705855,53.2295205,41.386608,47.5602535,40.6470849,43.3309433,51.3781162,49.9288196,30.5701223,44.8184339,-3.7599985,6.177375,60.3878586,46.9504896,52.0385519,32.8442838,52.4868584,45.7618799,44.4949323,44.4962318,50.7267715,47.2405045,47.312907,-15.7631573,-7.952194,-7.952194,-7.952194,53.106758,48.3586887,47.7429124,47.6450984,50.8670895,51.4584172,49.2606052,49.2606052,49.2606052,4.9773576,4.972665,4.972665,4.972665,4.972665,4.9773576,4.9773576,4.972665,4.972665,4.972665,4.972665,44.435458,-34.5998875,23.2393587,45.434532,36.5339048,36.529024,49.188971,49.188971,4.9738033,22.5749683,22.5749683,51.0786767,37.8718992,38.3185471,38.5382322,38.5382322,38.5321544,33.6404952,34.068921,33.9772035,32.8800604,32.8800604,34.4139629,36.9915847,37.3641651,52.2042666,10.0309641,10.0309641,-35.2776999,-43.5235375,5.1155334,-33.957652,-33.957652,-29.9657327,-36.8271925,28.6024274,-23.319648,14.6903672,41.7886079,-33.4445204,40.408141,22.3388589,33.228627,22.3886047,40.2089072,19.248962,51.8921099,53.22739,5.4075278,5.4075278,5.423392,5.423392,59.5684418,51.62406,50.9281625,6.9021771,40.4478246,40.4478246,-36.8294765,41.8077414,55.6802303,55.6802303,55.6802303,55.6802303,55.679577,55.6802303,55.6802303,55.6802303,55.988692,9.9369951,54.890876,6.3549534,-6.2247343,5.4075278,5.4075278,38.7856004,28.5449756,39.6729875,52.9384633,23.7339937,-7.0511282,4.6281349,42.4952453,56.4582447,56.4582447,-29.8674219,54.7649859,51.1888113,52.6235171,12.5099349,55.9445158,51.4277172,51.8777259,-22.9541412,-22.8184393,-3.7417415,-12.2003423,-12.2003423,-21.7618295,-22.3489822,-23.559985,-14.8144384,-15.2550535,-1.4689575,48.1159299,47.6450984,38.5731454,38.5731454,50.1712678,50.7371369,-13.0024602,-22.9030265,-2.5571836,-19.8690878,-25.4269081,-8.0517067,-8.1164625,-22.9541412,-22.7684765,-5.8393707,-30.0767792,-32.0311548,-8.0175094,-22.7491243,-27.6005949,-21.9841446,-21.9841446,-21.9841446,-10.9259972,-10.1771507,-19.7549285,-20.7548697,36.3100425,44.8334299,41.054643,12.2681437,43.7776426,26.1859607,29.6436325,26.336502,28.102457,27.711005,27.711005,41.449855,50.0949387,50.1270675,-29.1112771,47.9935441,46.806357,33.8533725,-7.7713847,46.199444,44.4149029,33.9480053,14.3941935,51.0179516,51.0465619,51.0465619,50.58052,41.9632822,55.8721211,12.589413,57.6981719,51.5408116,37.1865192,51.4502282,45.1892589,53.2192634,53.2200773,13.4322112,43.5320995,32.7614296,51.4861319,51.4874578,53.5665641,-7.2933372,20.996017,-5.1337227,21.296939,21.296939,21.296939,19.701015,21.296939,21.4434635,21.3800638,21.3102505,49.4191402,60.1866693,60.1726348,48.6838619,57.4749146,57.4749146,17.9622406,48.7109551,22.3386451,22.419094,22.2829989,27.2677141,27.2677141,41.506167,49.5836776,53.7456709,17.4567372,64.1396203,39.641222,40.1019523,41.8693039,40.1019523,3.4183678,2.929997,3.4183678,-6.3627638,-6.3627638,-6.3627638,47.2633542,45.814548,39.6158593,41.6627694,-6.3627638,21.526226,50.9295513,-7.4042404,-7.4042404,-26.1827521,-26.1839145,29.584332,4.606567,38.9543439,24.9418196,49.0119199,34.1279625,-7.024374,3.1681251,2.926543,2.929997,2.929997,2.929997,6.0313214,3.0991558,2.929997,2.929997,2.929997,2.929997,2.929997,2.929997,3.0991558,2.929997,8.5032375,15.5656153,15.6093653,54.3325316,49.204182,47.689426,-7.3247611,-1.7621963,2.3139052,-29.6196074,-29.6196074,-29.8674219,-29.6196074,-22.2604759,-20.902714,-20.902714,46.1476461,6.5193063,54.0103942,-0.1629336,56.9508098,-41.4272187,-41.4272187,46.5210895,46.780654,49.4964477,53.8069972,52.6211393,51.3385738,50.8779545,50.8132068,50.8132068,50.8132068,50.5629131,50.5830803,50.5830803,52.672582,38.733682,38.7526578,53.2909599,53.4086471,53.4035223,53.405936,53.405936,46.0491938,51.4987997,51.5174861,51.517615,51.5240671,51.425673,51.498371,51.2421839,4.6027603,30.2114404,38.2122761,51.9105385,45.7788727,13.0660293,13.0660293,48.01562,44.9012197,44.3370555,36.7199506,34.6692287,3.1201114,3.1187573,6.1642909,3.1201114,3.1201114,6.1642909,3.5437018,3.5437018,6.462173,6.462173,6.0313214,6.0333166,6.0333166,6.0313214,6.0313214,1.464945,5.4075278,5.4075278,5.4075278,5.4075278,5.395676,5.4075278,1.464945,5.4075278,53.4668498,15.8899448,38.2102166,39.2892131,38.9922646,38.9869183,38.3191408,41.6292932,41.6292932,42.3911569,-20.2338121,40.7400842,40.5255221,10.7549448,53.59109,54.3295878,-37.793693,38.1891564,25.7191685,25.879072,42.2780436,38.2766838,45.4601435,45.518383,41.5607319,44.97399,34.3659395,34.3664951,38.9403808,-23.5168343,38.2132975,50.4587714,42.4430124,43.6156599,43.6156599,43.6156599,43.6316206,16.1991156,51.9635705,-7.9623039,48.14966,51.9635705,37.9879046,52.2033023,35.9049122,35.892229,47.922891,3.5586825,3.5586825,33.2075909,19.3188895,19.3228313,18.5230649,19.2869652,45.4251774,19.3387706,19.3148211,27.4947768,9.9986861,-23.1342389,-1.2803586,50.466158,50.4645992,47.2095499,40.845492,40.845492,40.845492,40.830096,48.2365448,48.2365448,10.7624165,-29.8674219,42.797263,40.8201966,0.556408,46.9938549,36.1085197,45.305862,-22.2631296,-30.4899535,43.138948,35.0843187,-33.917347,-33.917347,54.9791871,-32.8927718,54.9791871,12.2681437,12.2675263,43.7169594,51.8193148,34.2239869,47.922891,3.5586825,-32.056084,41.7001908,-32.056084,2.9447579,52.831334,52.938636,52.938636,2.9427466,2.9427466,-22.2631296,38.6959379,45.2472885,-10.1544464,33.8779038,35.5822671,35.5822671,-29.1112771,59.9399586,49.8336826,-45.8646835,45.4231064,65.0593177,43.3617187,51.7520209,-1.3032909,51.7548164,51.7548164,45.3450045,45.406766,-2.2120723,-2.2120723,38.11779,9.3351014,48.8293187,48.757887,42.480214,48.6976847,48.8469373,43.6963085,2.991686,-3.6543735,-3.6543735,45.1867156,1.6235162,-7.3334706,3.685048,39.9522188,7.2565929,-8.0440603,42.6820428,42.6820428,5.4075278,2.991686,2.991686,2.991686,43.1163223,33.9978536,19.0284952,16.4054979,14.1648491,14.6551691,14.1648491,14.1648491,14.5758259,7.0857312,10.64204,10.7092509,10.7396212,10.64204,10.64204,16.4054979,16.4054979,14.6460651,14.6551691,14.6551691,10.64204,14.1648491,14.1648491,14.5758259,10.64204,10.64204,14.6551691,14.6551691,11.2482288,10.64204,14.6551691,45.3275748,43.7167235,40.4443533,50.3759061,46.5859908,43.6171442,43.6171442,39.4793254,41.109048,-17.576387,11.1649219,41.3789689,4.795761,41.1465479,50.7951866,52.3934598,-25.7545492,46.257492,18.2108895,18.2108895,31.4789841,3.2051443,3.2051443,2.9927628,2.9927628,3.2051443,2.9927628,2.991686,2.991686,3.2051443,2.991686,2.991686,2.991686,2.991686,2.991686,2.991686,2.991686,2.991686,2.991686,2.991686,5.4075278,2.9927628,2.9927628,2.9927628,2.991686,2.991686,34.6009864,48.4525603,46.3478707,48.4525603,-27.4954306,-27.4954306,51.4414205,49.0033869,47.6450984,-34.9025837,-20.902714,38.6487895,40.3351225,41.4860647,0.5103028,0.476395,0.476395,0.476395,41.9037626,54.0755448,41.1551366,41.1196609,5.9380921,26.2475821,30.6959406,-34.9201652,48.9773099,48.9773099,33.996112,27.763644,28.0587031,2.844204,2.844204,5.3559337,6.098095,6.0996155,5.3559337,5.1460189,5.3559337,5.3559337,5.3559337,5.3559337,5.3559337,40.9613376,1.4554793,1.4554793,10.3540762,10.3540762,41.1314071,3.390851,42.8885225,42.8885225,42.8885225,42.8796156,14.6096767,-22.7103575,-21.1622992,-23.5916604,52.1334003,50.609049,31.821994,50.609049,36.3851395,5.3559337,5.3559337,7.0864141,3.4183678,37.3807579,37.3807579,34.7265943,53.3809409,45.3790227,35.259904,34.965406,25.4427709,-1.3927152,34.0223519,55.3689827,55.3686303,7.0310811,8.4854828,30.3937917,48.9773099,48.9773099,50.9365797,-28.81799,55.3686303,30.3937917,56.3425909,-33.9328078,56.1459171,56.1492131,56.1304512,59.3621737,48.5790692,55.8628285,55.8628285,48.7816472,5.395676,5.395676,5.395676,5.395676,3.5586825,3.5586825,3.5586825,-26.7173619,-26.7173619,2.9385024,51.2421839,50.8660006,-33.888584,5.5709872,5.5709872,28.3944226,-0.8364322,-35.4041282,58.3810843,-42.9041217,-42.9041217,-33.8832376,-36.8532194,38.7368192,35.8540947,35.7058552,4.9773576,1.55849,1.55849,1.55849,3.0696089,3.0696089,4.3562052,6.4503419,3.0696089,3.0696089,3.0678542,4.35717,4.3857036,35.9544013,35.1406045,5.4075278,4.145627,32.7298718,30.2849185,27.8374221,29.5074654,26.3082047,29.5829699,32.7298718,30.2307338,29.315514,13.717978,39.3576754,34.6785286,34.070001,35.6277374,39.7158682,34.965406,35.7126775,43.6628917,43.784562,43.6628917,43.6628917,43.5943757,49.7457597,45.658667,69.679788,36.1088237,36.1088237,36.1088237,48.5294782,1.858626,36.829309,3.2151346,4.33939,4.33939,45.069428,60.4562974,60.4562974,42.4132174,52.2393971,-8.7982727,48.4222305,35.543932,55.0062362,59.814947,40.7649368,52.0901527,39.4793254,3.3759439,-22.9761353,19.1839498,52.355191,48.2544223,48.4631778,48.4634067,48.1986546,48.2131855,42.1694839,17.7181714,38.052408,51.5001344,30.5474687,13.1345645,10.641586,51.9845558,51.9845558,53.2295205,51.4713436,52.113607,53.2295205,51.5890596,51.5890596,52.3792525,48.5460236,47.6594423,43.4570369,47.680273,-31.981179,-31.981179,-31.981179,43.0095971,-34.068929,-33.888584,51.5182197,42.3043142,43.076592,43.078263,-26.190508,-34.4052137,49.7830083,35.6566427,16.8330365,3.8574086,3.8574086,53.9455334,-15.4181291,6.9099505,41.6420639,47.3976448,33.5903547,10.64204,14.1648491,48.8471036,48.8471036,13.5990961,59.8509005,10.64204,42.6820428,37.5287405,37.09024,38.7455467,39.1364977,41.493919,30.257543,30.257543,39.0352664,38.845703,41.015952,32.8904081,40.356539,41.3840395,31.3566281,31.0243824,61.1879121,40.5591345,29.7258496,39.7494916,21.2974026,39.351884,30.2250476,37.456387,36.9595102,37.4565356,41.5333485,32.7481208,30.0667646,38.8206003,46.7329641,38.8048355,-1.8479,26.0845477,27.4278216,39.4440025,40.7097557,33.9256282,41.3840395,40.7702671,39.0348317,28.068348,37.09024,-6.2591666,38.8730667,40.2998449,1.55849,-31.981179,-21.1542464,30.8489491,36.1447034,12.2387911,41.6867992,20.8449115,21.0469527,21.0469527,11.9088,12.2424933,12.2387911,-18.6533568,64.5472875,63.437679,39.199967,1.3731327,51.3535565,7.2731964,4.592395,37.217522,48.2544223,45.5391021,39.2319723,-41.2904017,-37.7239234,-37.7975879,13.0016016,48.1986546,17.9757058,21.0469527,10.8774839,21.0469527,21.0469527,21.0469527,10.7827158,21.0469527,21.0469527,21.0469527,21.0469527,21.0240532,21.0277644,21.0277644,20.9822597,21.0277644,21.0214823,20.9665439,21.0202242,21.0277644,21.0308517,10.8774839,10.8777684,10.8774839,21.037649,10.8774839,21.0277644,21.0363681,10.7771457,18.6795848,10.8230989,37.5495048,37.248772,37.2283843,10.744397,12.9717796,51.2190212,20.996017,10.7860295,8.7940558,64.8960485,52.3337568,52.3345709,50.821658,50.821658,50.821658,52.3337568,50.824239,54.5730622,39.9518653,26.5978372,53.4245365,39.6361396,32.2780595,52.4570113,51.9845558,51.9845558,51.9845558,51.9845558,-5.6504379,8.6415949,8.4303975,8.6415949,8.6415949,8.6415949,8.6415949,8.6415949,8.6415949,8.6415949,-37.7269114,39.0056247,39.0056247,-17.7057456,52.1621201,52.2212012,35.7087334,47.037292,46.7319225,38.6351441,21.0078319,20.9516658,55.02096,42.4652851,7.4648369,-41.4545196,27.925146,41.5566104,53.4245365,53.4245365,-31.9514362,12.6661944,12.6661944,12.7100116,9.4436553,10.0805,-34.069638,48.7342883,51.9635705,23.69781,-6.5704888,52.0214018,50.6090291,20.8569444,42.3629386,29.356803,-13.8506958,11.560212,14.581674,14.5593243,46.2327276,21.0277644,14.5817329,-18.1248086,17.9757058,-7.2702759,37.7917694,13.849811,13.849811,11.551921,17.5077664,-6.5832637,40.8502252,22.845641,-4.0434771,40.5742679,16.8351783,6.9270786,-25.274398,34.9771201,35.9703446,41.5242348,45.4506875,24.880095,24.4752847,14.1406629,24.880095,24.4752847,38.8990253,30.5443766,5.288564,5.288564,46.2327276,21.0277644,14.5817329,-6.2964704,-6.2964704,10.1925595,14.6620104,31.4853525,3.0767287,-0.789275,24.453884,-6.17511,13.7750508,10.0451618,21.036672,14.650296,38.9064872,26.820553,30.5443766,30.5117323,23.789362,5.288564,14.1406629,5.288564,11.5563738,12.879721,5.4356367,5.288564,39.7830753,30.5360485,11.5511705,0.4079681,50.1079793,11.5494037,-6.2964704,-6.2964704,-1.3361154,14.6620104,41.0727616,8.476641,24.4373484,24.4373484,24.43944,43.814132,34.6912688,41.3154435,41.3163244,38.2466122,34.1859563,34.1492316,33.5903547,32.394209,32.394209,29.5320522,34.4784912,-0.4731631,34.6919598,35.3444846,35.4746541,37.565784,37.565784,37.565784,13.7633196,39.9041999,24.4752847,24.880095,24.4752847,24.840109,33.1401715,31.0323677,26.2288917,6.9003004,6.9003004,53.0792962,21.270702,30.305626,30.305626,30.263944,30.034692,30.034692,29.1416432,30.219759,30.263944,30.263038,30.263944,34.746611,34.752941,22.517585,23.0965384,23.0965384,22.517585,53.10802,53.10802,52.5079196,-6.6036134,52.3669082,10.7135852,51.5352875,48.164891,22.5099602,13.0244963],[144.3896253,100.5763229,12.1022048,10.2076948,10.1990136,10.1990136,71.588159,72.0370018,-2.973917,-4.065692,-4.065692,22.2776003,50.9761625,71.588159,-82.3514973,-75.1712047,14.4478813,16.597831,14.4478813,14.4255376,14.5564365,116.38948,121.6145477,-63.5956238,80.5277117,34.774407,34.563063,-75.9869638,8.9266709,16.9154021,0.1218195,110.314513,27.8587248,-76.8088278,115.844908,121.0369801,88.9503161,4.518294,36.735067,99.5226618,106.8220367,1.968752,106.8650395,106.8650395,103.7877311,103.779601,-47.6578212,-1.310194,100.5682433,-121.998857,-75.7348819,19.3605431,50.9761625,119.7889248,103.7437506,103.7437506,28.3545344,31.2107799,116.1133488,51.39952,34.8172188,-89.44738,89.7924185,101.71629,14.5447261,103.7437506,103.7437506,103.7437506,-5.8487837,103.7437506,103.7437506,-79.8923404,-1.706322,2.3476575,6.233016,34.1235352,137.046995,136.8810211,137.0496143,136.9065647,100.4804061,100.4804061,31.281116,112.7599097,112.7583686,140.1338907,100.6144519,100.6144519,5.4314326,5.358066,139.6423446,127.0463559,127.2784676,127.2784676,140.0459028,140.0459028,140.1981405,140.1338198,8.5621158,8.5621158,31.168589,41.4829963,44.3784115,55.3004651,51.3925806,35.8925197,78.7946626,-134.3929999,-132.9589637,27.5721063,8.5805155,8.5805155,8.43802,78.0781901,88.3601604,100.5250276,-3.8099803,35.8925197,135.416766,135.4063757,10.2118019,10.2118019,12.0407312,12.0407312,12.231411,130.7078911,8.638335,139.743326,-73.9739882,-170.132217,35.4778503,31.499674,106.8650395,-3.1771279,104.623623,104.623623,37.5889677,37.5889677,37.5889677,105.8341598,85.3156409,100.4595468,100.4595468,100.4595468,98.380337,98.4024436,83.3241592,120.5948015,103.7809653,117.257372,118.380545,117.012485,117.227219,77.4776166,121.4433706,80.2363137,79.7653509,79.7132032,47.5518117,140.7473648,115.188916,-46.6716634,-45.453161,-46.6296128,122.3543397,5.3229345,80.251386,136.2091547,145.9706647,-4.4213988,99.3334198,100.5763229,-124.1636729,-94.1417261,-1.9812313,30.9697294,18.8365424,30.8958242,28.2497386,18.8729668,26.9404133,8.4461849,-77.19346,22.9592224,22.9592224,-111.9280527,100.5356298,121.2348665,100.5017651,-123.1059282,-76.9239801,-79.813234,-80.1398214,-75.7418973,-89.6145301,-89.5473719,-121.6095679,-90.9075061,-77.0300008,-93.6446612,3.7070107,-1.386919,142.3841412,142.3841412,127.6809317,-0.404359,77.6204003,77.6204003,100.4805818,100.6144519,100.6082862,100.6144519,100.6144519,100.6144519,100.6144519,100.6144519,120.31191,94.1956079,31.1777154,139.6423446,-157.7394444,100.6279005,103.7930398,103.7877311,166.4416459,121.0780915,121.0780915,121.0780915,6.1039168,6.1039168,-104.4969473,115.1672942,114.7388789,123.8857747,115.1672942,115.781146,115.2591503,115.781146,90.3965354,138.252924,-85.4951663,174.7934246,174.7673188,109.193759,121.6283098,152.978,142.7027956,151.2034248,147.054678,115.8159936,151.2131667,151.2131667,149.118527,16.4267426,149.0918,103.8806891,124.2310439,-107.3810207,139.390494,73.8727799,73.8905154,105.7215663,106.8222593,37.3979759,119.839644,119.5891247,106.7907129,-2.056585,115.2152505,106.8060388,-85.406986,12.0814274,124.1435427,100.5218798,100.5182713,82.9912582,107.6106591,107.6106591,90.4028312,90.4266584,90.283083,89.5403279,90.4202734,89.8065484,90.421171,90.3995606,-4.1611569,-4.1299873,100.4894275,123.6084033,102.1037806,11.559062,11.482412,11.482412,11.7274447,3.7130135,-63.611623,116.4073963,116.3663143,116.4073963,114.057865,116.4073963,116.4015829,116.3662615,116.421053,121.844618,-59.6406502,34.801461,-122.3289368,117.432336,115.888881,84.8782246,-64.6965892,9.9649322,-157.8706127,-157.8706127,120.9510607,120.3487663,121.0725738,86.9580281,76.87642,78.7445488,78.7445488,142.7027956,123.7180069,123.7180069,123.731579,-69.5800368,90.353196,121.195743,109.2255414,-110.9473053,142.7027956,103.8172788,124.8420794,100.5017651,8.6497918,103.792557,7.88884,-157.925331,133.0109196,140.1233063,106.8650395,-3.7063557,144.9630576,-20.3079617,153.0299405,142.7027956,100.6014419,100.5250276,100.5524127,100.6014419,100.6014419,101.6597651,12.4058161,12.4058161,3.8691262,12.237485,101.5183469,-157.8706127,-155.0779631,-103.8711895,-80.214449,81.5584975,139.6917064,14.5605484,8.4660395,106.7234744,106.7234744,106.7234744,106.7234744,106.8060388,16.337579,121.010369,110.355756,100.5097103,88.3737398,-71.1053991,131.9919421,151.2165708,-83.6300826,100.5953813,100.5953813,106.82272,90.4071914,113.9952789,110.1402594,112.2384017,120.65766,10.5292042,112.613289,112.613289,112.613289,106.7942405,-89.9872941,153.1944334,-111.6493156,-74.6079263,0.080016,-123.118889,96.1635528,-1.0795557,-0.1763672,-71.0889581,-79.2476925,-71.4025482,90.3995606,139.6423446,-78.229576,48.4898166,146.999905,-76.8835514,23.3320134,100.552348,10.451526,121.0477531,100.5250276,120.9359987,120.9510607,120.3487663,120.9487298,121.017052,121.052287,113.7632828,152.9931941,101.906256,100.925362,102.100623,100.925362,102.100623,100.925362,100.925362,100.925362,100.925362,100.925362,100.925362,100.925362,100.925362,100.925362,113.921327,5.9014457,103.1001641,50.8341335,3.7299312,79.2934913,79.2934913,121.774017,104.2861116,-0.5689174,-121.9246289,145.737881,31.2097971,31.2097971,-122.4660947,-123.8046596,-121.4690951,-117.820741,-119.7457805,-118.1140904,-121.7977985,-118.125269,-85.5889329,104.8089834,105.7468535,105.7689041,105.7689041,-75.68851,105.7689041,105.7689041,105.7689041,105.7689041,-3.1788641,-3.181683,-3.1788641,100.494,-1.3619268,8.1813171,8.1813171,-75.6960202,-79.9430128,-80.8463176,-81.6080342,53.0586328,53.0586328,53.0586328,15.0953223,106.8650395,127.0043275,4.7002953,4.6155909,4.7002953,4.6155909,173.2933987,-17.4676861,5.1770005,2.1477867,2.1477867,-71.2422603,-2.450895,-2.450895,3.8641384,27.3781658,3.8744315,92.7058432,91.3076691,68.8319005,5.1770005,80.9336418,88.3361729,80.2731246,72.8160876,72.8160876,85.8511827,85.8511827,80.9690809,120.9297398,120.9297398,120.9308655,120.9297398,120.9276824,120.9297398,80.2731246,74.1215694,83.3244957,70.375938,74.8559568,76.272114,-84.7756364,105.3829585,150.521576,140.0248611,120.7355399,103.705937,123.8857747,22.9980147,-79.4591378,101.8780351,100.5788992,101.0680282,101.0779596,98.9533621,101.909968,101.909968,11.9781232,100.5898381,102.0573235,102.0489533,100.5214084,130.8689199,130.8689199,151.0741568,14.4234889,100.1586396,100.992541,100.992541,100.522499,100.2743956,100.522499,100.522499,100.2743956,100.2743956,100.5633676,100.5811915,126.5580321,126.5580321,103.1668893,120.217172,120.2253459,98.9167568,98.995597,98.9533621,98.9533621,98.9533621,140.846095,140.1033682,-87.6351846,98.9533621,77.7066153,130.53705,130.5368514,116.682386,116.426567,120.363977,120.15507,120.335301,121.4497692,118.77875,116.4073963,116.682386,104.2861116,120.382609,113.7632828,112.2384017,120.382609,121.4737021,120.31191,101.3431058,102.832891,101.3431058,116.4073963,116.41481,121.4737021,116.682386,113.7632828,104.195397,113.264385,110.330235,112.2384017,119.7889248,102.832891,120.487224,120.345181,121.4652793,121.457491,114.057865,114.305539,101.3431058,116.352988,121.4737021,114.2109318,114.2109318,91.7884693,91.7884693,127.1293628,106.30632,126.9085954,126.9085954,127.698952,10.1147022,79.1348129,3.8496282,132.4552927,132.7360148,100.5788992,100.5775068,98.9740864,100.5296354,100.9269816,100.5323188,100.5323188,99.2500591,126.956954,121.2410215,127.4583253,127.4583253,127.4583253,127.3459548,127.3459548,127.3459548,101.528068,-110.3390336,85.8511827,72.8166445,-99.133208,-8.6381419,-8.6381419,139.6917064,3.876716,105.8187085,3.870279,55.5025595,9.7618836,110.179953,114.1784191,114.172793,114.172793,-82.8364148,-79.2821819,120.9510607,-116.9222488,80.2731246,-0.404359,9.2317163,11.1917746,12.5843335,14.10546,47.5245536,119.7889248,-0.370679,2.4961106,2.3560821,166.4416459,3.136381,2.1324916,2.2639934,-1.186318,4.7680023,3.8669676,166.4416459,2.1763537,2.3414155,7.0530432,4.077927,1.4628877,120.97128,120.5487597,99.6425883,100.992541,100.992541,99.7973271,100.992541,100.992541,100.992541,100.992541,100.992541,100.992541,100.5789536,100.5789536,100.5017651,100.992541,100.5017651,102.1038546,100.992541,100.5953813,102.0573235,100.0208109,98.8779054,-111.8282032,-111.8282032,115.8800019,105.7678715,105.7654981,-79.9371762,106.1110501,121.2353593,-3.9222799,119.493207,89.8785174,-76.713378,117.9249002,6.9727541,-104.9877597,-105.0865473,-122.65816,-73.9625727,-73.9090525,17.1160095,8.6120782,100.610157,-75.655996,153.0296929,-0.1847935,73.1565933,-96.7700579,153.156027,-72.9187917,-6.2850916,132.5509603,-39.752095,115.188916,115.2393046,115.2361107,131.2694794,-77.048901,115.2361107,115.2361107,106.8240531,121.0579128,115.2361107,-122.291206,-122.291206,2.760107,142.7027956,125.1598773,139.6423446,39.6682065,-76.4735027,-77.1315999,120.5246323,100.522499,152.99556,144.934294,139.6454822,2.1940517,100.5775068,2.213749,130.9280747,122.930004,2.648828,-5.8244232,2.114777,-6.2102094,2.114777,-3.9273778,2.836908,2.5808845,-3.6861518,-6.0059778,-8.7522039,80.9336418,73.8019463,79.7399875,76.7834577,80.9304926,146.7608745,146.7608745,144.3873531,144.3896253,144.679708,146.7608745,149.1142344,153.0299405,142.7027956,133.775136,149.0940616,149.1109152,138.6356532,115.8856356,153.010083,144.3896253,146.7608745,147.3226414,147.3384378,147.3380561,146.7608745,147.3384378,147.3226414,147.3380561,115.7901564,153.0299405,146.7608745,147.3380561,147.3380561,146.7608745,147.3226414,145.1319449,106.8650395,-1.1743197,101.6597651,110.6789833,78.5410355,-70.1917632,-84.328008,-84.328008,-84.328008,108.8076195,-84.328008,-84.328008,-84.328008,-116.6227487,106.1110501,-2.450895,100.5017651,4.0486617,-80.4437781,2.8002906,100.5017651,100.5017651,99.840576,100.5267345,100.4746879,100.4746879,100.5017651,100.5298926,99.3334198,39.9093269,90.3739053,90.4000333,85.8511827,-76.355046,3.876716,106.7521374,106.7521374,106.7521374,98.3380884,-106.829072,-110.3390336,-110.3390336,-110.3390336,-8.8041798,2.4846468,-119.1078451,-77.8701325,100.5416353,-89.2068976,103.786619,104.8606154,-1.5196603,47.5245536,2.624423,-8.7207268,128.1054495,-77.042754,2.3912362,-149.8295234,106.7893554,99.806671,103.6594267,135.7724388,135.7741315,145.4213555,8.84598,8.84598,-89.1348697,24.9383791,-73.9544488,-73.9646251,-73.9544488,-73.9492724,115.8944182,115.8944182,136.9065647,2.2297137,120.9510607,108.1536513,120.8910602,120.596098,128.6113282,126.4343234,145.0501127,-63.5916555,100.1574667,121.534699,121.534699,12.5797741,12.5797741,12.521381,12.514807,130.864334,125.670359,125.670359,125.670359,126.2200012,114.6905436,120.596098,12.5797741,83.3817193,121.042889,120.9932028,-1.13937,145.1151129,121.0244452,73.8431469,103.786946,-106.0764919,4.3758659,6.104697,4.3802156,103.9754959,100.331266,100.2299821,100.5069094,77.5980167,120.9949202,103.7786808,153.021049,121.0447282,147.6006357,51.5150077,-81.3605225,51.3617657,-105.1317887,100.5017651,100.5756696,103.0861072,116.080625,100.5755681,102.1038546,28.2703655,100.5755681,101.6831124,115.8580129,98.3380884,100.992541,100.992541,100.2743956,115.8580129,96.1026609,5.471422,100.5069094,100.565003,146.921099,100.516405,-86.5564121,100.5265793,130.983245,100.567566,-16.6525,142.7027956,153.0224813,147.328307,132.5509603,144.6370743,120.529267,101.686855,101.7122917,9.8809845,8.6729607,10.5312629,9.90413,98.673534,103.6922992,106.69762,103.692325,103.744874,-77.197232,145.9706647,18.5645949,11.3554966,8.9694947,123.3186955,110.4409009,110.4409009,110.4409009,110.4409009,2.1719655,105.8341598,88.3719879,147.3357869,13.7803199,101.696445,141.5424961,139.8237096,139.8237096,151.2180578,127.0007751,105.6432652,-73.1481218,-81.4625919,77.1674715,-75.1890913,103.786946,-6.2567865,103.83437,-76.6723069,-78.9382286,-6.393995,-75.5626325,5.4818126,-77.3645653,121.4101649,96.2867284,103.680472,36.7678607,-117.8761098,-88.1751507,101.2816261,101.4315319,101.4315319,8.6095476,13.8167934,2.1320008,-8.6291053,8.541694,53.0586328,151.267796,152.3187435,-17.4676861,1.383934,4.8271593,6.5667576,20.5692179,121.236869,-3.2132587,11.1351432,132.7750654,132.7656808,132.7656808,132.7656808,132.7725029,106.8462417,140.7339652,-96.9309783,-88.8130933,-48.3148281,-47.9029451,-47.9029451,-49.283557,105.8341598,54.9096668,54.3523963,-84.3239989,120.801475,9.8824523,-78.358616,55.4209317,54.3886344,-0.2068509,-79.8014007,-73.9872346,144.3553464,101.667563,58.5400988,-149.8295234,-74.734529,5.724524,-66.8910893,-99.1384825,-79.9644885,-2.4080193,2.7356328,8.5476699,127.40499,13.4270962,133.918553,101.7360548,102.238071,100.5013731,-56.221172,3.7082609,119.4845212,119.4845212,144.9589064,2.3601919,1.2348105,44.1910066,1.258409,2.336033,100.5341585,10.146268,4.8578922,102.8313968,3.7663056,101.711232,131.8869341,131.8926529,131.8926529,131.8926529,-107.3820598,123.9117812,7.6868565,9.2187292,-60.0349928,-74.072092,34.5508758,5.6653948,101.6991627,8.4893915,8.4893915,9.7320104,10.2378854,-46.761347,-46.558708,-48.4532218,5.1370126,7.488277,-82.7192671,106.1110501,-87.6169805,-87.6169805,178.4406103,24.9150946,-54.6217312,5.471422,106.8650395,110.4381254,100.5017651,-70.2563232,121.2545102,101.5450186,-63.5747898,-64.7855541,-123.1178786,-84.3323717,-67.0528555,-123.1112503,104.9283627,140.4467935,136.5085883,11.6524365,52.5318604,130.848733,-89.6584088,136.691881,132.2660029,140.7473648,132.2660029,141.363198,140.4467935,139.6523931,136.691881,129.7733722,139.0364126,131.609272,133.2048886,141.050896,132.5038854,100.2863038,100.2863038,100.2863038,101.9423782,100.2863038,120.4412162,100.2863038,100.5017651,145.163522,115.8580129,100.2863038,101.667126,136.691881,100.2863038,18.9553239,58.443987,4.4474978,5.0936866,13.3629531,138.5713281,138.5713281,-80.1022151,-82.6338283,-80.623022,-80.373949,-80.1324598,-84.2984889,-75.1837205,142.7027956,105.8341598,12.488965,12.488965,-122.2009223,105.8046913,140.1315284,100.5017651,101.6331096,101.6331096,77.9995589,106.6296638,105.8004621,140.1315284,135.7680294,105.5766626,7.113566,8.6517062,-105.077964,140.8721,139.0364126,-4.5583712,-76.3197819,-122.3237488,4.8657199,4.8598158,13.2896787,4.3822222,79.8356429,105.8256528,114.305539,102.0719101,102.5903099,-123.0140599,11.5715432,121.5036178,119.2648121,119.239701,119.2648121,118.089425,130.4250445,130.437949,138.252924,140.453736,133.229697,-43.952581,-43.242977,-52.1018397,5.1770005,126.687974,110.3747168,110.3774998,110.3774998,110.3774998,152.9706122,-9.0111511,3.9133792,115.2182076,128.8701409,127.743386,22.937506,32.823005,79.4879412,-74.6516931,-122.088499,3.876716,127.5183,10.7301855,103.7926476,2.4393375,100.52002,110.3505475,10.1482026,10.1482026,10.1482026,10.1826437,9.9374005,44.7747932,-77.314935,-77.521888,-77.0485992,-77.0722585,-84.396285,-81.7865347,11.4843471,12.3952673,12.3952673,121.0244452,10.5267696,10.5314925,3.7279181,136.7365752,10.4225769,102.0375565,105.8340387,127.3845475,73.834555,8.6677635,100.0372929,113.921327,113.921327,113.921327,113.921327,113.921327,113.921327,113.921327,115.0916254,-70.8209286,54.4396652,54.4396652,54.4396652,11.9492456,11.971878,-0.9050016,75.5696111,73.0699688,74.3078585,121.7536761,77.94764,79.131098,68.2779002,67.0259807,67.0259807,119.4522,81.0217774,115.8450393,139.761989,140.8308513,127.829672,-86.253123,15.4499126,147.433405,153.3806159,153.0532585,26.0846382,115.188916,6.2290914,113.348674,110.359368,110.305527,110.305527,110.305527,110.305527,110.359368,109.120161,108.2642217,108.8076195,109.120161,108.235758,108.366543,113.264385,90.414959,110.16711,110.316379,107.2902839,106.638986,107.2902839,106.638986,106.638986,128.8889,128.0939834,128.098095,128.098095,128.098095,128.098095,128.098095,100.0656849,32.8631984,-95.712891,-114.8563393,106.689566,110.198286,109.949686,110.198286,109.949686,110.198286,122.5227249,122.5227249,122.5227249,137.7439112,137.7261258,112.7925646,120.195509,120.15507,127.4211446,127.4211446,105.8306833,105.8013279,105.7833339,105.780001,105.7833339,105.7612347,105.858479,105.780001,105.8012362,105.807959,105.8433475,-85.4623681,11.5728736,11.5727771,127.0453219,127.0436832,-80.3546226,92.0058074,4.4777326,-91.7235465,123.8857747,121.0697988,-93.6257749,-71.1041157,-71.1166601,-71.1166601,-90.5204442,-117.7104699,19.0464429,119.4886845,119.4886845,119.4886845,100.4861289,100.465145,-156.7605548,35.2414009,35.2414009,8.6702492,128.738231,23.904032,23.904032,24.8092691,28.2207676,113.921327,23.7275388,25.2773184,12.4309111,12.4309111,10.5314925,8.5805155,34.762646,113.3823545,113.693866,-75.4928883,-3.320447,-8.4463114,-73.9881796,77.1674715,135.0530262,140.4732456,-93.7429401,132.34343,132.4552927,132.714908,132.714908,132.714908,139.7405684,-71.0568204,141.548781,104.195397,124.6367347,106.6874948,106.6594674,106.6594674,105.3539112,106.801639,106.6594674,-73.5994041,142.8634737,142.8634737,141.340013,144.3813556,141.3387981,141.340013,141.340013,-81.2999269,38.5048855,114.1818597,114.109497,114.1784191,113.944788,114.2654655,114.2654655,118.7198004,3.7027594,2.3650429,56.4397939,-0.061009,119.4886845,101.763899,-103.352052,102.2462288,-72.9572697,101.648912,-79.3871006,106.6785529,103.8482067,-77.0194377,97.65136,100.7575277,119.221857,116.960615,119.029348,114.353035,114.353035,114.305539,107.5797259,107.5874084,107.5874084,-77.0497039,-124.077821,13.3936551,111.8568586,111.8568586,112.94395,19.0464429,120.095035,139.6917064,-90.5925907,-123.1161624,135.1830706,-107.9263532,120.4058219,140.7570604,140.4425033,6.5673661,-121.8069487,6.5673661,6.5673661,51.4319908,106.8060388,88.3349295,85.8479837,83.3244957,82.8769541,90.3739053,90.3739053,-8.7478619,100.3091,121.0244452,139.6326463,-8.334517,-112.4297156,-6.267011,55.2903548,-4.756985,1.597567,55.2903548,-1.186318,3.876716,-1.547265,166.4440084,3.9102489,-0.7536531,51.3889736,51.3889736,101.7293984,101.7293984,46.415281,122.7343323,4.7796862,-0.174232,-0.1748772,139.7544575,139.7527995,-17.0347509,77.1628793,80.1275408,79.4750252,77.5671423,87.3105311,86.408729,-85.1077816,-86.1763393,80.175361,106.8180696,106.818061,106.878331,115.348051,115.21623,106.8650395,106.8461392,106.7953478,106.6584299,106.818061,113.921327,106.6584299,106.8461392,113.921327,101.139772,107.617923,106.6597043,106.8306951,106.8461392,106.8178525,123.9750018,106.8180696,106.8180696,106.8664656,107.668887,107.668887,-84.328008,-76.5464457,126.653116,-57.5322247,128.90269,100.5017651,104.9282099,147.0538862,49.4798635,49.4798635,54.4393363,-122.9006951,111.702182,3.8558017,-0.5770191,5.0692284,2.1736079,3.8745957,3.8570839,-61.664843,-1.7068658,6.4550797,1.3749602,-0.5770191,2.3604132,2.381984,3.7456402,106.7925977,-70.9077097,8.468946,8.5621158,8.468946,105.9413471,8.468946,10.3980263,105.9413471,106.6962576,109.201646,109.1962466,109.2023759,109.2023759,105.9413471,103.819836,10.2030118,105.815277,105.801282,139.6917064,8.2145521,103.6787862,100.5357852,105.8012335,37.5889677,-96.5783387,106.6296638,106.6667659,25.4410423,1.2363015,99.840576,24.1523097,76.95455,2.326844,102.6154804,13.64875,13.05501,-77.101331,-8.6291053,13.78434,13.7536518,26.5255946,106.8650395,-8.642663,135.5021651,102.615078,105.8341598,113.921327,115.188916,-63.596468,13.401217,106.7022657,106.7022657,106.7022657,105.8341598,5.584232,5.304137,5.3217549,-8.976366,-9.5718589,-68.1557106,139.7310952,101.6993573,101.6993573,10.1820866,120.6736482,103.7930398,-122.268468,-9.1393366,105.8433475,137.2445214,-123.4496345,16.3887396,109.2145774,109.2145774,119.7889248,104.066801,-111.9260519,-111.9400054,104.9165795,104.9165795,2.1253854,127.102603,47.5246677,3.0738908,109.1962466,2.3112488,104.9165795,103.7618154,100.2863038,106.8117,-45.0467718,6.7734556,10.5267696,8.2289999,3.7174243,8.673254,-108.4814884,-89.62636,-46.536611,166.4436551,5.36978,166.4436551,3.8673324,47.546423,106.8650395,3.8638656,166.4436551,-4.413714,166.4436551,-4.3051542,4.425806,103.8384389,3.8663996,7.5826,144.7851531,-109.9338488,107.6106591,114.8999212,112.7948972,112.7948972,105.3134384,107.6106591,-0.182552,4.4777326,-82.4769679,4.398266,10.937424,11.6524365,69.2400734,11.9586467,106.8650395,7.4238769,7.4246158,3.876716,-8.0724358,90.399759,90.3985457,90.4000333,90.399759,120.9932334,120.9932334,120.9842195,121.0222565,101.5016154,95.332918,4.3568515,103.3057594,101.7293984,101.7293984,101.7293984,101.7293984,103.302854,105.799092,139.6917064,101.6871485,101.946044,6.1320516,135.666974,121.2333155,121.2333155,121.033402,121.0199051,136.8638684,49.5891233,-98.4178688,106.801639,106.801639,126.9599478,-95.712891,122.5372741,-8.6291053,34.9162063,34.9166655,5.9384815,103.705808,101.7582411,4.1891398,106.477017,-2.6646497,-93.6464654,10.1227651,50.820314,49.5891233,49.4798635,51.3889736,51.3889736,51.4050447,51.1866456,3.8673324,115.2126293,3.128874,-17.4335003,115.2126293,106.8120699,5.36978,3.8638656,3.8673324,2.1253854,166.4436551,2.3306131,-3.9245084,121.254547,121.033402,121.0199051,0.659952,121.6919034,51.7823742,122.5576156,51.6679826,51.5319635,89.1511164,71.4758673,56.3363973,56.3363973,51.0753908,54.3125619,48.5600002,53.5776,49.8174942,49.8174942,49.5776188,47.0022395,51.4007017,110.4129219,86.4531326,28.8569495,-9.12603,12.4963655,34.951925,34.989571,11.4672909,12.4963655,107.6106591,6.8858056,18.0730402,7.6868565,34.888822,139.6917064,141.151564,141.1384546,18.4173352,-77.1990178,8.6542654,88.3718397,90.4110963,19.9341074,90.2674411,110.3395543,101.13442,103.6670838,146.7565073,145.691746,146.7565073,136.5930972,139.649923,139.6454822,139.6917064,139.6917064,139.6917064,130.4017155,139.6917064,139.6747678,140.086911,127.945404,100.3327617,139.7470855,139.7197809,139.7197809,131.609272,127.6809317,100.561539,-77.0452025,1.444209,77.0262794,126.5580321,126.5580321,126.5311884,109.2453948,109.2453948,109.2453948,120.277464,119.7889248,119.7889248,120.894291,115.8040853,113.921327,118.097337,36.8559734,113.347721,131.422437,140.086911,139.389932,106.842907,8.2415552,-87.614038,-1.2216281,-76.5906229,-76.6205177,103.742059,-0.239061,109.193759,106.6296638,105.8341598,139.3420253,123.422625,-83.2256739,14.4878586,121.058199,30.9419751,134.037969,130.553405,130.5573749,131.4126201,130.5452401,-85.598145,141.8856772,158.677726,146.561647,100.67013,139.6294307,139.6362085,139.620867,136.682253,136.7087378,128.8701409,128.8701409,128.8701409,127.743386,135.756584,-96.5847249,135.5021651,120.5246323,120.3097772,-61.6508831,39.7713825,37.1313493,15.4499126,8.4170303,76.4453743,77.4776166,74.8669778,77.4776166,74.8669124,18.0236579,18.0345725,100.5696141,100.5696141,100.5696141,100.5696141,100.5696141,100.5696141,99.9748993,99.9748993,99.9748993,98.6084641,4.7002953,4.6155909,39.104697,135.4057755,106.6584299,119.7889248,55.280046,5.8987296,101.6399208,-84.8558314,39.6820741,39.6820741,37.906193,36.7803998,76.3172304,122.207215,102.8278044,102.8278044,102.8278044,102.7462442,102.8278044,48.2038665,89.5323901,71.4441274,39.2575481,39.3073267,39.104697,100.5355885,100.7793163,100.4941512,100.7793163,100.7793163,100.7793163,100.7793163,100.513886,100.513886,100.4941512,100.5143626,100.4941512,100.5143626,46.8533173,46.6236596,46.6236596,-0.115997,135.587042,135.587042,135.587042,135.587042,128.6210824,128.8760574,139.7265757,141.1526839,139.3927379,139.72563,139.72563,141.1384546,166.4416459,105.8642593,39.6820741,100.7793163,120.3097772,5.1770005,117.8833262,135.2838829,135.2337002,135.2353953,133.5065983,133.4867065,133.4867065,103.066876,127.1399993,127.5183,126.7052062,127.0320039,-73.955756,129.0756416,126.8526012,127.0448511,127.0286009,127.9259497,126.832652,127.043023,129.4004195,128.8508106,127.043023,126.6468234,127.359903,127.0277773,128.1555,71.4812716,98.8779054,87.8549755,127.3845475,14.146265,108.3377614,131.255828,103.3057594,130.7078911,-81.1637245,130.7278725,49.041312,101.8980447,102.682988,126.6789191,123.670587,133.0109196,103.1324154,48.0901096,47.9175779,139.758024,135.718581,135.7808218,135.8092224,135.7819708,135.7808218,135.7808218,135.7808218,135.7808218,140.6111865,127.052544,128.6113282,128.6113282,130.8392259,130.7433975,130.4250445,130.7278725,122.5442088,-57.9511629,145.048414,3.7070107,3.7070107,4.7005176,-92.0198427,3.876716,2.8948332,13.7768182,3.709902,2.148201,3.8778681,166.4416459,3.1987259,121.04544,135.935022,100.992541,100.1927389,114.585885,105.2431929,174.848892,12.4963655,124.1435427,1.287125,8.2211873,-122.4217422,121.1022942,9.9708462,8.84598,8.84598,8.84598,13.404954,13.52148,13.64875,13.64875,10.1826437,10.1826437,11.5728736,11.5728736,13.52148,13.52148,4.485209,123.918194,-7.7208889,-7.7208889,-81.7350812,121.561079,-9.429499,-74.7106391,116.1115942,-2.760847,135.8853671,106.8772556,15.5760072,16.3486763,128.1990854,106.7953478,106.8497592,106.818061,-2.9712917,102.5247765,-0.1302803,-0.1302803,-0.1263442,-0.1302803,7.5979752,-106.3236904,-118.2436596,-118.2436596,-106.7371425,-90.0018973,-91.1800023,-91.1871466,-90.6618072,80.2340477,-87.6581921,107.763621,121.358532,13.203493,-3.860555,-95.712891,112.5890265,151.1126498,39.9980235,80.2670688,-3.6895273,78.0105374,99.8943462,99.8943462,99.8943462,99.0133136,99.0133136,99.0133136,103.2839975,100.8555194,100.8555194,103.2839975,103.2731634,103.2839975,103.2839975,103.2839975,103.2928463,100.3234171,100.3234171,100.3259181,99.1596365,100.3234171,100.3259181,100.3234171,100.3259181,100.1473133,100.3234171,100.3234171,100.3234171,100.3259181,100.3259181,100.3234171,100.533636,100.3234171,101.6495633,101.6495633,110.1402594,51.3889736,-69.6404509,-69.6404509,-68.7968142,102.6216211,102.6216211,102.6216211,103.819836,48.8498623,101.7015291,101.7676301,101.7424941,101.694244,101.694244,101.6390003,101.7670464,100.2916279,101.7110105,101.7424941,12.9945611,174.848892,-2.238619,74.9231362,93.9280425,73.22068,-2.3444786,121.021473,101.6980726,100.9070259,101.694244,101.518461,120.5558961,15.2499743,100.2743956,144.6286897,-4.1394582,-4.1394582,-70.6724209,98.408458,141.1526839,139.6917064,-2.5684655,135.5021651,139.9330266,10.97561,103.1324154,100.5017651,107.5805873,107.6191228,-8.976366,127.945404,-120.1832533,73.5163893,116.0759274,116.0759274,-21.9298816,-79.9009868,35.0063209,-2.0716575,100.5953813,103.801314,-76.5036455,16.605384,54.619113,54.619113,-8.408663,-71.0688305,175.619553,116.0921869,123.8917708,-21.7661839,-21.7661839,57.4783981,57.4947311,8.6298507,12.969643,9.058603,10.4339215,9.93757,13.3757242,11.94193,7.40964,8.9978309,53.093263,-73.5771511,-73.9388221,-79.9192254,-93.215629,-77.031439,-77.031439,96.195132,-79.946747,-79.946747,139.706648,139.5060464,139.6917064,139.548665,136.9750983,-48.5482195,127.9741678,106.0287512,-73.9559837,-52.7329053,-52.7329053,16.6161905,30.73476,4.835659,31.0130822,103.7789145,-1.1743197,138.3830845,0.0302354,101.7676301,-84.4821719,10.5314925,34.2550061,-87.9993831,136.6244847,136.521503,9.2297627,-88.974817,124.2433571,124.2439813,124.3220139,124.2951949,124.2951949,124.2876362,119.743846,125.12257,125.12257,124.2439813,124.2876362,119.743846,124.2547113,101.6930865,121.0445976,121.5269672,120.5161352,105.1552963,110.3502627,100.5069094,100.5069094,100.5069094,58.4275889,73.5129435,110.3537492,102.6198846,105.816438,106.1110501,105.8189274,140.1030455,104.9268394,120.382609,109.1880047,89.6353186,114.305539,108.084731,100.5069094,-84.1197643,100.5656374,100.5017651,100.5089831,98.3702489,101.708815,58.429534,58.429534,90.4000552,106.8772556,51.5285655,104.9388018,51.5285655,90.385934,159.9728999,115.2585973,106.8772556,104.89908,101.7072254,55.7181072,47.5079055,105.824832,159.9728999,106.8325507,85.306168,32.5856561,-1.5092521,104.8987707,103.8337475,102.616867,102.616867,104.9282099,96.1030592,105.2598819,106.8326516,106.8326516,106.8326516,100.538737,159.9728999,103.846605,102.616867,100.5246631,102.616867,106.8218281,101.686855,100.5250276,99.3334198,99.3217483,11.5020752,100.5289512,100.992541,101.6988824,101.6988824,129.940155,-88.7943766,-88.7943766,-71.09416,139.6917064,3.8547795,8.675277,3.8547795,3.8547795,140.8721,138.252924,131.4202411,131.4126201,92.6672765,2.3560821,139.6917064,126.4360807,127.3386078,-123.1207375,101.6009825,101.600835,101.6009825,101.6009825,101.6009825,101.6009825,101.6009825,101.6009825,101.6009825,145.1362182,106.9057439,-46.697312,-111.04981,-74.1985597,-121.788163,3.8558017,139.6423446,37.5286696,-82.5774491,109.1880047,101.5985588,-16.6888211,80.2459588,79.707466,103.631752,124.2433571,124.2439813,-64.3731004,-72.5761394,28.3713487,117.1541118,117.1541118,122.9546503,-83.8473015,115.8302603,115.8338628,6.1355918,4.518294,4.518294,-101.5291818,-43.2265289,130.8330811,130.8344436,130.8330811,130.8330811,116.4073963,6.158321,-1.151319,16.3598451,138.436415,139.7765252,-3.9170332,-118.2887703,4.4751528,2.3560821,174.778873,146.8222102,144.9717408,145.2178747,106.7969367,144.9717408,98.7283473,101.5960787,114.8552971,96.1026609,-78.7157267,-78.638485,-78.6820946,-78.6820946,-3.5350335,-2.2301193,-123.116672,100.57559,139.8076127,80.5308703,136.906398,129.877667,129.841409,136.934405,136.967573,121.4737021,44.5322252,104.7821066,102.0977693,102.113251,101.230431,4.860199,115.9048711,115.937347,118.796877,120.31191,118.774763,118.764947,118.756392,118.7789602,118.726956,120.886132,103.8490221,103.6831347,103.6831347,136.9632271,100.3684595,-3.2132587,135.7328585,135.828497,100.1922918,99.8963421,100.1922918,100.5250276,120.9866145,140.1210169,76.2710833,4.4751528,77.5615159,140.2840677,7.4489427,12.5771078,8.682501,-118.2887703,16.3598451,8.6517062,-0.1763672,-0.1763672,-118.2887703,16.3598451,-2.094278,133.1747162,0.5383585,103.8853654,103.8853654,127.0286009,121.0501241,27.598327,140.1021163,102.6331035,102.6423441,130.7078911,100.5017651,100.6014419,79.8715183,-122.4019261,80.9343732,76.2710833,81.0369873,77.1507927,100.527754,121.1922029,120.217172,120.999263,120.4851352,120.675326,120.675326,120.999263,46.6752957,127.0286009,-105.2750307,104.195397,100.6014419,100.6014419,100.6014419,100.6014419,100.6014419,100.6014419,100.6014419,100.6014419,100.6014419,100.6014419,100.6014419,138.5726697,105.8341598,109.0526076,23.5031044,116.324996,105.8341598,104.8606154,106.6880841,82.239156,121.541773,121.541773,121.541773,103.836188,-3.530875,12.087845,9.5549217,100.3327617,100.3327617,100.3327617,100.3327617,126.7052062,128.4010915,128.7100381,-73.6995749,121.030404,129.3434808,129.221143,121.030404,128.4010915,131.4706493,120.9288314,79.1750024,114.305539,96.1515927,105.8341598,100.5017651,140.1338907,3.8570839,127.0286009,140.0992679,140.0992679,140.1062562,137.1651934,13.5682895,88.399816,88.399816,100.6242201,100.6242201,28.0473051,127.0286009,100.6523929,126.6402802,140.1194881,129.2227084,51.1687455,106.8650395,-78.8831335,73.1519319,74.7943104,105.859707,-82.3665956,139.7175464,105.8341598,140.1194881,105.8341598,39.1104453,103.8453792,5.3220544,105.8591633,80.2119021,73.8053066,34.9568842,109.2145774,121.4737021,76.7317078,139.4088764,26.0459189,105.8387849,21.969309,172.608207,174.805119,174.7617145,175.3220313,174.805119,174.7779642,120.332831,120.3146027,120.3146027,120.405332,104.8606154,-117.2508403,-70.6739588,-88.5612262,20.9034248,147.1899636,-3.2730567,120.6985326,120.6989058,120.6989058,121.7983378,121.7983378,139.7765252,139.7765252,139.7765252,4.4751528,4.4751528,-77.0300509,4.4857176,-77.0260654,120.665551,120.9812475,-3.1771279,4.4751528,106.8226389,106.8226389,119.580436,119.580436,120.5037221,103.8162313,100.5798571,100.3684595,100.3684595,100.3684595,121.614682,103.7763939,9.2741868,112.2384017,31.2051298,31.2051298,130.5571158,139.6523931,136.691881,131.609272,136.691881,139.6115633,121.5146219,138.515395,138.515395,132.2660029,140.8110045,139.6523931,100.5696141,142.8634737,102.5298028,100.6035597,100.530629,100.6035597,100.6035597,100.6035597,100.530629,100.601741,100.7063388,100.6035597,100.601741,100.601741,100.601741,100.6035597,100.601741,99.3334198,139.7765252,100.7173566,139.7765252,100.6035597,100.601741,105.4068079,100.3684595,103.8490231,106.822746,120.265843,102.0977693,121.5453446,121.067897,121.5275321,121.7726863,121.7726863,121.7726863,121.5397518,121.5397518,120.5441003,120.9932316,120.9932316,-8.4836169,105.7816119,103.7834994,106.7929669,-8.4932737,-8.4932737,102.6344767,101.7774225,101.7005373,101.7727263,101.7005373,101.7727263,103.7831756,120.747348,103.7763939,103.7830556,103.7763939,17.6547742,80.771797,121.513702,121.513702,28.1903771,4.4751528,115.2586709,159.9728999,-77.0262443,100.9846717,142.4583605,99.5169683,80.9343732,-77.4334876,-77.434774,91.8977131,92.9375739,92.9375739,-16.6733441,4.8951679,123.3033843,122.9549969,122.9549969,25.6668562,-3.2063743,-4.1626569,4.4751528,4.4751528,5.67079,4.7872977,100.57559,100.5017651,-122.1817252,-70.9058197,99.827635,-75.130634,-74.1793409,-75.1312033,-106.7479059,152.1424555,151.8683913,151.2096088,-73.6055348,-73.8097434,-73.9413798,103.8597841,-1.6146608,121.4691774,146.921099,121.030404,126.7052062,129.0756416,121.030404,109.2023759,109.2023759,109.2023759,109.2023759,105.8004621,109.1962466,105.8341598,-77.1120071,-77.1044932,140.1194881,146.921099,100.5017651,-77.1008649,126.9779692,105.859707,105.8341598,139.7374214,139.7374214,139.0364126,105.7924437,121.561609,106.1369804,31.2357116,139.0364126,139.7591856,131.609272,139.6917064,139.547412,139.547412,139.547412,139.9928529,105.7674207,174.7617145,174.805119,144.9362439,-111.6547075,153.4130796,-87.4044868,132.5509603,132.5509603,-76.6724601,-117.2508403,-117.2508403,-80.1621812,-71.5163049,-121.8946594,-121.9349521,-122.0648352,-122.2570527,-77.0327262,91.1003313,91.1014557,8.468946,10.7560171,106.7925977,11.5096919,-0.363888,-96.8031712,27.0931653,-71.0891717,123.1558571,118.767221,105.2461642,105.2045573,106.6880841,108.073742,108.073742,108.073742,121.4376497,-121.9796149,10.714164,10.4066231,5.3220544,10.7318192,10.7678837,10.7678837,10.4020771,10.4020771,10.4020771,10.7522454,-77.0631359,-80.1109767,-122.2883384,106.7004418,138.3035837,122.8986812,-63.5873979,103.6515571,100.5287241,100.601741,100.6030723,100.6030723,151.3806853,121.100283,-1.2624309,103.7763939,103.7763939,123.6589219,113.921327,126.9568209,108.073742,108.073742,-73.9964609,-82.2236001,143.1731714,7.3067537,7.3067537,-70.8411558,-70.8411558,-117.8265049,129.077071,120.335301,120.335301,79.8722087,31.043472,133.775136,174.467589,10.4282364,100.5658033,147.2000138,143.95555,151.2064092,-61.6947193,109.1967488,102.6277988,-117.2511466,142.1919184,-83.0309143,-81.923104,-83.0693059,131.609272,131.615037,133.9222225,127.829672,127.829672,127.829672,127.6809317,127.7171899,127.829672,-97.0737222,-76.3067777,-95.9273508,55.7706188,26.412968,-80.0379376,114.1557148,0.0237402,-124.4014335,-122.8382608,-122.6858606,-123.2794443,11.5104586,140.1233063,-5.0390536,106.8650395,3.8725631,135.5050308,135.5439842,135.5021651,135.5021651,135.5077753,135.5244676,10.8791935,139.6917064,-1.224225,106.8461392,-74.0048567,-123.9554061,101.1192804,101.1192804,131.9919421,120.9842195,107.7746881,107.7746881,107.7746881,107.7746881,106.8111634,139.6423446,17.2600152,113.9034136,134.4661331,118.78551,120.9764125,76.7684117,76.7684117,122.070714,-157.9628849,145.7856324,-0.0475778,21.0089375,104.9165795,102.615078,-122.3394246,150.9881962,128.1957229,-46.651126,99.032191,4.3285405,-107.3994245,-82.4769679,116.3058739,115.823481,-77.8599084,45.0036487,100.2047691,55.975413,50.820314,50.821576,101.704701,106.8327857,99.6425883,100.992541,106.6651052,-3.7539381,-3.7539381,10.6749901,12.0115905,106.69104,106.69104,99.8963421,101.1526526,99.6425883,-75.1617544,121.2253976,121.223756,122.5372741,121.05586,121.0565244,120.890792,121.255439,120.890796,120.890796,8.7741641,121.1112615,100.5632844,100.5305229,100.5305229,98.362994,98.408458,98.408458,98.3869671,98.3869671,105.2045573,12.4963655,100.2048525,120.4703258,142.7027956,113.625328,105.7752173,28.285722,18.77258,-4.14782,23.8676724,18.8414567,21.0147439,9.2297627,13.5127759,119.5571677,-70.6403072,-51.1749011,-1.1743197,136.8781411,151.8409214,51.5150077,-1.0935923,-1.0935923,-1.0935923,100.5017651,100.533636,132.4596225,130.7278725,103.8988531,103.819836,103.819836,100.4986265,100.4986265,100.4944626,101.2367954,98.3520781,99.3566244,100.4986265,101.2367954,99.5787539,101.2367954,98.3520781,120.960515,100.4986265,100.4990782,99.3566244,100.4986265,100.4986265,99.3566244,100.4986265,100.4986265,101.7898334,-74.6514481,14.444216,77.2231782,-75.5395072,123.880193,127.6809317,98.5196269,106.8132612,136.8675427,137.107921,100.612739,100.5689599,114.166203,39.7813091,129.1031735,129.1031735,129.1031735,76.4496525,-86.9211946,129.0797747,129.0797747,106.8461392,106.8461392,106.8461392,101.7072254,102.063997,106.8430525,51.4876252,51.4876252,153.1964961,153.027266,101.8222356,108.654146,109.535136,-0.0403745,-5.9340493,-76.4951412,152.9907692,153.010083,151.1246311,139.4941622,153.0224813,153.0225762,142.7027956,142.7027956,153.0437568,153.4168187,153.031221,146.8222102,153.017992,153.017992,151.242781,153.028415,118.859457,128.1912981,103.4937107,5.8568877,5.8568877,114.9313054,100.577309,103.4937107,103.2839975,100.5103509,100.5999827,102.863597,102.1179487,98.952871,99.595546,100.5999827,99.4182057,100.5999827,100.5999827,99.309753,100.5999827,100.957752,100.537347,101.062116,102.117727,100.5999827,100.5999827,99.309753,98.9528273,80.2704588,76.8791673,88.6376273,100.5267864,102.1035236,100.6512131,100.618601,100.5854247,100.5854247,100.5854247,100.5854247,98.6084641,98.6009952,98.604495,116.419389,51.5360368,51.5360368,37.5778246,-88.1424262,101.2816261,47.1087916,127.0286009,96.195132,-38.5089752,115.242552,13.518915,-80.4548025,116.0753199,30.802498,79.7399875,99.9599033,100.2591955,100.5953813,115.2393046,106.8105998,106.8178525,106.8650395,106.7942405,106.8060388,106.8105998,106.8430525,106.8664656,106.8650395,106.8664656,106.8461392,106.8461392,-0.3560166,105.9413471,106.6887649,106.6962576,105.9413471,106.6887649,105.9413471,105.9413471,105.9413471,105.9413471,106.6880841,105.8440675,106.6962576,106.6962576,109.1962466,109.201646,105.9413471,106.0111203,119.6962677,120.5279,59.4247484,106.8060388,107.668887,106.8060388,106.8650395,20.1420241,135.7644644,119.4327314,106.6880841,106.8664656,121.03703,121.03703,121.03703,106.7213155,107.668887,107.763621,100.475,106.7942405,-53.935519,6.0656869,136.2091547,-71.408908,26.5163135,107.1816257,106.1110501,119.6962677,106.8060388,106.8336118,-95.4819592,139.6128256,106.9442118,135.7233044,4.518294,144.9642263,100.5017651,-73.9556889,26.045824,12.1407801,12.1407801,-62.760681,4.4491408,100.572311,100.9506095,-3.2092309,151.2180578,151.2165708,-0.295573,12.5870231,12.5870231,12.5870231,100.5803978,100.5803978,-0.5630625,18.0706646,100.5581347,101.0921473,144.9559889,4.0571163,4.7977149,4.7977149,-79.3947771,-79.3947771,100.5803978,18.0518802,-75.7502437,12.5724096,12.5420772,-3.105353,-2.7676454,101.557408,106.6887649,-1.9823598,7.2616093,126.9807358,37.5778246,37.5778246,30.2968626,30.373985,131.8930623,109.193759,-74.4473991,-74.1753095,-74.4282545,-75.2223617,-74.4473991,129.0598736,135.753372,127.945404,138.6031129,138.638592,-79.8993248,113.3536811,113.3536811,113.7632828,31.7073682,116.0650939,116.6408951,80.7872185,10.9359536,130.2915283,123.4642535,-77.41165,-77.4137129,106.6822583,139.608483,-117.2453427,-117.2453427,-117.2453427,120.878393,13.0605021,124.8272348,124.8272348,124.8272348,100.3090992,100.2740821,100.3090992,123.9115758,-117.1474594,-117.0718893,-122.406073,120.9809677,44.1904983,-121.8061921,126.955252,126.955252,-119.685713,-46.6618971,12.5144384,141.3321401,110.3518389,110.372095,110.4995965,50.1485605,101.5183469,21.8568586,-2.1015257,100.4941512,76.2828612,142.537571,-1.6146608,-1.6146608,-4.1611569,96.1314245,99.3334198,2.8304263,-5.4411761,-5.4411761,47.0291162,-117.7102128,-117.2503233,-117.2503233,-117.2422769,10.4282364,122.352239,122.352239,122.3543397,122.3543397,121.062647,122.352239,122.352239,121.2175622,103.1324154,122.352239,-90.4688572,151.2023358,142.7027956,23.741862,153.4255161,122.6616851,121.2175622,122.3543397,121.062647,122.3543397,106.8257375,-97.1722807,110.8567293,121.4737021,103.7413591,103.7618154,-107.4795173,166.442932,116.0753199,123.94475,129.7733722,127.945404,127.945404,115.1485647,8.6821267,8.6517062,14.9867053,8.6517062,126.9519053,127.0014281,126.9519053,126.9519053,126.9519053,126.9989668,5.3459885,112.7948972,108.93977,51.3946864,51.3980673,51.3980673,122.120419,121.5232173,121.483138,121.4737021,121.432841,121.896556,121.432841,116.681972,111.549913,111.436217,14.4883442,37.511034,106.2431205,114.057865,103.7904787,133.0681165,130.9424672,130.945876,137.9789403,52.5236912,77.5751227,138.3827596,138.4317,99.2826962,99.2826962,99.2826962,100.6014419,104.066801,104.1428146,130.4017155,88.5971547,123.3090576,123.3066689,123.3066689,100.4902354,100.0401043,99.8615203,99.8615203,-122.9198833,115.9557124,115.8556098,68.5433463,103.8159136,103.679736,103.8362083,103.949547,103.772203,103.772203,103.7796987,103.9637891,103.793023,100.0401043,-81.0216773,10.8278106,75.6870856,17.1058496,17.6628688,-72.6372341,-77.0212813,-77.0212813,-77.0260229,-79.5473269,-77.0212813,-79.5438074,-80.1917902,-79.5322079,-79.5438074,101.790988,11.563548,153.297815,153.297815,106.6296638,-89.2249134,-89.995654,121.0651477,113.334184,120.382609,118.089425,2.3522219,11.976675,11.976675,11.976675,11.976675,55.6578603,110.4037357,31.7073682,105.7756477,139.3285205,139.608389,37.6575682,100.6136497,100.5953813,100.4986265,120.651928,121.5457421,126.93169,2.3432774,26.5198588,26.5198588,138.6031129,113.3536811,32.7452739,122.3543397,122.3543397,122.352239,121.2175622,153.297815,39.7285959,39.7285959,124.9799425,124.9799425,100.5876017,106.6296638,125.6198369,108.277199,106.595843,142.8634737,77.6527774,100.5345214,100.565277,100.9834355,100.5345214,100.58553,104.732662,104.732662,80.043554,80.0452801,103.819836,-3.2101853,-103.3418711,5.0594915,-61.7737202,-61.7737202,-90.2379054,-122.1101353,121.0351823,-74.45942,77.7387702,77.7387702,77.7387702,9.2060188,11.0119611,9.1903016,105.8341598,-121.9042029,-122.169719,73.3242361,-96.6011518,82.2373947,112.618478,120.15507,120.382609,118.089425,118.752143,112.6155368,3.7280873,119.4293562,112.618478,134.070162,136.8252838,39.1918298,3.6889843,73.3242361,73.304315,8.7246962,14.2360345,18.0552015,18.057831,-0.244589,9.1903016,100.5103509,100.5087796,100.5087796,-71.6228464,32.269729,-103.6497628,101.524094,58.1730322,58.1730322,58.1730322,58.1730322,58.1730322,103.082795,119.3018747,113.298883,113.298883,113.576677,113.298883,126.993606,126.993606,126.9753024,101.6095502,-73.945734,-73.1233889,-76.1390412,102.0206962,102.0206962,102.0206962,102.0206962,99.3334198,126.047129,103.4937107,103.4886789,103.4886789,-157.8583333,105.8341598,122.352239,-117.2508403,104.0333107,-97.938351,102.0206962,-3.948239,18.0899315,17.6628688,110.3572259,145.0389546,8.6095476,8.5476699,7.577261,7.577261,7.577261,151.2133071,151.2568644,95.3678808,95.3698263,91.901952,19.36137,3.8973147,121.4272092,121.561618,120.801475,121.7391833,120.609764,120.3014353,120.2880624,120.9674798,112.5229278,120.542329,120.542329,140.189868,130.7248211,121.614682,130.7078911,116.0929372,76.9354529,78.1072634,80.2670688,121.4500026,103.846107,109.345425,30.9991409,39.263978,38.9733768,75.9103429,52.0429511,52.0429511,51.3840198,148.5817624,106.7891017,108.0248652,101.6174411,31.1984178,-94.0022229,100.6358309,10.5292042,12.521381,12.521381,12.521381,12.521381,13.7289964,13.7289964,11.5678602,11.5678602,107.6106591,101.5037611,100.628807,34.8043877,34.8432612,34.955621,103.776992,139.6917064,103.8657583,-96.781345,-96.336499,-97.3241433,-94.8183176,-97.3627717,-101.8782822,-96.3535289,100.513886,105.8126125,100.5017651,100.6074577,100.6144519,100.57301,99.9472244,99.9472244,99.9472244,100.6081933,100.4908128,100.6130605,100.6130605,100.6130605,100.6130605,100.6130605,-81.2999269,100.6220295,118.089425,79.2110118,100.5017651,12.089746,117.1303827,117.3616476,117.3616476,117.708631,88.7878678,100.71491,112.5394091,100.8018045,139.7250331,140.8780533,140.874074,140.7473648,140.8858281,140.781174,140.874074,138.5146583,139.6845412,134.5548438,139.7264134,139.6835302,139.764451,139.712321,139.6223903,139.377033,139.6320116,139.6320116,139.6320116,139.7470855,139.7470855,139.7470855,139.3393495,121.4990819,134.1733553,132.4552927,137.2136768,137.4086346,101.6147871,99.3334007,-78.2902191,-6.2496295,121.0219862,-98.4831622,106.7902797,3.3792057,-117.2330895,-92.5808645,116.3264439,116.3264439,13.3408847,-71.1190232,-71.6771811,-71.6771574,-89.9707787,103.3885498,96.195132,103.3885498,103.1324154,65.5329479,55.676034,107.5908628,74.8559568,-58.3730695,104.8475639,104.8475639,104.9001281,104.9001281,104.6900752,-0.1340401,4.6155909,-118.4407589,-3.9204979,101.733255,115.1723921,115.1723921,102.7945178,-43.3689823,-48.4532218,1.9362478,-43.6850354,-3.8363562,-5.475509,2.645559,101.447238,-92.924745,101.718542,4.3822222,48.3899555,2.3912362,3.8673324,7.5734783,19.82723,20.3036711,103.428906,3.876716,3.876716,103.0880208,103.0880208,85.3145103,124.3310364,-47.8706311,103.8218107,4.3568515,116.4073963,-47.5442132,-49.0317116,-48.4943054,106.7407651,106.8205147,-77.0862793,138.0803529,101.7278699,10.2030118,10.2030118,10.2030118,2.0927299,-2.1015257,-2.1015257,-2.1015257,-2.1015257,-2.1015257,2.3413236,-25.6637984,38.8105513,138.6062277,138.6062277,106.7925977,106.7880746,73.0671994,77.5776269,74.8669124,74.8559568,77.5776269,74.8484286,8.6204401,122.5630241,112.7583638,5.4396339,-81.5113386,43.3057707,-86.8063416,-147.8335418,-147.8335418,-113.5263186,-3.3628674,29.9132281,-7.9722079,-0.560858,-0.5132249,4.9557263,4.9557263,13.5127759,100.4595468,100.4595468,100.4595468,74.8669124,-52.301034,-75.5691113,-70.4053939,4.404815,8.2116446,73.0814936,-110.9501094,-94.1736551,174.7691073,174.7983785,-73.2516591,-102.2913556,2.16402,-3.6943619,-100.3119038,-100.4109356,-89.623128,-8.6573943,66.9906101,-7.9882836,106.0776645,-4.1299873,2.16402,7.5826,15.8086005,-2.9678921,-2.3272635,11.585831,50.2482693,20.4575676,102.272497,1.2130002,5.3217549,7.438119,8.4918287,59.292513,-1.8882174,3.1127133,11.3560278,11.354157,7.0865227,6.0226187,5.066068,-47.8706311,112.613289,112.613289,112.613289,8.85204,-4.5627205,-3.3854288,-2.7461995,-0.087914,-2.6029792,-123.2459938,-123.2459938,-123.2459938,114.8999212,114.89397,114.89397,114.89397,114.89397,114.8999212,114.8999212,114.89397,114.89397,114.89397,114.89397,26.1012481,-58.3730695,87.8521916,12.326197,-6.2977486,-6.213326,-0.363888,-0.363888,8.3410031,88.3630965,88.3630965,-114.1339589,-122.2585399,-123.0718536,-121.7617125,-121.7617125,-121.7613572,-117.8442962,-118.4451811,-117.3322661,-117.2340135,-117.2340135,-119.848947,-122.0582771,-120.4254615,0.1149085,105.7689041,105.7689041,149.118527,172.5839233,-1.2908962,18.4611991,18.4611991,-71.3508732,-73.0513655,-81.2000599,150.521576,-17.4641355,-87.5987133,-70.6509277,116.682386,91.8358461,-8.488858,-79.9524954,-8.4241509,-103.698803,-8.4932737,-4.129263,103.0880208,103.0880208,103.066876,103.066876,9.2797328,-3.948239,6.9288192,79.8606821,-3.7285872,-3.7285872,-73.0341825,-72.2539805,12.5724096,12.5724096,12.5724096,12.5724096,12.5420772,12.5724096,12.5724096,12.5724096,12.346539,-84.0510292,-2.922234,2.4082599,39.3122578,103.0880208,103.0880208,-75.1603986,77.1926284,-104.9651706,-1.4965209,90.392784,110.4409009,-74.0653932,-90.6956357,-2.9821428,-2.9821428,30.9807272,-1.5782029,6.794503,1.2458189,124.6670097,-3.1892413,6.801008,0.9472069,-43.1753638,-47.0647206,-38.5386862,-38.9716384,-38.9716384,-41.2905536,-49.0317116,-46.6514327,-39.0272993,-40.2709711,-77.9959949,-1.67296,-2.7461995,-7.9056599,-7.9056599,-5.1238373,-3.5351475,-38.5089752,-43.1167225,-44.3093764,-43.9663841,-49.2617658,-34.9504323,-35.2983545,-43.1753638,-43.6850354,-35.2007727,-51.1162263,-52.1018397,-34.9492219,-43.6975642,-48.5196415,-47.8802454,-47.8802454,-47.8802454,-37.1023059,-48.3619284,-47.9311777,-42.8785858,59.5295839,11.6262692,-83.6547373,109.2023759,11.259765,-80.1637775,-82.3549302,-81.438451,-81.712026,-82.378889,-82.378889,15.5665863,8.6613099,8.6677635,26.1869783,7.8459496,7.152006,10.1018956,110.3774998,6.1451157,8.926743,-83.3773221,33.5280471,3.6940638,3.7279181,3.7279181,8.6780201,2.8304263,-4.2882005,37.4442387,11.971878,9.9374005,-3.5999228,0.0763389,5.7710128,6.5629872,6.5635813,144.8033219,-80.2268613,35.0195184,11.9695315,11.9695966,9.9846195,112.7925646,105.807959,119.4886845,-157.8171118,-157.8171118,-157.8171118,-155.0814688,-157.8171118,-157.7576325,-157.7839021,-157.8607075,8.6702492,24.827682,24.9510419,6.19405,-4.1827045,-4.1827045,102.613907,9.2119425,114.2622729,114.210469,114.1370848,56.4397939,56.4397939,-81.605371,17.2382219,-0.3367413,78.326384,-21.9518896,2.645559,-88.2271615,-87.6474502,-88.2271615,101.4368427,101.7774225,101.4368427,106.8270482,106.8270482,106.8270482,11.3838006,8.827665,20.8450898,-91.5510462,106.8270482,39.188567,11.5898195,109.2453948,109.2453948,27.999315,28.0019147,35.0238648,-74.067801,-95.2557961,67.1207023,8.4170303,74.8365172,110.4037357,101.7005373,101.771045,101.7774225,101.7774225,101.7774225,116.1183261,101.7254901,101.7774225,101.7774225,101.7774225,101.7774225,101.7774225,101.7774225,101.7254901,101.7774225,76.9473306,32.5436657,32.5417112,10.1809815,8.107545,9.1868777,110.4995965,120.6376364,102.321226,30.3959516,30.3959516,30.9807272,30.3959516,166.402528,55.481468,55.481468,-1.1549415,3.3993265,-2.7877294,-78.4595144,24.1163132,147.1405331,147.1405331,6.5801606,-71.2670202,0.1282725,-1.5569566,-1.1246325,12.3784615,4.7002953,4.3822222,4.3822222,4.3822222,4.6976856,5.559064,5.559064,-8.57676,-9.160333,-9.158245,-3.0277512,-2.969825,-2.9712917,-2.9655722,-2.9655722,14.5041552,-0.1748772,-0.1730473,-0.173353,-0.0403745,-0.5630625,-0.099873,-0.5905421,-74.0660722,-92.0204121,-85.7585023,-0.3737751,4.8679611,80.2831719,80.2831719,0.160896,-68.6666508,-69.7993345,-4.4160927,72.0621468,101.6543903,101.6626524,102.2839563,101.6543903,101.6543903,102.2839563,103.428906,103.428906,100.163132,100.163132,116.1183261,116.1228611,116.1228611,116.1183261,116.1183261,110.426859,103.0880208,103.0880208,103.0880208,103.0880208,103.082795,103.0880208,110.426859,103.0880208,-2.2338837,-97.0753365,-75.6848002,-76.6249759,-76.9439866,-76.9425543,-76.454426,-71.0061561,-71.0061561,-72.5267121,57.4975177,-74.1897041,-74.4702391,106.6630339,9.9753867,10.1406851,144.898734,15.5530341,-80.2771253,-80.196161,-83.7382241,-0.6895119,9.1945843,9.213452,-8.3962368,-93.2277285,-89.5253597,-89.5192484,-92.3277375,-46.1827169,48.2944966,3.9521652,19.2405941,3.8720638,3.8720638,3.8720638,3.8634485,103.2839975,7.6131826,112.6245387,11.5678602,7.6131826,-1.1259234,0.1205885,-79.0469134,-75.660102,-97.0768014,98.6587695,98.6587695,-97.1565947,-99.1843676,-99.1865774,-88.2709592,-99.1848257,-75.718906,-99.2004308,-99.2228883,-99.5132832,-84.1111858,-64.32123,36.8162641,4.860199,4.8574926,-1.5559718,14.2578058,14.2578058,14.2578058,14.2461965,16.337579,16.337579,106.6812013,30.9807272,-1.6321518,-96.7004763,123.063603,6.9387031,-115.1431709,-66.084523,166.4031554,151.6410199,-70.9370252,-106.6197812,151.2312675,151.2312675,-1.6146608,151.7041775,-1.6146608,109.2023759,109.1989986,7.2670603,5.8568877,-77.8701325,-97.0768014,98.6587695,115.743425,-86.2379328,115.743425,101.8761751,-1.251164,-1.1951582,-1.1951582,101.8759146,101.8759146,166.4031554,-9.3217062,19.8540518,123.6589219,130.7139356,-97.5358282,-97.5358282,26.1869783,10.7217496,18.1636092,170.5144227,-75.6831329,25.4662935,-5.8462698,-1.2577263,36.8128627,-1.2543668,-1.2543668,11.9576365,11.8774462,113.9034136,113.9034136,13.369838,2.6466933,2.3817936,2.1694163,3.136381,2.1764839,2.344468,7.3067537,101.71629,128.1957229,128.1957229,9.1561041,124.7602683,112.7889174,101.524094,-75.1932137,80.5966093,-34.8861167,2.8988733,2.8988733,103.0880208,101.71629,101.71629,101.71629,12.3868543,71.4867227,99.8963421,120.5981614,121.2412634,121.0651477,121.2412634,121.2412634,120.9866145,125.4841158,122.230829,122.5665883,122.9694704,122.230829,122.230829,120.5981614,120.5981614,121.068937,121.0651477,121.0651477,122.230829,121.2412634,121.2412634,120.9866145,122.230829,122.230829,121.0651477,121.0651477,125.0078219,122.230829,121.0651477,8.4245634,10.3988593,-79.960835,-4.1395777,0.3460479,13.5127759,13.5127759,-0.3593863,16.8783749,-149.608695,-4.3051542,2.1797941,7.0245725,-8.6156998,-1.0935923,13.1298635,28.2314476,-63.1375074,-67.1409025,-67.1409025,74.2661627,113.0927907,113.0927907,101.7241331,101.7241331,113.0927907,101.7241331,101.71629,101.71629,113.0927907,101.71629,101.71629,101.71629,101.71629,101.71629,101.71629,101.71629,101.71629,101.71629,101.71629,103.0880208,101.7241331,101.7241331,101.7241331,101.71629,101.71629,50.8259149,-68.5121318,-72.578105,-68.5121318,153.0120301,153.0120301,-0.9418157,12.0956283,-2.7461995,-56.1987495,55.481468,-90.3107962,-3.8721822,-71.5308537,101.455243,101.380647,101.380647,101.380647,12.5144384,12.1035044,1.1047362,1.260497,80.5761344,127.7651421,-88.184236,138.6066669,14.4478813,14.4478813,-81.0274276,-82.6355589,-82.4138539,101.78189,101.78189,100.3025177,102.2838032,102.2804409,100.3025177,100.4928769,100.3025177,100.3025177,100.3025177,100.3025177,100.3025177,-5.6669251,124.8272348,124.8272348,123.9115758,123.9115758,14.7778386,-76.551167,-8.5449726,-8.5449726,-8.5449726,-8.5435624,120.9896407,-47.6328752,-47.8526875,-46.6490459,-106.6313582,3.138063,117.28059,3.138063,127.3683413,100.3025177,100.3025177,125.6155082,101.4368427,-5.9912307,-5.9912307,10.7171842,-1.4879469,-71.9293882,136.2192717,138.4317,68.3004492,120.7560502,-118.285117,10.4282364,10.4288434,125.1138123,124.6564928,-88.7974042,14.4478813,14.4478813,-1.3960169,153.297815,10.4288434,-88.7974042,-2.7972254,18.864447,-3.918879,-3.9267942,-3.9345293,18.057831,7.7664545,-4.2428535,-4.2428535,9.1724985,103.082795,103.082795,103.082795,103.082795,98.6587695,98.6587695,98.6587695,153.0624859,153.0624859,-75.2930046,-0.5905421,-0.0892748,151.1873473,95.3698263,95.3698263,36.476802,119.8936937,-71.6358479,26.7198659,147.3247697,147.3247697,151.2004942,174.7673188,-9.138705,50.9844875,51.3941057,114.8999212,103.6381827,103.6381827,103.6381827,101.5037611,101.5037611,100.9545838,100.2797452,101.5037611,101.5037611,101.5076962,100.9556447,100.9792862,-83.9294564,-90.0303855,103.0880208,96.197349,-97.1140116,-97.7340567,-97.0537979,-98.5753521,-98.1740447,-98.61972,-97.1140116,-97.7555514,-94.8183176,100.5216138,22.9506205,43.6540423,134.559832,140.1033682,141.1384546,138.4317,139.761989,-79.3956564,-79.1885068,-79.3956564,-79.3956564,1.4506494,6.6884698,13.7934477,18.9710079,140.103652,140.103652,140.103652,9.043774,103.0855782,10.146268,101.7265447,101.139772,101.139772,7.6889006,22.2851138,22.2851138,12.1122117,6.8508709,115.1723921,9.955582,129.256337,-7.3236107,17.6628688,-111.8421021,5.1226018,-0.3593863,-76.5333902,30.4464797,-96.1354327,9.801455,16.4316619,-123.311554,-123.3116935,16.3684626,16.3600504,-8.6834785,-64.7977451,-78.514662,-2.5475301,-87.216141,-59.6297875,-61.399471,5.6657696,5.6657696,-4.1299873,-3.1751292,-4.07654,-4.1299873,-3.3269295,-3.3269295,-1.5614704,-123.0129873,-122.3110325,-80.5394907,16.5771423,115.8199096,115.8199096,115.8199096,-81.2737336,150.790497,151.1873473,-0.1411362,-83.066039,-89.4124875,-87.8819686,28.0302973,150.8759624,9.9708462,138.6010022,96.1360919,11.501365,11.501365,-1.0561667,28.2819343,122.0755747,-0.9015065,8.549552,130.4017155,122.230829,121.2412634,2.357499,2.357499,100.5998319,17.6300093,122.230829,2.8988733,45.0468582,-95.712891,-77.0849394,-84.5110767,-71.422646,-88.1073284,-88.1073284,-76.983186,-77.120073,-77.532563,-84.6810381,-83.0646162,-72.9168159,-92.4348062,-89.163182,-149.8035624,-105.0827372,-82.4189401,-105.2202327,-157.8620891,-77.931242,-92.0450047,-122.1713009,-122.0570674,-122.1712591,-70.6506594,-79.9031855,31.293637,-77.0262443,-117.0399525,-77.0469214,120.5279,-80.239892,-80.4086962,-77.4279616,-89.6145301,-83.3588076,-72.9168159,-89.7540199,-76.9074739,-82.417874,-95.712891,106.9490635,-76.99529,-109.9780728,103.6381827,115.8199096,-175.2195141,-83.2892064,-86.8026551,109.1967488,-73.8957002,106.6880841,105.8004621,105.8004621,109.147903,109.1942297,109.1967488,-173.9825244,11.4470459,-20.267321,-96.584237,103.8298341,-0.4935499,80.6037663,101.11946,-93.3739669,16.4316619,10.2282509,9.1818159,174.7686615,145.0533,144.9531748,101.4474005,16.3684626,102.6331035,105.8004621,106.7929669,105.8004621,105.8004621,105.8004621,106.7012411,105.8004621,105.8004621,105.8004621,105.8004621,105.8478856,105.8341598,105.8341598,105.793772,105.8341598,105.8075314,105.7888361,105.8069419,105.8341598,105.8370733,106.7929669,106.801639,106.7929669,105.7816119,106.7929669,105.8341598,105.815277,106.6805452,105.6813333,106.6296638,-77.4509718,-76.500082,-80.4234167,124.792071,79.1589042,5.0936866,105.807959,106.7032001,78.1272232,-23.7060262,4.8657199,4.8598158,4.394886,4.394886,4.394886,4.8657199,4.3953131,-2.0261505,-75.5999933,-81.8323696,14.5355133,-79.9559358,-80.8171361,4.5937988,5.6657696,5.6657696,5.6657696,5.6657696,123.8902463,99.89736,99.9631219,99.89736,99.89736,99.89736,99.89736,99.89736,99.89736,99.89736,145.0553169,-77.0531111,-77.0531111,168.307938,21.0467135,21.0080857,139.7196485,-122.897613,-117.1542121,-90.2629289,105.8243819,85.0985236,8.43802,-71.2658919,80.0214925,145.9706647,120.71201,-72.6569041,14.5355133,14.5355133,115.8581495,108.0382475,108.0382475,108.2377519,118.5593281,118.905998,150.7933263,-122.4866109,7.6131826,120.960515,106.8029474,5.6765136,-2.452688,-156.3130556,-71.0890559,113.12873,-171.7513551,104.930684,120.984419,121.0176674,6.1343571,105.8341598,120.9844543,178.4500789,102.6331035,112.812144,-122.4017184,100.5798571,100.5798571,104.930271,-88.2031483,106.8015635,-73.8784993,89.5403279,39.6682065,-73.9757778,96.1145565,79.861243,133.775136,138.3830845,126.9547563,-70.6711607,68.8319005,102.832891,101.3431058,121.4691774,102.832891,101.3431058,-77.0424279,31.6798307,100.284188,100.284188,6.1343571,105.8341598,120.9844543,106.8301357,106.8301357,118.9265829,121.0363729,74.3412889,101.6242604,113.921327,54.3773438,106.8650395,100.5399646,105.7468535,105.7659119,121.058199,-77.0518389,30.802498,31.6798307,31.6967438,90.4036037,100.284188,121.4691774,100.284188,104.9282099,121.774017,100.3091,100.284188,-84.0627863,114.3643219,104.9271556,9.4313331,8.6660405,104.9220486,106.8301357,106.8301357,133.1747162,121.0363729,-74.0177906,124.6460898,118.097855,118.097855,118.090169,87.566297,135.1830706,-72.9224158,-72.9223431,140.3493705,131.4706493,131.469104,130.4017155,119.412939,119.412939,35.0063209,132.4528278,117.1290331,135.7005364,139.6506282,139.5881466,126.938572,126.938572,126.938572,100.5321567,116.4073963,101.3431058,102.832891,101.3431058,102.849645,119.7889248,61.4902483,127.3031566,122.0828784,122.0828784,8.8016936,110.359368,120.195509,120.195509,120.123279,122.113923,122.113923,119.7889248,120.027706,120.123279,120.171995,120.123279,113.625328,113.644854,113.39277,113.298883,113.298883,113.39277,8.84598,8.84598,13.3377547,106.7969367,4.912576,122.5665128,-0.1534303,11.482412,88.3339888,80.2741097],[0.05,0.05,0.05,0.05,0.65,0.15,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.2,0.2,0.05,0.35,0.1,0.05,0.05,1.75,0.05,0.05,0.05,0.15,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.15,0.1,0.05,0.05,0.05,0.1,0.05,0.25,0.15,0.05,0.05,0.2,0.6,0.15,0.05,0.45,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.4,0.05,0.2,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.3,0.05,0.4,0.05,0.05,0.05,0.05,0.05,0.35,0.05,0.05,0.1,0.05,0.1,0.05,0.05,0.15,0.05,0.05,0.1,0.05,0.05,0.15,0.1,0.1,0.1,0.15,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.55,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.2,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.1,0.25,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.4,0.1,0.05,0.15,0.05,0.05,0.05,0.1,0.2,0.05,0.05,0.05,0.05,0.05,0.05,0.3,0.05,0.05,0.7,0.8,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.1,0.05,0.65,0.05,0.05,0.15,0.45,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.65,0.05,0.15,0.05,0.05,0.05,0.2,0.05,1.25,0.05,0.1,1.35,0.4,0.05,0.15,0.05,0.05,0.05,0.1,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.1,0.05,0.05,0.05,0.1,0.05,0.05,0.5,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.15,0.1,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.3,0.05,0.25,0.15,0.05,0.1,0.15,0.05,0.05,0.05,0.35,0.05,0.05,0.05,0.05,0.05,0.05,0.85,0.05,0.05,0.05,0.1,0.05,0.2,0.1,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.1,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.2,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.25,1.6,0.05,0.15,0.15,0.05,0.05,0.05,0.05,0.05,0.2,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.25,0.05,0.05,0.05,0.2,0.05,0.45,0.05,0.05,0.15,0.05,0.05,0.1,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.1,0.1,0.3,0.05,3.15,0.05,0.05,2.15,0.05,0.05,0.05,0.05,0.05,0.05,0.2,0.05,0.05,0.05,0.1,0.05,0.15,0.05,0.05,0.05,0.1,0.05,0.9,0.05,0.05,0.2,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.65,0.8,0.05,0.05,0.5,0.65,0.05,0.05,0.05,0.3,0.05,0.05,0.1,0.05,0.15,0.05,0.05,0.1,0.05,0.15,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.25,0.25,0.05,0.15,0.05,0.15,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.1,0.05,0.25,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.15,1,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.3,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.1,0.05,0.2,0.15,0.05,0.05,0.05,0.35,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.1,0.05,0.05,0.1,0.05,0.1,0.15,0.1,0.05,0.05,0.15,0.05,0.05,0.1,3.95,0.05,0.05,0.55,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.2,0.2,0.05,0.05,0.05,1.05,2.75,0.05,0.15,0.05,0.15,0.05,0.3,0.9,0.1,0.05,0.1,0.05,0.15,0.4,0.05,0.3,0.55,0.05,0.1,0.1,0.1,0.05,0.05,0.15,0.1,0.05,0.05,0.05,0.05,0.1,0.15,0.7,0.15,18,0.1,0.05,0.1,0.05,0.15,0.2,0.55,0.25,0.05,0.05,0.05,1.7,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.35,0.05,0.05,0.05,0.05,0.85,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.25,0.05,0.1,0.05,0.25,0.05,0.05,0.15,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.1,0.25,0.05,0.05,0.05,0.05,0.05,0.1,0.15,0.05,0.05,0.05,0.1,0.05,0.1,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.3,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.25,0.05,0.15,0.05,0.05,0.1,0.15,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.5,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.25,0.05,0.1,0.1,0.05,0.15,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.1,0.05,0.05,0.1,0.05,0.3,0.25,0.2,0.35,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.7,0.55,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.2,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.1,0.05,0.05,0.05,0.4,0.1,0.05,0.2,0.1,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.3,0.05,0.05,0.05,0.05,0.05,0.1,0.25,0.2,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.35,0.05,0.1,0.15,0.1,0.05,0.05,0.05,0.05,0.15,0.1,0.05,0.25,0.05,0.1,0.05,0.05,0.05,0.7,0.05,2,0.05,0.05,0.05,0.05,0.1,0.05,0.15,0.2,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.9,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.2,0.05,0.05,0.1,0.3,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.1,0.05,0.25,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,1.15,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.2,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.2,0.2,0.05,0.1,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.1,0.25,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.1,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,1.1,0.05,0.05,0.1,0.05,0.05,0.1,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.45,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.5,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.1,0.05,0.05,0.05,0.35,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.3,0.05,0.05,0.05,0.05,0.15,0.1,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.35,0.4,0.35,0.1,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.1,0.6,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.55,0.1,0.05,0.05,0.1,0.1,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.1,0.3,0.2,0.05,0.05,0.05,0.15,0.05,0.05,0.1,0.15,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.75,0.05,0.15,0.35,0.05,0.3,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.1,0.15,0.05,0.3,0.05,0.05,0.05,0.05,0.1,0.05,0.1,0.05,0.3,0.05,0.05,0.05,0.05,0.25,0.25,0.05,0.1,0.05,0.05,0.1,0.1,0.05,0.1,0.1,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.2,0.1,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.55,0.05,0.05,0.05,0.05,0.1,0.05,0.1,0.25,0.05,0.15,0.05,0.1,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.35,0.1,0.05,0.1,0.2,0.05,0.05,0.1,0.15,0.05,0.85,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.95,0.15,0.05,0.05,0.05,0.05,0.2,0.05,0.05,0.05,0.1,0.05,0.1,0.5,0.05,0.25,0.05,0.05,0.15,0.05,0.05,0.25,0.15,0.1,0.2,0.05,0.05,0.05,0.35,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.15,0.3,0.05,0.1,0.1,1.25,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.65,0.05,0.05,0.05,1.05,0.95,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.2,0.05,0.05,0.05,0.05,0.05,0.05,2.7,0.05,0.05,0.05,0.05,0.15,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.1,0.25,0.05,0.1,0.05,0.1,0.05,0.05,0.25,0.1,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.1,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.2,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.15,0.25,0.2,0.05,0.05,0.1,0.05,0.05,0.05,0.3,0.05,1.3,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.1,0.05,0.1,0.05,0.45,0.2,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.35,0.1,0.05,0.05,0.05,0.15,0.05,0.1,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.35,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.1,0.35,0.1,0.05,0.9,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.3,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.2,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.25,0.35,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.1,0.05,0.25,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.25,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.35,0.05,0.05,0.1,0.1,0.05,0.05,0.05,0.05,0.1,0.35,0.1,0.1,1.05,0.05,0.05,0.05,0.35,0.05,0.05,0.05,0.05,0.2,0.1,1.05,0.05,0.15,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.3,0.05,0.15,0.05,0.05,0.7,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.25,0.05,0.05,0.55,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.25,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.2,0.05,0.05,0.15,0.45,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.4,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,1.2,0.1,2.9,0.05,0.15,0.05,0.05,0.05,0.1,0.05,0.05,0.15,0.9,0.05,0.05,0.05,0.05,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.65,0.1,0.05,0.05,0.2,0.05,0.15,0.05,0.05,0.05,0.1,0.05,0.25,0.1,0.25,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.3,2.95,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.25,0.2,0.05,0.05,0.1,0.05,0.2,0.05,0.15,0.2,0.05,0.05,0.05,0.05,0.3,0.05,0.15,0.25,0.05,0.05,0.05,0.1,0.05,0.05,8.75,0.05,0.05,0.05,1,0.05,0.05,0.05,0.05,0.25,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.15,0.1,0.05,0.1,0.05,4.75,0.05,0.35,0.2,0.4,0.15,0.2,0.05,0.3,0.05,0.15,0.05,0.05,0.05,0.9,0.15,0.05,0.15,0.05,0.05,0.05,1.1,0.05,0.05,0.05,1.1,0.1,0.15,0.05,0.5,0.05,0.3,0.05,0.05,0.05,0.5,0.4,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.35,1,0.05,0.05,0.05,0.05,0.7,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.15,0.05,0.05,0.05,0.05,0.8,0.05,0.05,0.1,0.05,0.05,0.15,0.05,0.1,0.05,0.05,0.05,0.55,0.05,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,2.1,0.1,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,1.35,0.1,0.05,0.05,0.2,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.1,0.05,0.1,0.15,0.05,0.05,0.05,0.05,0.05,0.4,0.4,0.05,0.05,0.05,0.15,0.05,0.1,0.15,0.1,0.05,0.1,0.35,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.1,0.1,0.1,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.35,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.1,0.05,0.1,0.05,0.05,0.2,0.05,1.45,0.05,0.05,0.55,0.15,0.05,0.3,0.05,0.05,0.1,1.1,0.2,0.05,0.05,0.05,0.05,19.65,0.35,0.05,3.15,0.05,0.15,1.2,0.05,0.2,0.2,0.2,0.05,0.05,0.05,0.15,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.1,0.3,0.05,0.15,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.1,0.2,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.1,0.05,0.1,0.05,0.05,0.05,0.15,0.15,0.05,0.1,0.05,0.05,0.05,0.05,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.15,0.05,0.55,0.1,0.25,0.1,0.05,0.05,0.05,0.05,0.05,0.1,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.2,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,1.75,0.05,0.05,0.05,0.05,0.05,0.2,0.1,0.05,0.05,0.1,0.05,0.2,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.5,0.1,0.05,0.05,0.05,0.05,0.2,0.05,0.05,0.05,0.1,0.25,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.2,0.15,0.1,0.05,0.05,0.05,0.1,0.15,0.35,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.25,0.05,0.45,0.3,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.1,0.05,0.05,0.05,0.05,0.2,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,2.25,0.05,0.05,0.05,0.05,0.05,0.2,0.2,0.05,0.8,0.1,0.05,0.05,0.25,0.15,0.4,0.5,0.05,0.05,0.1,0.05,0.25,0.1,0.05,0.4,0.35,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,1.2,0.05,0.05,0.05,0.05,0.05,0.15,0.15,0.2,0.05,0.05,0.1,0.1,0.05,0.1,0.25,0.05,0.05,0.05,1.5,0.1,0.2,0.05,0.25,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.3,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.05,2.7,0.1,0.25,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,4.2,0.05,0.05,0.1,0.05,0.05,0.05,0.05,1.2,0.2,0.05,0.05,0.05,0.7,0.05,0.6,0.05,0.15,0.05,0.05,0.1,0.1,0.05,1.9,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.45,0.05,0.25,0.8,0.05,0.05,0.05,0.05,0.05,0.05,0.1,1.1,0.35,0.05,0.45,0.95,0.35,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.15,0.3,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.1,0.05,0.1,0.1,0.05,0.05,0.05,0.2,0.05,0.65,0.3,0.05,0.25,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.2,0.4,0.7,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.1,0.1,0.05,0.05,0.15,0.05,0.4,0.35,0.05,0.1,0.05,0.15,0.05,0.1,0.05,0.1,0.35,0.1,0.05,0.05,0.05,0.05,0.45,0.05,0.05,0.05,0.05,0.05,0.6,0.05,0.05,0.2,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.4,0.05,0.05,0.1,0.05,0.05,0.2,0.05,0.05,0.05,0.55,0.5,0.05,0.45,0.15,0.05,0.05,0.05,0.05,0.05,0.45,0.45,0.05,0.1,0.05,0.2,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.4,0.05,0.05,0.05,0.15,0.1,0.1,0.05,0.1,0.25,0.05,0.05,0.05,0.05,0.15,0.1,0.15,0.05,0.35,0.05,0.05,2.8,1,0.1,0.05,0.3,0.05,0.1,0.05,0.1,0.2,0.05,0.05,0.05,0.05,0.05,0.05,0.65,0.05,0.05,0.05,0.3,1.25,1.9,0.05,0.05,1.95,0.05,0.1,0.05,0.05,0.05,0.05,0.15,0.7,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.2,27.4,0.1,0.05,0.05,0.15,0.05,0.55,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.45,0.15,0.05,0.05,0.15,0.4,0.05,0.05,0.1,0.05,0.05,0.15,0.1,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.65,0.05,0.05,0.05,0.05,0.05,0.05,0.2,0.05,0.3,0.2,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.2,0.1,0.15,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.45,0.05,0.3,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.15,0.45,0.05,0.05,0.05,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.75,0.05,0.05,0.05,0.25,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.2,0.05,0.05,0.1,0.15,0.05,0.15,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.1,0.3,1.9,1.1,0.05,0.15,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.35,0.05,0.05,0.05,0.1,0.05,0.05,0.35,0.1,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.3,0.05,0.05,0.05,0.05,0.4,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.3,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.15,0.05,0.25,0.05,0.1,0.1,0.2,0.1,0.2,0.1,0.35,0.05,0.05,0.45,0.05,0.1,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.15,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.6,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.15,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.25,0.1,0.05,1.45,0.05,0.15,0.1,0.1,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.35,0.05,0.05,0.15,0.05,0.15,0.05,0.05,0.1,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.25,0.05,0.05,0.05,0.1,0.05,0.05,4.1,0.05,0.05,0.7,0.2,0.05,6.85,0.3,0.35,0.1,0.05,0.5,0.05,0.05,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.2,0.6,0.05,0.55,0.05,0.05,0.1,0.1,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.25,0.05,0.05,0.05,0.05,0.4,0.05,0.05,0.1,0.05,0.05,0.15,0.05,0.05,0.1,0.1,0.05,0.1,0.05,0.8,0.1,0.1,0.8,0.05,0.05,0.05,0.05,0.75,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.15,0.05,0.05,0.1,0.05,0.05,0.2,0.85,0.05,0.1,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.35,0.05,0.2,0.05,0.85,0.7,0.25,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.25,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.15,0.1,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.55,0.1,0.05,0.15,0.05,0.5,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.2,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.2,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.1,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.1,0.05,0.3,0.05,0.2,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.3,0.05,0.05,0.75,0.05,0.25,1.5,0.05,0.5,0.15,0.1,0.05,0.05,0.05,0.05,0.05,0.3,0.05,0.05,0.25,0.15,0.05,0.1,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.15,0.1,0.05,0.05,0.05,0.05,0.05,1.35,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.2,0.05,0.1,0.05,0.05,0.15,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.1,0.05,0.05,0.05,0.05,0.25,0.05,0.1,0.05,0.05,0.05,0.1,0.05,0.05,0.05,7.7,0.15,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.25,0.05,0.1,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.2,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.15,0.05,1.95,0.05,0.05,0.1,0.05,0.05,0.05,0.15,0.05,0.05,0.3,0.05,0.15,0.7,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.25,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.5,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.1,0.25,0.05,0.05,0.6,0.05,0.1,0.4,0.05,0.05,0.05,0.1,0.05,0.05,0.25,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.15,0.2,1.25,0.15,0.1,0.1,0.15,0.05,0.1,0.1,0.1,0.05,0.35,0.05,0.3,0.05,0.05,0.1,0.05,0.05,0.05,0.1,0.05,0.1,0.05,0.05,0.05,0.15,0.05,0.4,0.05,0.05,0.05,0.05,0.05,0.05,0.2,0.1,0.05,0.05,0.25,0.05,0.5,0.05,0.1,0.05,0.45,0.15,0.1,0.1,0.1,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,4.1,0.05,0.1,0.05,0.05,0.05,0.05,0.15,0.6,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.1,0.05,0.05,0.5,0.05,0.05,0.05,0.05,0.05,0.15,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.45,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.2,0.05,0.05,0.1,0.05,0.05,0.05,0.15,0.05,0.1,0.05,0.15,0.05,0.05,0.05,0.05,0.05,1.4,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.25,0.9,0.05,0.05,0.05,0.05,0.05,0.05,0.45,0.05,0.05,0.05,0.1,0.05,0.15,0.05,0.05,0.25,0.05,0.15,0.05,0.05,0.05,0.05,0.55,0.15,0.05,0.1,0.1,0.05,0.35,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.5,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.25,0.15,0.05,0.05,0.05,0.05,0.25,0.05,0.05,0.4,0.05,0.05,0.1,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.1,0.05,0.05,0.05,0.3,0.1,0.05,0.05,0.05,1.55,0.1,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.15,0.1,0.15,0.05,0.1,0.05,0.05,0.1,0.05,0.05,0.15,0.3,0.05,0.95,0.45,0.05,0.05,0.05,0.15,0.1,0.1,0.05,0.1,0.05,0.15,0.15,0.95,0.1,4.05,0.25,0.3,0.05,0.05,0.25,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.1,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.2,0.05,0.05,0.05,0.05,0.05,0.05,0.25,0.25,0.05,0.1,0.05,0.05,0.05,0.2,0.5,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.1,0.15,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.3,0.15,0.1,0.05,0.05,0.2,0.05,0.05,0.05,0.15,0.05,0.05,0.1,0.65,0.05,0.05,0.15,0.05,0.05,0.3,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.3,0.1,0.1,0.1,0.05,0.05,0.9,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.25,0.05,0.05,1.2,0.1,0.35,0.15,0.1,0.05,0.1,0.1,0.1,0.05,0.05,1,0.05,0.1,0.05,0.05,0.05,0.6,0.05,0.05,0.25,0.1,0.05,0.05,0.05,0.05,0.35,0.05,0.05,0.05,0.4,0.05,0.1,0.05,0.2,0.05,0.1,0.05,0.15,0.05,0.15,0.6,0.1,0.05,0.05,0.05,0.2,0.9,0.05,0.05,0.05,0.15,0.05,0.05,0.7,0.15,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.1,0.05,0.2,1,0.05,0.15,0.9,0.05,0.1,0.8,0.05,0.7,0.1,0.35,0.65,0.05,0.25,0.05,0.3,0.1,0.6,0.05,0.05,0.15,0.15,0.05,0.15,0.1,0.05,0.1,0.05,0.85,0.5,0.05,0.05,0.05,0.05,0.2,0.15,0.05,0.05,0.15,0.05,0.05,0.3,0.05,0.05,0.05,0.2,0.2,0.05,0.15,0.2,0.85,0.6,0.05,0.35,0.05,0.3,0.15,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.35,0.05,0.05,0.05,0.1,0.05,0.05,0.25,1,0.1,0.05,0.05,0.05,0.5,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.15,0.15,0.05,0.05,0.05,0.05,0.25,0.25,0.05,0.05,0.2,0.1,0.05,0.05,0.05,0.05,0.15,0.05,0.25,0.05,0.05,0.05,0.05,0.15,0.05,0.1,0.05,0.05,0.1,0.05,0.7,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.2,0.05,0.4,0.25,0.1,0.05,2.45,0.5,0.05,0.05,0.3,0.05,0.25,0.2,0.1,0.05,0.05,0.1,0.05,0.1,0.45,0.1,0.1,0.05,0.1,0.15,0.05,0.05,0.45,0.2,0.05,0.05,0.45,0.05,0.55,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.35,0.1,1.4,0.05,0.1,0.05,0.05,0.05,0.05,0.1,0.5,0.05,0.1,0.25,0.3,0.05,0.05,0.2,0.25,0.45,0.05,0.1,0.05,0.05,0.2,0.05,0.05,0.35,0.05,0.05,0.15,0.1,0.1,0.05,0.35,0.25,0.15,0.05,0.05,0.05,0.05,0.1,0.9,0.05,0.05,1,0.05,0.4,1.4,0.05,2.5,0.05,0.05,0.05,0.25,0.05,0.05,0.45,0.05,0.1,0.15,0.05,0.15,0.05,0.05,0.25,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.15,0.15,0.05,0.25,0.1,0.25,0.05,0.05,0.25,0.05,0.05,0.1,0.05,0.1,0.05,0.3,0.2,0.05,0.05,0.05,0.25,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.1,0.2,0.05,0.2,0.1,0.05,0.05,0.05,0.1,0.1,12.4,0.1,0.05,0.05,0.05,0.05,0.35,0.05,0.05,0.45,3.05,0.15,0.8,0.1,0.05,1.75,0.2,4.1,0.05,0.05,0.05,0.05,0.05,0.05,0.2,0.05,0.2,0.1,0.05,0.05,0.15,0.05,0.1,0.05,0.1,0.05,0.05,1.05,0.1,0.1,0.4,0.6,0.05,0.05,0.25,0.05,0.25,0.05,0.1,0.05,0.05,0.05,0.15,0.05,0.05,0.1,1.05,0.05,0.05,0.05,0.05,0.45,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.4,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.25,0.1,0.05,0.05,0.05,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.35,0.1,0.8,0.1,0.2,0.1,0.2,0.2,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.4,0.05,0.05,0.15,0.05,0.2,0.05,0.3,0.05,0.1,0.05,0.05,0.05,0.1,0.1,0.05,0.2,0.15,0.3,0.2,0.05,0.1,0.15,0.05,0.05,0.8,0.2,0.2,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.2,0.45,0.05,0.05,0.05,0.05,0.2,0.3,0.05,0.2,0.1,0.3,0.05,0.05,0.25,0.05,0.4,0.05,0.75,0.2,0.05,0.8,0.3,0.05,1.25,1.15,0.05,0.05,0.25,0.05,0.05,0.05,1.95,0.05,0.1,0.15,0.05,0.05,0.25,0.55,0.65,0.05,0.2,0.05,0.05,5.5,1.1,0.05,0.05,0.05,0.05,0.15,0.05,0.15,0.05,0.05,0.6,0.05,0.05,0.05,0.05,0.05,0.05,0.65,0.55,0.15,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.35,0.05,0.05,0.8,0.5,10.65,0.1,0.05,2.4,0.05,0.05,0.05,0.05,0.2,0.05,0.1,0.05,0.05,0.05,0.05,0.1,0.05,3.6,0.1,0.25,0.05,0.05,0.05,0.05,0.05,0.5,0.05,0.05,0.05,0.25,0.25,0.4,0.05,0.05,0.05,4.05,0.05,0.1,0.2,0.05,0.1,0.05,0.1,0.05,0.25,4.45,0.55,0.05,0.05,0.05,2.5,0.1,0.55,0.1,0.05,0.05,0.15,0.05,0.05,0.5,0.05,0.05,0.05,0.05,0.05,0.05,0.55,0.05,0.05,0.55,0.05,0.1,0.05,0.05,0.05,0.1,0.1,0.05,0.05,0.05,0.05,0.1,0.2,0.05,0.1,0.25,0.05,0.05,0.05,0.2,0.1,0.05,0.05,0.15,0.05,0.05,0.3,0.05,0.05,0.05,0.1,0.05,0.05,1.65,0.05,0.25,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.25,0.15,0.05,0.05,0.05,1.05,0.05,0.1,0.05,0.75,0.05,0.1,0.05,0.05,0.05,0.05,0.05,1.15,0.35,0.05,0.05,0.2,0.1,0.05,0.05,0.3,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.4,0.05,0.1,0.2,0.05,0.05,0.05,0.1,0.5,0.05,0.05,0.05,0.1,0.05,0.1,0.1,0.05,0.05,0.05,0.05,0.95,0.35,0.1,4.1,0.05,0.05,0.05,0.35,0.15,0.1,0.15,0.1,0.35,0.1,0.15,0.05,0.05,0.05,0.05,0.15,0.1,0.2,0.05,0.05,0.2,0.3,0.15,0.05,0.05,0.05,0.1,1.75,0.1,0.15,0.05,0.05,0.05,0.05,0.15,0.25,0.05,0.05,0.8,0.1,0.05,0.05,0.05,0.2,0.05,0.05,0.65,0.05,0.1,0.1,0.05,0.3,0.05,0.1,0.15,0.05,0.55,0.05,0.05,0.05,0.1,0.7,0.1,0.05,0.05,0.05,0.1,0.25,0.05,0.1,0.2,0.4,0.05,0.05,0.1,0.05,0.2,0.05,0.1,0.1,0.1,0.05,0.05,0.15,0.05,0.05,0.05,0.85,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.15,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.25,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.6,1.6,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.55,0.1,0.05,0.05,0.05,0.05,0.05,0.1,0.3,0.05,2.7,0.1,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.6,0.15,0.05,0.6,0.05,0.1,0.05,0.15,0.15,0.15,0.05,0.05,0.05,0.2,0.05,0.05,0.05,0.2,0.05,0.1,0.4,0.2,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.9,0.05,0.05,0.05,0.45,0.65,0.2,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.1,0.05,0.05,0.05,0.1,0.05,0.35,0.1,0.1,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.6,0.05,0.05,0.05,0.15,0.05,0.05,0.15,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.25,0.1,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.3,0.05,0.05,0.05,0.05,0.2,0.05,0.05,0.1,0.3,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.15,0.05,0.1,0.15,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.45,0.05,0.95,0.1,0.05,0.25,0.2,0.15,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.2,0.55,0.05,0.05,0.05,0.4,0.15,0.05,0.5,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.45,0.05,0.15,0.85,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.05,0.05,0.1,0.1,0.05,0.05,0.45,0.05,0.05,0.05,0.05,0.1,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.05,0.05,0.15,0.05,0.05,0.05],null,null,{&#34;interactive&#34;:true,&#34;className&#34;:&#34;&#34;,&#34;stroke&#34;:true,&#34;color&#34;:&#34;#03F&#34;,&#34;weight&#34;:0.1,&#34;opacity&#34;:0.2,&#34;fill&#34;:true,&#34;fillColor&#34;:&#34;#03F&#34;,&#34;fillOpacity&#34;:0.2},null,null,null,null,null,{&#34;interactive&#34;:false,&#34;permanent&#34;:false,&#34;direction&#34;:&#34;auto&#34;,&#34;opacity&#34;:1,&#34;offset&#34;:[0,0],&#34;textsize&#34;:&#34;10px&#34;,&#34;textOnly&#34;:false,&#34;className&#34;:&#34;&#34;,&#34;sticky&#34;:true},null]}],&#34;limits&#34;:{&#34;lat&#34;:[-53.1633626,69.679788],&#34;lng&#34;:[-175.2195141,178.4500789]}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;As this makes clear it is relatively straightforward to generate quick maps with R and even easier to export the data to tools such as Tableau for publication quality and interactive maps. We will go into mapping in more depth in a future article.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round Up&lt;/h2&gt;
&lt;p&gt;In this article we looked at three R packages for geocoding data on research affiliations from the scientific literature using Web of Science. We focused on the use of the &lt;code&gt;placement&lt;/code&gt; package as it is very easy to use. However, your needs may differ with packages such as &lt;code&gt;ggmap&lt;/code&gt; and &lt;code&gt;googleway&lt;/code&gt; offering different functionality.&lt;/p&gt;
&lt;p&gt;The main take away message is that geocoding using the Google Maps API will normally be an iterative process that may requires multiple passes and adjustments to the data to arrive at accurate results. One things should now also be clear, while the Google Maps API has dramatically improved in its ability to offer geocoded results (including on messy names) these results should not be taken at face value. Instead, and depending on your purpose, multiple iterations may be needed to improve the resolution of the results. In this article we have not gone all the way with this but have hopefully provided enough pointers to allow you to take it further.&lt;/p&gt;
&lt;p&gt;R is a functional programming language meaning that it will be feasible to construct a function that brings together the functions used to process the data in the above steps. We will not go there today, but to round up lets think about some of the elements that we might want to use to address this in a single R function based on the steps that we have taken above.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;import dataset&lt;/li&gt;
&lt;li&gt;address case issues&lt;/li&gt;
&lt;li&gt;separate organisation, city, country&lt;/li&gt;
&lt;li&gt;resolve abbreviations on organisation names&lt;/li&gt;
&lt;li&gt;unite organisation, city and country into a new field&lt;/li&gt;
&lt;li&gt;send the cleaned field to the API and retrieve results&lt;/li&gt;
&lt;li&gt;adjust column names to match&lt;/li&gt;
&lt;li&gt;join results to original&lt;/li&gt;
&lt;li&gt;review the location type&lt;/li&gt;
&lt;li&gt;adjust and rerun as needed to improve rooftop and geometric centre results vs. approximate results&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In many cases it will make sense to choose a threshold based on counts of records before sending the data to the API. For example where dealing with publications (as in this case) it could make sense to exclude records where there is only one record. For example, in our original input table 1,624 entries only had one record. If no one is ever likely to look at data points with only one record you may wish to filter them out and concentrate on the accuracy of geocoding for scores above the threshold.&lt;/p&gt;
&lt;p&gt;We have also seen that while the focus of geocoding is logically on mapping, in reality geocoding services may offer new opportunities for the vexed problem of accurate name cleaning when working with the scientific literature or patent data. We will look at this in more detail in a future article. For the moment, congratulations, you have survived geocoding using the Google Maps API in R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-R-tidyverse&#34;&gt;
&lt;p&gt;Wickham, Hadley. 2017. &lt;em&gt;Tidyverse: Easily Install and Load the ’Tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-stringr&#34;&gt;
&lt;p&gt;———. 2018. &lt;em&gt;Stringr: Simple, Consistent Wrappers for Common String Operations&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=stringr&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=stringr&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-usethis&#34;&gt;
&lt;p&gt;Wickham, Hadley, and Jennifer Bryan. 2018. &lt;em&gt;Usethis: Automate Package and Project Setup&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=usethis&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=usethis&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-dplyr&#34;&gt;
&lt;p&gt;Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2018. &lt;em&gt;Dplyr: A Grammar of Data Manipulation&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=dplyr&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=dplyr&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyr&#34;&gt;
&lt;p&gt;Wickham, Hadley, and Lionel Henry. 2018. &lt;em&gt;Tidyr: Easily Tidy Data with ’Spread()’ and ’Gather()’ Functions&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyr&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=tidyr&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;If you would like to install &lt;code&gt;qdap&lt;/code&gt; but run into problems with &lt;code&gt;rjava&lt;/code&gt; on a Mac the instructions &lt;a href=&#34;https://github.com/MTFA/CohortEx/wiki/Run-rJava-with-RStudio-under-OSX-10.10,-10.11-(El-Capitan)-or-10.12-(Sierra)&#34;&gt;here&lt;/a&gt; can solve installation problems.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Counting Patent First Filings the Tidy Way with R</title>
      <link>/counting-patent-first-filings/</link>
      <pubDate>Fri, 11 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/counting-patent-first-filings/</guid>
      <description>&lt;p&gt;This article provides an in depth introduction to counting patent first filings or priority counts. It is a work in progress chapter for the WIPO Patent Analytics Handbook focusing on advanced patent analytics and builds on the introductory &lt;a href=&#34;https://wipo-analytics.github.io/&#34;&gt;WIPO Manual on Open Source Patent Analytics&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Counting first filings is an important subject for patent statistics because the first filing of a patent application marks the date that is closest to investment in research and development leading to the invention. For this reason it is widely used by economists and statisticians as a proxy indicator for the analysis of trends in science and technology.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; For patent applicants, the first filing, and the priority date that defines it, can be the difference between success and failure. An applicant with priority in the filing of an application for the same invention as their competitor will win in a dispute, and potentially win in multiple countries around the world. Millions, and in some cases hundreds of millions, of dollars may ride on who holds priority over an invention.&lt;/p&gt;
&lt;p&gt;For such an important subject, remarkably little has been written on the practical aspects of counting patents by priority or, as is more intuitive, first filings. The international patent priority system has its origins in the &lt;a href=&#34;http://www.wipo.int/treaties/en/ip/paris/summary_paris.html&#34;&gt;1883 Paris Convention for the Protection of Industrial Property&lt;/a&gt; and basically means that inventors in contracting states to the Convention have a twelve month period from the date of filing their invention to file in other contracting states. During that period any filing in a contracting state will be treated as if it was filed on the same date as the original filing and will enjoy priority over other competing claims to the same invention.&lt;/p&gt;
&lt;p&gt;The best existing guide to understanding priority counts is the 2009 &lt;a href=&#34;http://www.oecd.org/sti/inno/oecdpatentstatisticsmanual.htm&#34;&gt;OECD Patent Statistics Manual&lt;/a&gt;. The &lt;a href=&#34;http://www.oecd.org/sti/inno/oecdpatentstatisticsmanual.htm&#34;&gt;OECD Patent Statistics Manual&lt;/a&gt; is an excellent resource but does not focus on practical demonstration. This article focuses on the practical issues involved in counting by priority.&lt;/p&gt;
&lt;p&gt;By the end of this article you will have an understanding of what priority numbers are and how to use them to generate descriptive patent statistics. You will also be aware of the challenges involved in using priority counts and how to address them.&lt;/p&gt;
&lt;p&gt;We will use R inside RStudio because it provides much more flexibility than tools such as Excel. If you are new to R follow the instructions below on installing R and RStudio. If you are familiar with R but new to patent data, welcome to the challenge. We will use a tidy approach to working with the data and the &lt;code&gt;tidyverse&lt;/code&gt; suite of packages. This allows us to write code that is easy to read and to be transparent about the steps we are taking.&lt;/p&gt;
&lt;div id=&#34;installing-r-and-rstudio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Installing R and RStudio&lt;/h3&gt;
&lt;p&gt;To install R for your operating system choose the appropriate option &lt;a href=&#34;http://cran.rstudio.com/&#34;&gt;here&lt;/a&gt; and install R. Then download the free RStudio desktop for your system &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/#download&#34;&gt;here&lt;/a&gt;. We will be using a suite of packages called the &lt;code&gt;tidyverse&lt;/code&gt; that make it easy to work with data. When you have installed and opened RStudio run this line in your console.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;tidyverse&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next load the library&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ──────────────────────────────────────────────────────── tidyverse 1.2.1 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 2.2.1.9000     ✔ purrr   0.2.4     
## ✔ tibble  1.4.2          ✔ dplyr   0.7.5     
## ✔ tidyr   0.8.0          ✔ stringr 1.3.1     
## ✔ readr   1.1.1          ✔ forcats 0.3.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ─────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will now see a bunch of messages as the packages are loaded. You should now be good to go.&lt;/p&gt;
&lt;p&gt;If you would like to learn more about R then try the excellent &lt;a href=&#34;http://www.wipo.int/treaties/en/ip/paris/summary_paris.html&#34;&gt;DataCamp&lt;/a&gt; online courses or read Garrett Grolemund and Hadley Wickham’s &lt;a href=&#34;http://r4ds.had.co.nz/&#34;&gt;R for Data Science&lt;/a&gt;. Learning to do things in R will make a huge difference to your ability to work with patent and other data and to enjoy the support of the R community in addressing new challenges. There is never a better time to start learning to do things in R than right now.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-drones-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Drones Dataset&lt;/h3&gt;
&lt;p&gt;The drones dataset consists of 18,970 patent publications that contain the words drone or drones somewhere in the text. The dataset is based on a search of the full text patent collections of the United States, the European Patent Office (covering members of the European Patent Convention), the Japan Patent Office, and the Patent Cooperation Treaty (WO) administered by the World Intellectual Property Organisation. The data is based on a search of the commercial &lt;a href=&#34;https://clarivate.com/products/derwent-innovation/&#34;&gt;Clarivate Analytics Derwent Innovations&lt;/a&gt; database which makes it easy to search and download full text data at scales up to 60,000 records at a time.&lt;/p&gt;
&lt;p&gt;We will be working with the drones &lt;code&gt;numbers&lt;/code&gt; set that is confined to just patent numbers in their raw form as downloaded from Derwent Innovation. You can download the dataset &lt;a href=&#34;https://github.com/wipo-analytics/drones_data/blob/master/numbers.csv?raw=true&#34;&gt;here&lt;/a&gt; or import it directly by running the following in your RStudio console.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
numbers &amp;lt;- read_csv(&amp;quot;https://github.com/wipo-analytics/drones_data/blob/master/numbers.csv?raw=true&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-priority-number&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The priority number&lt;/h3&gt;
&lt;p&gt;In the table below we can see that we have a dataset consisting of five columns starting with the priority number.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;numbers&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 18,970 x 5
##    priority_number             application_num… family_first family_number
##    &amp;lt;chr&amp;gt;                       &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;        
##  1 US2016578323F 2016-09-20    US2016578323F 2… &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;         
##  2 US14954632A 2015-11-30      US14954632A 201… &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;         
##  3 US15360203A 2016-11-23      US15360203A 201… &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;         
##  4 US62203383P 2015-08-10; US… US15454805A 201… &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;         
##  5 US62200764P 2015-08-04; US… US15263985A 201… &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;         
##  6 KR201528901A 2015-03-02     US15057264A 201… &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;         
##  7 US15217944A 2016-07-22; US… US15217944A 201… &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;         
##  8 US2008100721P 2008-09-27; … US14808174A 201… &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;         
##  9 FR20142036A 2014-09-12      US14848061A 201… &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;         
## 10 US14970643A 2015-12-16      US14970643A 201… &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;         
## # ... with 18,960 more rows, and 1 more variable: publication_number &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will only be working with the priority number and the application number, but as a general principle it is useful to understand the relationship between these fields which can be simply described as follows.&lt;/p&gt;
&lt;p&gt;priority number &amp;gt; application number &amp;gt; publication number &amp;gt; family members&lt;/p&gt;
&lt;p&gt;We can get a clearer understanding of the relationship between these numbers by looking at the front page of a patent document from our dataset using the popular &lt;a href=&#34;mailto:esp@cenet&#34;&gt;esp@cenet&lt;/a&gt; database. You can access this example &lt;a href=&#34;https://worldwide.espacenet.com/publicationDetails/biblio?FT=D&amp;amp;date=20151210&amp;amp;DB=EPODOC&amp;amp;locale=en_EP&amp;amp;CC=US&amp;amp;NR=2015357831A1&amp;amp;KC=A1&amp;amp;ND=4&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/priority/witricity.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the front page or biblio of a patent record contains a large amount of information. It is typically this information that is used to generate patent statistics. For our purposes we can see that the priority numbers field consists of one or more priority numbers. We might expect that there would only be one priority number as the original filing for the invention. However that is often not the case as we will discuss below.&lt;/p&gt;
&lt;p&gt;From the front page we can see that if we proceed up from the priority numbers the first priority number in the list exactly matches the application number on the front page. This tells us that this is the first filing for this particular application. This application is then published as US2015357831A1.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; We can clearly see the relationship we described above&lt;/p&gt;
&lt;p&gt;&lt;code&gt;priority number &amp;gt; application number &amp;gt; publication number&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In a separate article we will address family members (patent publications that link to one or more of priority numbers) but these family members can be accessed through the INPADOC Patent Family &lt;a href=&#34;https://worldwide.espacenet.com/publicationDetails/inpadocPatentFamily?CC=US&amp;amp;NR=2015357831A1&amp;amp;KC=A1&amp;amp;FT=D&amp;amp;ND=4&amp;amp;date=20151210&amp;amp;DB=EPODOC&amp;amp;locale=en_EP&#34;&gt;here&lt;/a&gt; and will include our target publication.&lt;/p&gt;
&lt;p&gt;For the moment however, let’s make sure we have a good understanding of priority numbers.&lt;/p&gt;
&lt;p&gt;The OECD Manual on Patent Statistics describes the Priority number and the priority date as follows&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Priority number. This is the application or publication number of the priority application, if applicable. It makes it possible to identify the priority country, reconstruct patent families, etc.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Priority date. This is the first date of filing of a patent application, anywhere in the world (usually in the applicant’s domestic patent office), to protect an invention. It is the closest to the date of invention. (OECD 2009: 25)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In practice there are other aspects to the priority number that we need to understand&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-priority-numbers&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multiple Priority Numbers&lt;/h3&gt;
&lt;p&gt;As we can see in the example above this record contains multiple priority numbers when intuitively we might have assumed that one invention = one priority number. The patent system does not actually work like that and there are two reasons for this that we need to understand.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Patent applicants frequently file in more than one country&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In a simple case there is a single priority number and the application number will be identical to that priority number. We can see this in the first example from our dataset. The priority number and the application number are the same.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
numbers[1,] %&amp;gt;% 
  select(priority_number, application_number)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##   priority_number          application_number      
##   &amp;lt;chr&amp;gt;                    &amp;lt;chr&amp;gt;                   
## 1 US2016578323F 2016-09-20 US2016578323F 2016-09-20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, the patent system is an international system. A patent applicant may choose to file for patent rights in up to 152 contracting parties to the &lt;a href=&#34;http://www.wipo.int/pct/en/&#34;&gt;Patent Cooperation Treaty&lt;/a&gt; (although applications in all member states is unusual). As the applications are submitted in multiple countries additional priority numbers will appear in the record. An example of this is provided below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/priority/japan.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case we can see that the earliest priority number is for Japan (JP20150122335 20150617) and the second priority number is for WO (the Patent Cooperation Treaty) and incorporates the country code of the first filing JP into the new priority number (WO2016JP67809 20160615). The Patent Cooperation Treaty is the vehicle through which applicants can submit applications in multiple countries. In this case we can see that the applicant from Japan has chosen to pursue an application in the United States using the Patent Cooperation Treaty (WO) resulting in the application number US201615322008 20160615 filed in 2016 that was then published as &lt;a href=&#34;https://worldwide.espacenet.com/publicationDetails/biblio?FT=D&amp;amp;date=20170518&amp;amp;DB=EPODOC&amp;amp;locale=en_EP&amp;amp;CC=US&amp;amp;NR=2017137104A1&amp;amp;KC=A1&amp;amp;ND=4&#34;&gt;US2017137104A1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;From this we might be tempted to assume that the earliest priority will always appear at the front of the list of priorities. However, this assumption is not safe as we will see below where the first filing appears at the end of the list of priorities. We will be on safer ground by identifying the earliest date in the sequence.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Multiple earlier inventions&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The second situation where we observe multiple priority numbers is cases where multiple earlier applications by the same applicants contribute to the claimed invention. We can see this in the first example above. As an anecdotal observation, the presence of multiple other inventions in the priority field appears to vary by field. Thus, from personal experience, it is uncommon in agriculture, pharmaceuticals and biotechnology but appears to be more common in cases such as computing.&lt;/p&gt;
&lt;p&gt;In the case of &lt;a href=&#34;https://worldwide.espacenet.com/publicationDetails/biblio?FT=D&amp;amp;date=20151210&amp;amp;DB=EPODOC&amp;amp;locale=en_EP&amp;amp;CC=US&amp;amp;NR=2015357831A1&amp;amp;KC=A1&amp;amp;ND=4&#34;&gt;US2015357831A1&lt;/a&gt; above we are dealing with a wireless power system for a drone with an electronic display. Close inspection of the priority numbers reveals that all the earlier priority numbers are filings in the United States with many containing the kind code P standing for Provisional application at the end of the number (e.g. US20090169240P 20090414). The Provisional application system was introduced in the United States in 1995 as a means of harmonizing its system with the wider international system.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; A provisional patent application establishes the priority date for the invention, allowing the applicant to claim priority over other claims, but has no other legal meaning until a full patent application is submitted. Provisional patent applications are not published and are not accessible for analysis.&lt;/p&gt;
&lt;p&gt;To understand this a bit better we can look at the list of actual applications that appear at the start of the sequence of priority numbers.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;US201514815121 20150731 is the priority filing for the new invention of a wireless power system for an electronic display with an impedance matching network with identical application number (US201514815121 20150731) and publication number &lt;a href=&#34;https://worldwide.espacenet.com/publicationDetails/biblio?DB=EPODOC&amp;amp;II=0&amp;amp;ND=3&amp;amp;adjacent=true&amp;amp;locale=en_EP&amp;amp;FT=D&amp;amp;date=20151210&amp;amp;CC=US&amp;amp;NR=2015357831A1&amp;amp;KC=A1&#34;&gt;US2015357831A1&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://worldwide.espacenet.com/publicationDetails/biblio?FT=D&amp;amp;date=20151210&amp;amp;DB=EPODOC&amp;amp;locale=en_EP&amp;amp;CC=US&amp;amp;NR=2015357831A1&amp;amp;KC=A1&amp;amp;ND=4#&#34;&gt;US201113267750 20111006&lt;/a&gt; is for a Wireless Powered Television&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://worldwide.espacenet.com/publicationDetails/biblio?FT=D&amp;amp;date=20151210&amp;amp;DB=EPODOC&amp;amp;locale=en_EP&amp;amp;CC=US&amp;amp;NR=2015357831A1&amp;amp;KC=A1&amp;amp;ND=4#&#34;&gt;US201113232868 20110914&lt;/a&gt; is for a wireless energy distribution system.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As this makes clear, the first filing for this specific invention is the priority number that matches the application number. The other applications in the list could be described as contributing inventions. That is, the specific invention is based on combinations of elements of the other inventions in the list or elaborates on specific aspects of them as a new invention. Note that if we were to start exploring the provisional applications (kind code P) we would be confronted with lists of applications that arise from those provisional applications because provisional applications are not published directly except where they become full applications. You can test that with this example &lt;a href=&#34;https://worldwide.espacenet.com/publicationDetails/biblio?DB=EPODOC&amp;amp;II=0&amp;amp;ND=3&amp;amp;adjacent=true&amp;amp;locale=en_EP&amp;amp;FT=D&amp;amp;date=20151210&amp;amp;CC=US&amp;amp;NR=2015357831A1&amp;amp;KC=A1#&#34;&gt;US20100411490P&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;What does this mean in terms of counting first filings or priority documents? If we choose the &lt;em&gt;earliest filing&lt;/em&gt; in the list &lt;a href=&#34;https://worldwide.espacenet.com/searchResults?PL=true&amp;amp;ND=5&amp;amp;DB=EPODOC&amp;amp;query=PR%3DUS20090169240P&#34;&gt;US20090169240P 20090414&lt;/a&gt; we will be choosing a provisional application for a contributing invention at the base of a set of inventions.&lt;/p&gt;
&lt;p&gt;So, we could:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Choose this filing as the earliest filing bearing in mind it is for a contributing invention rather than the invention itself, or&lt;/li&gt;
&lt;li&gt;We could choose the priority number where the application number is identical as the first filing of the application claiming a wireless power system for an electronic display (US201514815121 20150731).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;!--- in the case of an non provisional application based on a provisinal application the nonprovisional application will enjoy the effective filing date of the provisional application----&gt;&lt;/p&gt;
&lt;p&gt;If our aim is to simply to identify the earliest filing then we would choose option one. This will take us to the earliest in the set of filings but that may be some years before the research and development leading to the specific invention. This is the easiest option because in effect all we have to do is identify the earliest priority date in a set.&lt;/p&gt;
&lt;p&gt;However, if we choose option 2 we will identify the date that is closest to the investment in research and development leading to the specific invention. At first sight this is more attractive in using patent data as an indicator for technology trends but it is significantly more challenging in terms of methodology.&lt;/p&gt;
&lt;p&gt;As this helps to clarify, when dealing with patent counts we are often dealing with &lt;em&gt;many to many relationships&lt;/em&gt;. The application number, as we have just seen, is central to our ability to navigate these relationships and serves as the key field in patent databases such as the EPO World Patent Statistical Database (PATSTAT). The reason for this is that where an application number is identical to a priority number in a set we know it is the first filing. Any other priority numbers either reflect the filing route (national to regional to international) or are for contributing inventions. Any other application numbers or publications are members of the family linked to that first filing. We will address this in more detail in an article on family members.&lt;/p&gt;
&lt;p&gt;In this article our aim will be to map priority filings by identifying the earliest priority dates in the set of priorities linked to an application. In the process we will explore some of the issues that need to be considered when counting patents by priority.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;We now have enough background to begin counting first filings using the priority number. In summary:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The priority number records the first filing of a patent application anywhere in the world;&lt;/li&gt;
&lt;li&gt;Where a priority document is the first filing the application number will be identical to the priority number;&lt;/li&gt;
&lt;li&gt;A single application may contain multiple priority numbers that reflect:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;The history of the filing route of applications with the earliest priority date being the first filing;&lt;/li&gt;
&lt;li&gt;Multiple contributing inventions where the first filing of the target application will be the priority number that is identical to the application number and the earliest priority will be the base of a set of inventions or patent family.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;counting-priority-numbers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Counting Priority Numbers&lt;/h2&gt;
&lt;p&gt;We are now in a position to begin working on counting first filings based on the identification of the earliest priority dates for a set of applications.&lt;/p&gt;
&lt;p&gt;To approach this we will need to start by asking two questions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Does our dataset contain duplicate records? If so we will over count.&lt;/li&gt;
&lt;li&gt;Does our dataset contain missing data? If so, what is the appropriate way to deal with that?&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;dealing-with-duplicates&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Dealing with duplicates&lt;/h3&gt;
&lt;p&gt;We will deal with the question of duplicate data first. This is extremely common with patent data. Duplication arises because a single patent application may be published and republished multiple times (as an application, grant or correction). Duplication is inherent to the system as a global system. Duplication is also prominent when working with patent data because the most common way of retrieving data from a patent database is through publication numbers. Put simply, we can’t read a document that hasn’t been published and so when querying databases it is publications that we see. It is also publications that are downloaded from databases. For some databases, such as Derwent Innovation, there does not appear to be a way to deduplicate the data prior to export and so this has to be handled after export. In other cases, such as the free Lens patent databases or other commercial databases, it is possible to reduce the data onto a single filing. However, the criteria that are applied when deduplicating at source are often unclear - and may vary between databases - so this can impact on your ability to understand the data. If in doubt choose the rawest form and work from there.&lt;/p&gt;
&lt;p&gt;Let’s look at the data again to gain an understanding of the duplication issue. We will arrange the data by the application number for reasons that will become clear in a moment. If you are following this in R then note that &lt;code&gt;arrange()&lt;/code&gt; puts the application number in alphabetical order. &lt;code&gt;select()&lt;/code&gt; using &lt;code&gt;-&lt;/code&gt; drops the columns we don’t want to see right now. Because the duplicates can be difficult to spot I have selected a few rows to make this clear.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;numbers %&amp;gt;% 
  arrange(application_number) %&amp;gt;% 
  select(-family_first, -family_number, -publication_number) %&amp;gt;% 
  .[53:60,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 2
##   priority_number                                      application_number 
##   &amp;lt;chr&amp;gt;                                                &amp;lt;chr&amp;gt;              
## 1 EP1980400905A 1980-06-19; FR197916840A 1979-06-29    EP1980400905A 1980…
## 2 EP1980400905A 1980-06-19; FR197916840A 1979-06-29    EP1980400905A 1980…
## 3 SE19799920A 1979-11-30                               EP1980850181A 1980…
## 4 SE19799920A 1979-11-30                               EP1980850181A 1980…
## 5 JP1979145870A 1979-11-09; JP1979145871A 1979-11-09;… EP1980902127A 1980…
## 6 JP1979145870A 1979-11-09; JP1979145871A 1979-11-09;… EP1980902127A 1980…
## 7 US1979105606A 1979-12-20                             EP1981900248A 1980…
## 8 US1979105606A 1979-12-20                             EP1981900248A 1980…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are two things we need to note in this view.&lt;/p&gt;
&lt;p&gt;First, some of our data is concatenated (joined) with &lt;code&gt;;&lt;/code&gt; as the separator. Second, and more importantly for the moment, we can see that we seem to have duplicate application numbers e.g. EP1980400905A in the first and second rows and then onwards.&lt;/p&gt;
&lt;p&gt;The reason that we have duplicates in the data is that a patent application may be published multiple times (for example as an application and as a grant or with corrections etc.). So, in the data above we can see that EP1980400905A 1980-06-19 has been published as EP22391A1 and EP22391B1 with the two letter &lt;code&gt;kind codes&lt;/code&gt; at the end of the publication number representing the first publication of the application (A1) and kind code B1 representing the first publication of a patent grant.&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;numbers %&amp;gt;% 
  arrange(application_number) %&amp;gt;% 
  select(-family_first, -family_number, - priority_number) %&amp;gt;% 
  .[53:60,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 2
##   application_number       publication_number
##   &amp;lt;chr&amp;gt;                    &amp;lt;chr&amp;gt;             
## 1 EP1980400905A 1980-06-19 EP22391B1         
## 2 EP1980400905A 1980-06-19 EP22391A1         
## 3 EP1980850181A 1980-11-28 EP30219B1         
## 4 EP1980850181A 1980-11-28 EP30219A1         
## 5 EP1980902127A 1980-11-06 EP39740B1         
## 6 EP1980902127A 1980-11-06 EP39740A1         
## 7 EP1981900248A 1980-12-17 EP42004B1         
## 8 EP1981900248A 1980-12-17 EP42004A1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where a document is republished the application number will be duplicated. This means that we will also end up with duplicated priority numbers and we will over count. Removing duplicate records is the key requirement for accurate counts of patent data.&lt;/p&gt;
&lt;p&gt;So, lets remove the duplicate application numbers first. To do that we will use a simple piece of R code from the &lt;code&gt;dplyr&lt;/code&gt; package (you loaded it with the tidyverse) to add a new column that identifies the duplicated records. We will create a new column called duplicated using &lt;code&gt;mutate()&lt;/code&gt; which adds columns. We will add the duplicated column by applying the R function &lt;code&gt;duplicated()&lt;/code&gt; to the application_number column. What this does is to loop over the column and identifies the first instance of the application_number and then duplicates of the application_number. The first instance of the application_number will be marked as FALSE (not duplicated) and the others as TRUE (duplicated). We will then use the &lt;code&gt;filter()&lt;/code&gt; function from &lt;code&gt;dplyr&lt;/code&gt; to limit the data to our non-duplicated numbers &lt;code&gt;== FALSE&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A couple of other things to note is that we will put this in a new table called numbers unique by using the assignment &lt;code&gt;&amp;lt;-&lt;/code&gt; operator. We also use the pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt; operator which takes what it finds on the left hand side and passes it into the right hand side. So, we see that &lt;code&gt;numbers %&amp;gt;% mutate()&lt;/code&gt; passes the numbers table or data.frame from the left hand side into &lt;code&gt;mutate()&lt;/code&gt; to create a new column based on the contents of the call to mutate. It’s simple and logical when you become familiar with it. At the end of this chunk of code we will limit the data to just the priority_number and the application_number which we will be using as a key.&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; The &lt;code&gt;select()&lt;/code&gt; function from &lt;code&gt;dplyr&lt;/code&gt; will only select those columns that we name inside it and will drop the others. These three functions: &lt;code&gt;select()&lt;/code&gt; for columns, &lt;code&gt;filter()&lt;/code&gt; for rows, and &lt;code&gt;mutate()&lt;/code&gt; to add new values connected with the pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt; represent the building blocks for almost everything you need to do with patent data in R. The rest such as &lt;code&gt;duplicated()&lt;/code&gt; help you to perform particular operations and we will go into more detail with that below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;numbers_unique &amp;lt;- numbers %&amp;gt;%
  mutate(duplicated = duplicated(application_number)) %&amp;gt;% 
  filter(duplicated == &amp;quot;FALSE&amp;quot;) %&amp;gt;% 
  select(priority_number, application_number, publication_number)

nrow(numbers_unique) # count the rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 15776&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This reduces our original 18,970 records to 15,776 records. We now want to take a look at our data to check for missingness.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;missing-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Missing Data&lt;/h3&gt;
&lt;p&gt;We will be counting and graphing the priority numbers. So we will want to check that all of our records have a priority number. We will also be using dates to graph the data and it will be a very good idea to check the dates at this stage. The reason for this is that strange things can happen with patent dates and this is often linked to missingness in the data as we will see in a moment.&lt;/p&gt;
&lt;p&gt;In R missing data is represented by NA for Not Available. Working with NA data can be awkward and a source of considerable frustration because NA is not a value, it is the absence of a value. We can solve this by adding a column using &lt;code&gt;mutate()&lt;/code&gt; that will test the priority number field for NA values &lt;code&gt;is.na()&lt;/code&gt;. We will then apply a filter to allow us to see the top results where the value for &lt;code&gt;is.na()&lt;/code&gt; is TRUE. To see all the data add &lt;code&gt;%&amp;gt;% View()&lt;/code&gt; to the end.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;numbers_unique %&amp;gt;% 
  mutate(missing_priority = is.na(priority_number)) %&amp;gt;% 
  filter(missing_priority == &amp;quot;TRUE&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 96 x 4
##    priority_number application_number    publication_num… missing_priority
##    &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;            &amp;lt;lgl&amp;gt;           
##  1 &amp;lt;NA&amp;gt;            USD502486A 0001-01-01 US502486A        TRUE            
##  2 &amp;lt;NA&amp;gt;            USD500197A 0001-01-01 US500197A        TRUE            
##  3 &amp;lt;NA&amp;gt;            USD499490A 0001-01-01 US499490A        TRUE            
##  4 &amp;lt;NA&amp;gt;            USD497518A 0001-01-01 US497518A        TRUE            
##  5 &amp;lt;NA&amp;gt;            USD565353A 0001-01-01 US565353A        TRUE            
##  6 &amp;lt;NA&amp;gt;            USD474115A 0001-01-01 US474115A        TRUE            
##  7 &amp;lt;NA&amp;gt;            USD459287A 0001-01-01 US459287A        TRUE            
##  8 &amp;lt;NA&amp;gt;            USD540479A 0001-01-01 US540479A        TRUE            
##  9 &amp;lt;NA&amp;gt;            USD522772A 0001-01-01 US522772A        TRUE            
## 10 &amp;lt;NA&amp;gt;            USD593712A 0001-01-01 US593712A        TRUE            
## # ... with 86 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first thing we notice about this data is that the dates for the records are 0001-01-01. This type of device (along with 999999) is often used to denote the absence of a date. In this case if we look up some of these cases we will discover that they are very old records. For example &lt;a href=&#34;https://worldwide.espacenet.com/publicationDetails/biblio?DB=EPODOC&amp;amp;II=0&amp;amp;ND=3&amp;amp;adjacent=true&amp;amp;locale=en_EP&amp;amp;FT=D&amp;amp;date=18850728&amp;amp;CC=US&amp;amp;NR=322982A&amp;amp;KC=A&#34;&gt;US322982A&lt;/a&gt; dates to 1885. The Paris Convention did not enter into force in the United States until May 1887 and it is unclear when exactly the USPTO started using the system, so it is not surprising that these documents lack priority numbers.&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; If we continue to keep these records we will see an artificial spike of activity somewhere at the start of our graph. In this case we can safely drop these records using the handy &lt;code&gt;drop_na()&lt;/code&gt; function from &lt;code&gt;dplyr&lt;/code&gt;. We will simply overwrite the existing table and specify the priority number as the column where we will drop the rows with NA values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;numbers_unique &amp;lt;- numbers_unique %&amp;gt;% 
  drop_na(priority_number) %&amp;gt;% 
  select(-publication_number)

nrow(numbers_unique)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 15680&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have now reduced our dataset to 15,680 unique application numbers. Note that you may want to also use this type of &lt;code&gt;test, investigate, decide&lt;/code&gt; approach with other fields but it is always a good idea to note down the decisions that you make when doing so, otherwise what Hadley Wickham has called “future you” will have no idea and your audience will also have no clue.&lt;/p&gt;
&lt;p&gt;When working with this kind of data it is useful to create a reference number or even a full table that allows you to work out whether any operations you run afterwards are working correctly. In this case we now know that we have 15,680 application numbers. In the next section we will be working out the earliest priority dates for each of these documents. We therefore need to ensure that we end up with 15,680 application numbers. We will create a reference number called target from the number of rows &lt;code&gt;nrow()&lt;/code&gt; in the dataset before we go any further. This can help us work out what is going wrong if we end up with different numbers at the end. For more complex cases try creating a copy of the full table that you can use to work out what is getting lost or not counting correctly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;target &amp;lt;- nrow(numbers_unique)
target&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 15680&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now know that we have no missing priority dates so we can proceed to wrangling or processing the priority numbers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wrangling-the-priority-numbers&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Wrangling the Priority Numbers&lt;/h3&gt;
&lt;p&gt;We can’t count concatenated data properly, so our next step is to separate out the concatenated priority numbers into individual rows. We will also want to extract the dates from the priority numbers that we will use later on to create a graph. We will do this in one go. In the first step we will use &lt;code&gt;separate_rows()&lt;/code&gt; from &lt;code&gt;tidyr&lt;/code&gt; to break the priority numbers onto individual rows using &lt;code&gt;;&lt;/code&gt; as the separator. We then use &lt;code&gt;separate()&lt;/code&gt; to separate out the priority number and the date component. This will create two new columns called priority and priority_date. We will then apply two functions to these columns using &lt;code&gt;mutate()&lt;/code&gt;. The first will convert the priority date to date format in R. The second will trim any white space that appears at the front or end of the priority number field from the earlier separation. Trimming white space is an extremely important step. For example &lt;code&gt;US1234&lt;/code&gt; and the same number with a white space at the front or rear _US1234, where &lt;code&gt;_&lt;/code&gt; stands for the space, will be treated as a distinct number and will not count correctly. Trimming white space is a fundamental task when counting patent data and the single most common reason that your counts will not be correct at the end of all your hard work!&lt;/p&gt;
&lt;p&gt;As a final step in data preparation we will add some additional features. We will identify the US provisional applications and we will count the number of priorities associated with an application. We will also extract the two letter country codes at the beginning of the priority and application number fields as they may assist us later and will be used in counts. Note that the count of priority numbers in &lt;code&gt;n&lt;/code&gt; reveals the total number of priorities associated with an application number. We will not use all of these fields for this type of count but they are useful to assist with understanding the data as we move along.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;numbers_unique &amp;lt;- numbers_unique %&amp;gt;% 
  separate_rows(priority_number, sep = &amp;quot;;&amp;quot;) %&amp;gt;% 
  mutate(priority_number = str_trim(priority_number, side = &amp;quot;both&amp;quot;)) %&amp;gt;%
  separate(priority_number, into = c(&amp;quot;priority&amp;quot;, &amp;quot;priority_date&amp;quot;), sep = &amp;quot; &amp;quot;, remove = FALSE) %&amp;gt;% 
  mutate(priority_date = lubridate::ymd(priority_date)) %&amp;gt;% 
  mutate(priority = str_trim(priority, side = &amp;quot;both&amp;quot;)) %&amp;gt;%
  mutate(priority_number = str_trim(priority_number, side = &amp;quot;both&amp;quot;)) %&amp;gt;% 
  mutate(provisional = str_detect(.$priority_number, &amp;quot;[[:digit:]]P &amp;quot;)) %&amp;gt;%
  group_by(application_number) %&amp;gt;%
  mutate(priority_count = seq_along(1)) %&amp;gt;%
  add_tally(wt = priority_count) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  mutate(priority_country = str_sub(.$priority_number, 1,2)) %&amp;gt;% 
  mutate(application_country = str_sub(.$application_number, 1,2)) %&amp;gt;%
  select(-priority_count, -priority) # drop temporary count and unused column

numbers_unique&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 68,361 x 7
##    priority_number     priority_date application_number  provisional     n
##    &amp;lt;chr&amp;gt;               &amp;lt;date&amp;gt;        &amp;lt;chr&amp;gt;               &amp;lt;lgl&amp;gt;       &amp;lt;int&amp;gt;
##  1 US2016578323F 2016… 2016-09-20    US2016578323F 2016… FALSE           1
##  2 US14954632A 2015-1… 2015-11-30    US14954632A 2015-1… FALSE           1
##  3 US15360203A 2016-1… 2016-11-23    US15360203A 2016-1… FALSE           1
##  4 US62203383P 2015-0… 2015-08-10    US15454805A 2017-0… TRUE            2
##  5 US62314047P 2016-0… 2016-03-28    US15454805A 2017-0… TRUE            2
##  6 US62200764P 2015-0… 2015-08-04    US15263985A 2016-0… TRUE            2
##  7 US62314042P 2016-0… 2016-03-28    US15263985A 2016-0… TRUE            2
##  8 KR201528901A 2015-… 2015-03-02    US15057264A 2016-0… FALSE           1
##  9 US15217944A 2016-0… 2016-07-22    US15217944A 2016-0… FALSE           3
## 10 US2015196885P 2015… 2015-07-24    US15217944A 2016-0… TRUE            3
## # ... with 68,351 more rows, and 2 more variables: priority_country &amp;lt;chr&amp;gt;,
## #   application_country &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s quickly review what we just did in plain language. We separated each priority number onto its own row using the semicolon as the separator then we split off the priority number and the date and reformatted the date before finally trimming the white space around the numbers in the priority column. A couple of points to note here: in the call to separate we specified the separator or &lt;code&gt;sep&lt;/code&gt; as a space, we opted to keep the original column with &lt;code&gt;remove = FALSE&lt;/code&gt; (the default is true and removes the column). We then added a simple count because we know the number of application numbers are duplicated to the number of priorities and then grouped the applications to add a count of the total priorities per application with &lt;code&gt;add_tally&lt;/code&gt;. We ungrouped the table and then extracted the priority country and application country. Ungrouping is important, but hard to remember, because if we do not ungroup the data then any calculation we apply will be applied by group. This will normally cause unexpected results or the calculation simply won’t work.&lt;/p&gt;
&lt;p&gt;When preparing data in this way one of the signs that there are unresolved issues with your data is that you will receive warnings about missing or extra pieces of data when you use &lt;code&gt;separate()&lt;/code&gt;. If you see these messages go back and inspect your data. It can mean that there are NA values in the column you are separating or it can mean that you have extra spaces (so there will be too many pieces) or something else is present in the data. Issues with white space are common culprits with patent data (following separation) and this is one of the reasons that there are two calls to trim white space with &lt;code&gt;str_trim()&lt;/code&gt; as a security blanket to avoid later problems.&lt;/p&gt;
&lt;p&gt;To make this clearer lets just try and run separate on our original concatenated data using the space as the separator.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;numbers %&amp;gt;% 
  separate(priority_number, into = c(&amp;quot;one&amp;quot;, &amp;quot;two&amp;quot;), sep = &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Expected 2 pieces. Additional pieces discarded in 10937 rows
## [4, 5, 7, 8, 11, 21, 22, 32, 33, 34, 37, 39, 40, 41, 42, 43, 44, 45, 48,
## 49, ...].&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 18,970 x 6
##    one         two        application_number    family_first family_number
##    &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;        
##  1 US20165783… 2016-09-20 US2016578323F 2016-0… &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;         
##  2 US14954632A 2015-11-30 US14954632A 2015-11-… &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;         
##  3 US15360203A 2016-11-23 US15360203A 2016-11-… &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;         
##  4 US62203383P 2015-08-1… US15454805A 2017-03-… &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;         
##  5 US62200764P 2015-08-0… US15263985A 2016-09-… &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;         
##  6 KR20152890… 2015-03-02 US15057264A 2016-03-… &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;         
##  7 US15217944A 2016-07-2… US15217944A 2016-07-… &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;         
##  8 US20081007… 2008-09-2… US14808174A 2015-07-… &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;         
##  9 FR20142036A 2014-09-12 US14848061A 2015-09-… &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;         
## 10 US14970643A 2015-12-16 US14970643A 2015-12-… &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;         
## # ... with 18,960 more rows, and 1 more variable: publication_number &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We immediately get a warning about extra pieces in over 10,000 rows signifying that we need to go back and pay more attention to our data. In other cases you will not always be concerned about this, although it is an extremely good idea to be clear about why you are not concerned, and you can deal with extra data by specifying &lt;code&gt;extra = &amp;quot;merge&amp;quot;&lt;/code&gt;. For fun let’s try that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;numbers %&amp;gt;% 
  separate(priority_number, into = c(&amp;quot;one&amp;quot;, &amp;quot;two&amp;quot;), sep = &amp;quot; &amp;quot;, extra = &amp;quot;merge&amp;quot;) %&amp;gt;% 
  select(one, two)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 18,970 x 2
##    one           two                                                      
##    &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;                                                    
##  1 US2016578323F 2016-09-20                                               
##  2 US14954632A   2015-11-30                                               
##  3 US15360203A   2016-11-23                                               
##  4 US62203383P   2015-08-10; US62314047P 2016-03-28                       
##  5 US62200764P   2015-08-04; US62314042P 2016-03-28                       
##  6 KR201528901A  2015-03-02                                               
##  7 US15217944A   2016-07-22; US2015196885P 2015-07-24; US62196885P 2015-0…
##  8 US2008100721P 2008-09-27; US2008108743P 2008-10-27; US2008121159P 2008…
##  9 FR20142036A   2014-09-12                                               
## 10 US14970643A   2015-12-16                                               
## # ... with 18,960 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we would expect from using the space as a separator, the function is showing us the number in the first column and the date in the second but is then tacking on the rest of the data in cases with multiple priority numbers. Warnings and arguments such as &lt;code&gt;extra = &amp;quot;merge&amp;quot;&lt;/code&gt; can help you get to grips with the issues in your data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;identifying-the-earliest-priority-date&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Identifying the earliest priority date&lt;/h3&gt;
&lt;p&gt;In the discussion of the options identified above we noted that we could:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Identify the earliest priority document&lt;/li&gt;
&lt;li&gt;Identify the priority that is closest to the specific invention&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here we will focus on simply identifying the earliest priority. We can do this in a straight forward way by grouping our application numbers and then using the &lt;code&gt;rank()&lt;/code&gt; function inside a call to &lt;code&gt;mutate()&lt;/code&gt; to rank the dates from 1 to x. A key point here is that the default ranking method for the rank function is actually average. We therefore need to specify &lt;code&gt;ties.method = &amp;quot;first&amp;quot;&lt;/code&gt; to get what we want. We then ungroup our table and filter to the earliest priority date using &lt;code&gt;filing_order == 1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;earliest &amp;lt;- numbers_unique %&amp;gt;% 
  group_by(application_number) %&amp;gt;% 
  mutate(filing_order = rank(priority_date, ties.method = &amp;quot;first&amp;quot;)) %&amp;gt;%
  ungroup() %&amp;gt;% 
  filter(filing_order == 1)

earliest&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 15,680 x 8
##    priority_number     priority_date application_number  provisional     n
##    &amp;lt;chr&amp;gt;               &amp;lt;date&amp;gt;        &amp;lt;chr&amp;gt;               &amp;lt;lgl&amp;gt;       &amp;lt;int&amp;gt;
##  1 US2016578323F 2016… 2016-09-20    US2016578323F 2016… FALSE           1
##  2 US14954632A 2015-1… 2015-11-30    US14954632A 2015-1… FALSE           1
##  3 US15360203A 2016-1… 2016-11-23    US15360203A 2016-1… FALSE           1
##  4 US62203383P 2015-0… 2015-08-10    US15454805A 2017-0… TRUE            2
##  5 US62200764P 2015-0… 2015-08-04    US15263985A 2016-0… TRUE            2
##  6 KR201528901A 2015-… 2015-03-02    US15057264A 2016-0… FALSE           1
##  7 US2015196885P 2015… 2015-07-24    US15217944A 2016-0… TRUE            3
##  8 US2008100721P 2008… 2008-09-27    US14808174A 2015-0… TRUE           22
##  9 FR20142036A 2014-0… 2014-09-12    US14848061A 2015-0… FALSE           1
## 10 US14970643A 2015-1… 2015-12-16    US14970643A 2015-1… FALSE           1
## # ... with 15,670 more rows, and 3 more variables: priority_country &amp;lt;chr&amp;gt;,
## #   application_country &amp;lt;chr&amp;gt;, filing_order &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a data frame that identifies the earliest priority numbers in a set. The 15,680 records corresponds match our target of 15,680 application numbers and so all is good.&lt;/p&gt;
&lt;p&gt;The final step with this data is to remember that this dataset is based on unique &lt;em&gt;application numbers&lt;/em&gt; and not unique priority numbers. In practice, some of the application numbers in our set will share priority numbers with other applications and will be follow on filings. We therefore need to identify duplicates in the priority numbers and deduplicate to unique priority numbers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;earliest_unique &amp;lt;- earliest %&amp;gt;% 
  mutate(duplicate_priority = duplicated(.$priority_number)) %&amp;gt;% 
  filter(duplicate_priority == &amp;quot;FALSE&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This reduces our dataset to a total of 9,366 priority numbers. That is, these priority numbers are the earliest filings giving rise to the 15,680 applications in the drones dataset.&lt;/p&gt;
&lt;p&gt;By pursuing this option we have arrived at the absolute earliest dateline in this dataset on drones through a process of deduplication. However, as the numbers suggest we have also taken out a lot of potentially useful information. At this point it is important to bear in mind that this type of calculation can only be used to graph baseline first filings. We will look at this in further depth in a follow on article.&lt;/p&gt;
&lt;p&gt;Let’s quickly graph this data. Here we are using the popular R graphing package ggplot2 to draw quick graphs of the data. To learn more about using &lt;code&gt;ggplot2&lt;/code&gt; try the excellent &lt;a href=&#34;http://www.cookbook-r.com/Graphs/&#34;&gt;R Graphics Cookbook by Winston Chang&lt;/a&gt; which is available in open access form online. A step by step walk through on using ggplot2 to visualise patent data is available in this &lt;a href=&#34;https://www.pauloldham.net/graphing-patent-data-with-ggplot2-part2/&#34;&gt;article&lt;/a&gt;. If you prefer using Excel or Tableau then write the file to a .csv and then open it in your tool of choice. You can do this simply with the following line of code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;readr::write_csv(earliest_unique, &amp;quot;earliest_unique.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ggplot2 is quite a lot more involved than working with Tableau, Excel or other tools but provides a powerful way to control graphing. Let’s take a quick look at the data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-11-counting-patent-first-filings_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that graphs of priority data display a characteristic data cliff the closer that we move towards the present. This reflects the fact that patent applications are normally published at least 24 months after they were originally filed. This data cliff can easily mislead an audience into believing that interest in a technology has suddenly collapsed when in reality we are missing or only have partial data for the period. It is therefore important to pull the year range back to accommodate this. Depending on your data it is sensible to pull back the year by at least two years and possibly three years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;earliest_unique %&amp;gt;% 
  select(-n) %&amp;gt;% 
  mutate(year = lubridate::year(priority_date)) %&amp;gt;% 
  filter(year &amp;gt;= 1990 &amp;amp; year &amp;lt;= 2015) %&amp;gt;% 
  group_by(year) %&amp;gt;%
  tally() %&amp;gt;%  
  ggplot(., aes(x = year, y = n)) +
  geom_line() +
  labs(title = &amp;quot;Trends in First Filings of Patent Applications for Drone Technology&amp;quot;, x = &amp;quot;priority year&amp;quot;, y = &amp;quot;first filings&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-11-counting-patent-first-filings_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note the speed bump in the data around 2008 that is likely to reflect the impact of the financial crisis with filings relating to drone technology before accelerating rapidly in recent years.&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this dataset we can also gain an insight into the countries driving this trend by ranking them in a bar graph for the same period.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggthemes)
earliest_unique %&amp;gt;%
  select(-n) %&amp;gt;% 
  filter(priority_date &amp;gt;= &amp;quot;1990-01-01&amp;quot; &amp;amp; priority_date &amp;lt;= &amp;quot;2017-12-01&amp;quot;) %&amp;gt;% 
  group_by(priority_country) %&amp;gt;% 
  tally(sort = TRUE) %&amp;gt;% 
  filter(n &amp;gt; 100) %&amp;gt;% 
  ggplot(aes(x = reorder(priority_country, n), y = n, fill = priority_country)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;) +
  coord_flip() +
  labs(title = &amp;quot;First Filings by Priority Country&amp;quot;, x = &amp;quot;Priority Country&amp;quot;, y = &amp;quot;First Filings&amp;quot;) +
  geom_text(aes(y = n, label = n), size = 3, hjust = -0.1) +
  theme_igray() +
  scale_fill_tableau(&amp;quot;tableau20&amp;quot;) +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-11-counting-patent-first-filings_files/figure-html/echo-1.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note here that the United States emerges first in the top 5 followed by Japan, France, Korea and the Patent Cooperation Treaty (WO). It is important to bear in mind here that WO records will typically be filed through national offices, although no priority number will be present, as we can see in the references to country codes inside the WO priority documents.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;earliest_unique %&amp;gt;% 
  filter(priority_country == &amp;quot;WO&amp;quot;) %&amp;gt;% 
  select(priority_number)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 286 x 1
##    priority_number          
##    &amp;lt;chr&amp;gt;                    
##  1 WO2016US65141A 2016-12-06
##  2 WO2015CN79094A 2015-05-15
##  3 WO2014CN86739A 2014-09-17
##  4 WO2015EP76803A 2015-11-17
##  5 WO2014EP72175A 2014-10-16
##  6 WO2013US65291A 2013-10-16
##  7 WO2014PL50044A 2014-07-24
##  8 WO2014US21626A 2014-03-07
##  9 WO2013US46840A 2013-06-20
## 10 WO2012US69292A 2012-12-12
## # ... with 276 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can summarise this data by extracting the country codes in the middle of the WO numbers. This suggests that the country where the WO application was submitted was the US followed by China (CN) and so on. The reference to the IB in these numbers are for so called PCT direct filings that are filed directly with WIPO as the International Bureau (IB) for the Patent Cooperation Treaty.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;earliest_unique %&amp;gt;% 
  filter(priority_country == &amp;quot;WO&amp;quot;) %&amp;gt;% 
  select(priority_number) %&amp;gt;% 
  mutate(wo_source = str_sub(.$priority_number, 7,8)) %&amp;gt;% 
  count(wo_source, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 25 x 2
##    wo_source     n
##    &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;
##  1 US           71
##  2 CN           46
##  3 EP           43
##  4 JP           43
##  5 IB           19
##  6 KR           14
##  7 SE           13
##  8 RU            7
##  9 FR            5
## 10 PL            5
## # ... with 15 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As such, for a fuller count we might consider reallocating these priority numbers to their respective national country offices.&lt;/p&gt;
&lt;p&gt;Note that we cannot go much further with this data to chart application countries accurately because we have deduplicated the priority numbers that would provide access to the application country data. A superior approach would be to create a temporary field for the unique priorities that allows the linked application countries to be viewed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bringing-together-the-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bringing together the code&lt;/h3&gt;
&lt;p&gt;To finish off this discussion let’s briefly summarise the code required to reduce the dataset to the early priority filings. Here we will present the code in one go following some pruning to remove extra elements that we did not use to generate this calculation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;earliest_priority &amp;lt;- numbers %&amp;gt;%
  mutate(duplicated = duplicated(application_number)) %&amp;gt;% 
  filter(duplicated == &amp;quot;FALSE&amp;quot;) %&amp;gt;% 
  select(priority_number, application_number, publication_number) %&amp;gt;% 
  drop_na(priority_number) %&amp;gt;% 
  separate_rows(priority_number, sep = &amp;quot;;&amp;quot;) %&amp;gt;% 
  mutate(priority_number = str_trim(priority_number, side = &amp;quot;both&amp;quot;)) %&amp;gt;%
  separate(priority_number, into = c(&amp;quot;priority&amp;quot;, &amp;quot;priority_date&amp;quot;), sep = &amp;quot; &amp;quot;, remove = FALSE) %&amp;gt;% 
  mutate(priority_date = lubridate::ymd(priority_date)) %&amp;gt;% 
  mutate(priority_number = str_trim(priority_number, side = &amp;quot;both&amp;quot;)) %&amp;gt;% 
  mutate(priority_country = str_sub(.$priority_number, 1,2)) %&amp;gt;% 
  group_by(application_number) %&amp;gt;% 
  mutate(filing_order = rank(priority_date, ties.method = &amp;quot;first&amp;quot;)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  filter(filing_order == 1) %&amp;gt;% 
  mutate(duplicate_priority = duplicated(.$priority_number)) %&amp;gt;% 
  filter(duplicate_priority == &amp;quot;FALSE&amp;quot;) %&amp;gt;% 
  select(-priority)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Calculating the earliest priority from the moment of import involved 18 lines of code focusing on using &lt;code&gt;mutate(), filter(), select(), drop_na(), separate_rows(), group_by()&lt;/code&gt;, and &lt;code&gt;ungroup()&lt;/code&gt;. Inside the &lt;code&gt;mutate()&lt;/code&gt; function we created new columns to test for duplicates using &lt;code&gt;duplicated()&lt;/code&gt;, we trimmed white space with &lt;code&gt;str_trim()&lt;/code&gt;, extracted data with &lt;code&gt;str_sub()&lt;/code&gt; and ranked data with &lt;code&gt;rank()&lt;/code&gt;. As this makes clear R functions from the tidyverse provide building blocks that can be chained together in an easy to read way to transform data into a desired result. For this reason we advocate a tidy approach to patent analytics with R.&lt;/p&gt;
&lt;p&gt;One of the most important features of R as a functional programming language is that we can wrap this code (a collection of instructions to functions) into a single function. We will call it extract priority. This code basically reproduces that above but with some additional decoration to address something called &lt;a href=&#34;https://cran.r-project.org/web/packages/dplyr/vignettes/programming.html&#34;&gt;tidy evaluation&lt;/a&gt; in R. Tidy evaluation is intellectually challenging and will not be addressed here.&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extract_priority &amp;lt;- function(data = NULL, priority_number = NULL, key = NULL){
  x &amp;lt;- data %&amp;gt;%
    select(!!priority_number, !!key) %&amp;gt;%
    mutate(duplicated = duplicated(.[[!!key]])) %&amp;gt;%
    filter(duplicated == FALSE) %&amp;gt;%
    drop_na(!!priority_number) %&amp;gt;%
    separate_rows(!!priority_number, sep = &amp;quot;;&amp;quot;) %&amp;gt;%
    mutate(!!priority_number := str_trim(.[[!!priority_number]], side = &amp;quot;both&amp;quot;)) %&amp;gt;%
    separate(!!priority_number, into = c(&amp;quot;priority&amp;quot;, &amp;quot;priority_date&amp;quot;), sep = &amp;quot; &amp;quot;, remove = FALSE) %&amp;gt;%
    mutate(priority_date = lubridate::ymd(priority_date)) %&amp;gt;%
    mutate(!!key := str_trim(.[[!!key]], side = &amp;quot;both&amp;quot;)) %&amp;gt;%
    mutate(priority_country = str_sub(.[[!!priority_number]], 1,2)) %&amp;gt;% 
    group_by(!!!rlang::syms(key)) %&amp;gt;% 
    mutate(filing_order = rank(priority_date, ties.method = &amp;quot;first&amp;quot;)) %&amp;gt;% 
    ungroup() %&amp;gt;% 
    filter(filing_order == 1) %&amp;gt;%
    mutate(duplicate_priority = duplicated(.[[!!priority_number]])) %&amp;gt;% 
    filter(duplicate_priority == &amp;quot;FALSE&amp;quot;) %&amp;gt;% 
    select(-priority, -duplicated, -filing_order, -duplicate_priority)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function takes three arguments. Data is a dataset, the priority number is the field that contains the raw priority number data and the key is the field that is used for grouping (assumed to be the application number).&lt;/p&gt;
&lt;p&gt;We can test this as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results &amp;lt;- extract_priority(data = numbers, priority_number = &amp;quot;priority_number&amp;quot;, key = &amp;quot;application_number&amp;quot;)
results&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9,366 x 4
##    priority_number      priority_date application_number  priority_country
##    &amp;lt;chr&amp;gt;                &amp;lt;date&amp;gt;        &amp;lt;chr&amp;gt;               &amp;lt;chr&amp;gt;           
##  1 US2016578323F 2016-… 2016-09-20    US2016578323F 2016… US              
##  2 US14954632A 2015-11… 2015-11-30    US14954632A 2015-1… US              
##  3 US15360203A 2016-11… 2016-11-23    US15360203A 2016-1… US              
##  4 US62203383P 2015-08… 2015-08-10    US15454805A 2017-0… US              
##  5 US62200764P 2015-08… 2015-08-04    US15263985A 2016-0… US              
##  6 KR201528901A 2015-0… 2015-03-02    US15057264A 2016-0… KR              
##  7 US2015196885P 2015-… 2015-07-24    US15217944A 2016-0… US              
##  8 US2008100721P 2008-… 2008-09-27    US14808174A 2015-0… US              
##  9 FR20142036A 2014-09… 2014-09-12    US14848061A 2015-0… FR              
## 10 US14970643A 2015-12… 2015-12-16    US14970643A 2015-1… US              
## # ... with 9,356 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What this means is that where we have a dataset with a priority number field and the application number field as a key we do not need to write all the code again by hand. We may have to adjust the code… for example if the numbers contain different separators (such as ;; in the case of the Lens database) or junk such as “[” is found in a data field. However, the ability to turn code into a reusable function is the most powerful feature of programming languages such as R and a powerful reason to engage with R when working with patent data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wrap-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Wrap Up&lt;/h3&gt;
&lt;p&gt;In this article we have taken a deep dive into the exploration of how to count the first filings of patent applications using information in the priority number field. We have focused on reducing a set of 18,970 patent applications to the earliest filings and arrived at 9,366 results.&lt;/p&gt;
&lt;p&gt;The key take home messages from this article are that to identify the earliest priority filing we have to do the following&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Deduplicate our data on application numbers&lt;/li&gt;
&lt;li&gt;Separate the individual priority numbers onto their own row&lt;/li&gt;
&lt;li&gt;Make sure we trim white space&lt;/li&gt;
&lt;li&gt;Group the data on application numbers and then identify the earliest priority date for each application&lt;/li&gt;
&lt;li&gt;Filter the data to the earliest priority date per application&lt;/li&gt;
&lt;li&gt;Identify and remove duplicate priority numbers&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As discussed above, this approach focuses on a straightforward method for identifying the earliest priority filing. A more sophisticated approach would break the dataset down to identify the cases where the priority number is identical to an application number and then work through the data focusing on provisional applications. The outcome of such an exercise will not be radically different, however it would arguably be more accurate in terms of identifying the priority date closest to the date of a specific invention and working through the filing route issues. For today however this is more than enough for a first deep dive into counting patent filings by priority. If you have survived this far congratulations. You now know more than most people alive about how to count priority filings. Yay!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;One of the most widely cited works providing an overview of the use of patents statistics is Griliches, Z 1998 Patent Statistics as Economic Indicators: A Survey, in Griliches, Z (ed.), R&amp;amp;D and Productivity: The Econometric Evidence. Cambridge: Cambridge University Press, available at &lt;a href=&#34;http://www.nber.org/chapters/c8351.pdf&#34; class=&#34;uri&#34;&gt;http://www.nber.org/chapters/c8351.pdf&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;the date field is missing in our dataset and this is common, Clarivate also adds zeros as padding so it is US20150357831A1. &lt;a href=&#34;mailto:esp@acenet&#34;&gt;esp@acenet&lt;/a&gt; adds the year following the kind code as US201514815121 20150731 whereas in our Derwent Innovation data the number is US14815121 20150731&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;For details see the &lt;a href=&#34;https://www.uspto.gov/patents-getting-started/patent-basics/types-patent-applications/provisional-application-patent&#34;&gt;USPTO web page on Provisional Applications&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;In formal terms kind codes refer to publication types and publication levels. Their use varies over time in individual countries and across countries and should therefore be approached with a degree of caution. At major patent offices kind code A typically denotes an application and kind code B a patent grant, except for US patent documents prior to 2001 where kind code A denotes a patent grant. As this suggests, caution is needed.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;in everyday practice you may want to keep the publication number to look up records and check you are on the right track&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.wipo.int/treaties/en/ShowResults.jsp?lang=en&amp;amp;treaty_id=2&#34; class=&#34;uri&#34;&gt;http://www.wipo.int/treaties/en/ShowResults.jsp?lang=en&amp;amp;treaty_id=2&lt;/a&gt;&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;note that the drones dataset is a training set that includes noisy terms and is not expected to fully reflect trends in drone technology&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;See &lt;a href=&#34;https://edwinth.github.io/blog/dplyr-recipes/&#34;&gt;Edwin Theon’s blog&lt;/a&gt; for an introduction along with the &lt;a href=&#34;https://www.rstudio.com/resources/webinars/tidy-eval/&#34;&gt;RStudio video&lt;/a&gt; and &lt;a href=&#34;https://maraaverick.rbind.io/2017/08/tidyeval-resource-roundup/&#34;&gt;Mara Avericks’s tidy eval resource roundup&lt;/a&gt;&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An Update on Importing Excel Data in R</title>
      <link>/importing-excel-data-into-r-updated/</link>
      <pubDate>Tue, 06 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/importing-excel-data-into-r-updated/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#import-directly-from-the-rstudio-menu&#34;&gt;Import Directly from the RStudio Menu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reading-an-excel-file-from-a-url&#34;&gt;Reading an Excel file from a URL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tidying-column-names-with-janitor&#34;&gt;Tidying column names with &lt;code&gt;janitor&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exporting-to-excel&#34;&gt;Exporting to Excel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#round-up&#34;&gt;Round Up&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Back in 2015 I wrote a long blog post on &lt;a href=&#34;https://www.pauloldham.net/reading-writing-excel-files-r/&#34;&gt;importing Excel tables into R&lt;/a&gt;. Happily for everyone this is now a lot easier than it was. This post provides an update on importing spreadsheets into R and exporting from R to Excel. I’ll also cover reading an excel file into R from a url as that seems to be an ongoing struggle.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;import-directly-from-the-rstudio-menu&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Import Directly from the RStudio Menu&lt;/h2&gt;
&lt;p&gt;The big change is that it is now very easy to import from Excel using the RStudio Menu: &lt;code&gt;File &amp;gt; Import Dataset &amp;gt; From Excel&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/excel/file_import.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, navigate to the file that you want to import and select it. You will then see something like this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/excel/import_panel.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One point to bear in mind is that the import will often default to the name &lt;code&gt;dataset&lt;/code&gt; so that you need to make sure you enter a meaningful name for the dataset.&lt;/p&gt;
&lt;p&gt;If your workbook has multiple sheets then you can choose a sheet number using &lt;code&gt;Sheet&lt;/code&gt;, choose the maximum number of rows or skip rows if you have a bunch of filler junk in the top rows. Regular Excel users may also want to select columns by Range.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/excel/import_panel_options.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can also click on a column and choose to skip it or change the format.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/excel/skip_col.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is worth bearing in mind that if you are importing a number of worksheets you can easily lose track. I sometimes use the approach of copying the import chunk into an Rmarkdown document to keep track of what I am doing and where a file came from.&lt;/p&gt;
&lt;p&gt;When copying chunks note the small clipboard icon in the top right above the chunk that will copy the chunk to the clipboard for pasting into the console or an R markdown code chunk to document your import steps for the future. My approach when working with multiple sheets is to create an R markdown file and copy and paste the import code into chunks that I then save. That allows “future me”, to borrow from Hadley Wickham, to understand where the datasets came from.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/excel/import_panel_chunk.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see from importing the file behind the scenes RStudio is using the &lt;code&gt;readxl&lt;/code&gt; library to import the file.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;readxl&lt;/code&gt; will commonly generate warning messages during the import process. For example this dataset generated a long long string of warnings that looked like this.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Expecting logical in AH5501 / R5501C34: got ‘Aaptos suberitoides’Expecting logical in AH5502 / R5502C34: got ’Abdopus abaculus’Expecting logical in AH5503 / R5503C34: got ’Abdopus aculeatus’Expecting logical in AH5504 / R5504C34: got ’Abralia armata’Expecting logical in AH5505 / R5505C34: got ’Abraliopsis hoylei’Expecting logical in AH5506 / R5506C34: got ’Abudefduf bengalensis’Expecting logical in AH5507 / R5507C34: got ’Abudefduf sexfasciatus’”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;These warnings arise because &lt;code&gt;readxl&lt;/code&gt; guesses the column type by reading the top 1000 rows for each column. However, where a column contains a mix of numbers or characters this can lead to an &lt;code&gt;expecting logical/expecting integer&lt;/code&gt; type of error. A lot of the time this is not actually a problem. However, it is important to pay attention to the warnings because they may indicate an actual problem with your data (such as lines spilling across rows).&lt;/p&gt;
&lt;p&gt;To fix this there are a number of options to try.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Use the guess_max argument&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Use the &lt;code&gt;guess_max&lt;/code&gt; argument to increase the number of rows that are read to guess the column type. The default is 1000 and here we reset it to 2000. In the case of our example dataset this didn’t work because the problems appeared lower down but it often will. You can add an &lt;code&gt;n_max&lt;/code&gt; value (shown below as NULL) where you know the maximum number of rows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readxl)
taxonomy &amp;lt;- read_excel(&amp;quot;/Users/pauloldham17inch/Desktop/open_source_master/asean/data-taxonomy/taxonomy_final.xlsx&amp;quot;, 
    guess_max = min(2000, n_max = NULL))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An alternative to this approach is simply to set &lt;code&gt;min&lt;/code&gt; as the maximum number of rows. The issue here is that you would of course need to already have opened the spreadsheet to identify the number of rows, but there is no reason not to simply guess large.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readxl)
taxonomy &amp;lt;- read_excel(&amp;quot;/Users/pauloldham17inch/Desktop/taxonomy_final.xlsx&amp;quot;, 
    guess_max = min(8400, n_max = NULL))
taxonomy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8,400 x 50
##    scientificname  type  genusorabove specificepithet parsed authorsparsed
##    &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;           &amp;lt;lgl&amp;gt;  &amp;lt;lgl&amp;gt;        
##  1 abramis brama   SCIE… Abramis      brama           TRUE   TRUE         
##  2 acanthamoeba p… SCIE… Acanthamoeba polyphaga       TRUE   TRUE         
##  3 acaudina molpa… SCIE… Acaudina     molpadioides    TRUE   TRUE         
##  4 acipenser dabr… SCIE… Acipenser    dabryanus       TRUE   TRUE         
##  5 acipenser fulv… SCIE… Acipenser    fulvescens      TRUE   TRUE         
##  6 acipenser mika… SCIE… Acipenser    mikadoi         TRUE   TRUE         
##  7 acipenser oxyr… SCIE… Acipenser    oxyrinchus      TRUE   TRUE         
##  8 acipenser ruth… SCIE… Acipenser    ruthenus        TRUE   TRUE         
##  9 acipenser schr… SCIE… Acipenser    schrencki       TRUE   TRUE         
## 10 acrocalanus gr… SCIE… Acrocalanus  gracilis        TRUE   TRUE         
## # ... with 8,390 more rows, and 44 more variables: canonicalname &amp;lt;chr&amp;gt;,
## #   canonicalnamewithmarker &amp;lt;chr&amp;gt;, canonicalnamecomplete &amp;lt;chr&amp;gt;,
## #   rankmarker &amp;lt;chr&amp;gt;, gbif_id &amp;lt;chr&amp;gt;, db &amp;lt;chr&amp;gt;, match &amp;lt;chr&amp;gt;,
## #   multiple_matches &amp;lt;lgl&amp;gt;, pattern_match &amp;lt;lgl&amp;gt;, uri &amp;lt;chr&amp;gt;, kingdom &amp;lt;chr&amp;gt;,
## #   phylum &amp;lt;chr&amp;gt;, class &amp;lt;chr&amp;gt;, order &amp;lt;chr&amp;gt;, family &amp;lt;chr&amp;gt;, genus &amp;lt;chr&amp;gt;,
## #   species &amp;lt;chr&amp;gt;, kingdom_id &amp;lt;chr&amp;gt;, phylum_id &amp;lt;chr&amp;gt;, class_id &amp;lt;chr&amp;gt;,
## #   order_id &amp;lt;chr&amp;gt;, family_id &amp;lt;chr&amp;gt;, genus_id &amp;lt;chr&amp;gt;, species_id &amp;lt;chr&amp;gt;,
## #   query &amp;lt;chr&amp;gt;, scientificname1 &amp;lt;chr&amp;gt;, required_fields_check &amp;lt;dbl&amp;gt;,
## #   environment_aphia_worms &amp;lt;chr&amp;gt;, name_aphia_worms &amp;lt;chr&amp;gt;,
## #   aphiaid_worms &amp;lt;dbl&amp;gt;, accepted_name_aphia_worms &amp;lt;chr&amp;gt;,
## #   valid_aphiaid_worms &amp;lt;dbl&amp;gt;, status_aphia_worms &amp;lt;chr&amp;gt;,
## #   taxonmatch_matchcount_worms &amp;lt;dbl&amp;gt;, taxonmatch_note_worms &amp;lt;chr&amp;gt;,
## #   species1 &amp;lt;chr&amp;gt;, match1 &amp;lt;lgl&amp;gt;, environment &amp;lt;chr&amp;gt;, marine &amp;lt;chr&amp;gt;,
## #   brackish &amp;lt;chr&amp;gt;, freshwater &amp;lt;chr&amp;gt;, terrestrial &amp;lt;chr&amp;gt;,
## #   noenvironment &amp;lt;chr&amp;gt;, worms_id &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. Specify the column types &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If that doesn’t work for you then a third option is to work out what the format should be and pass it as a string. Arguably, this should be the first option. However, it can also be the most time consuming.&lt;/p&gt;
&lt;p&gt;A toy example is the following data frame that we can write to excel (see below on writing files).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(writexl)
df &amp;lt;- tibble(a = c(1,2,3), 
             b = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;), 
             c = c(TRUE, FALSE, TRUE), 
             d = c(&amp;quot;2017-12-10&amp;quot;, &amp;quot;20170815&amp;quot;, &amp;quot;2017_06_12&amp;quot;)) %&amp;gt;%
  writexl::write_xlsx(., &amp;quot;df.xlsx&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we read this in we specify the column types. Note that in this case we need to use the term “text” rather than the familiar “character” or we get an error.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- read_excel(&amp;quot;/Users/pauloldham17inch/blog/content/post/df.xlsx&amp;quot;, col_names = TRUE, 
    col_types = c(&amp;quot;numeric&amp;quot;, &amp;quot;text&amp;quot;, &amp;quot;logical&amp;quot;, &amp;quot;text&amp;quot;))
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 4
##       a b     c     d         
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;lgl&amp;gt; &amp;lt;chr&amp;gt;     
## 1    1. a     TRUE  2017-12-10
## 2    2. b     FALSE 20170815  
## 3    3. c     TRUE  2017_06_12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The documentation for read_excel (&lt;code&gt;?read_excel&lt;/code&gt;) sets out quite a few other options. For example we could specify the format of some columns and leave the function to guess the others. That would look like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- read_excel(&amp;quot;/Users/pauloldham17inch/blog/content/post/df.xlsx&amp;quot;, col_names = TRUE, 
    col_types = c(&amp;quot;guess&amp;quot;, &amp;quot;guess&amp;quot;, &amp;quot;logical&amp;quot;, &amp;quot;guess&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;3. Convert all columns to a single type&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For a dataset with a lot of columns trying to work out the column types or writing &lt;code&gt;guess, logical, character&lt;/code&gt; can rapidly become painful. Depending on your needs it may be easier to simply use the &lt;code&gt;col_types = &amp;quot;text&amp;quot;&lt;/code&gt; for all columns and change them where needed later using &lt;code&gt;as.character()&lt;/code&gt;, &lt;code&gt;as.logical()&lt;/code&gt;, &lt;code&gt;as.numeric()&lt;/code&gt; or &lt;code&gt;as.Date()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readxl)
taxonomy &amp;lt;- read_excel(&amp;quot;/Users/pauloldham17inch/Desktop/taxonomy_final.xlsx&amp;quot;, 
    col_types = &amp;quot;text&amp;quot;)
taxonomy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8,400 x 50
##    scientificname  type  genusorabove specificepithet parsed authorsparsed
##    &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;        
##  1 abramis brama   SCIE… Abramis      brama           TRUE   TRUE         
##  2 acanthamoeba p… SCIE… Acanthamoeba polyphaga       TRUE   TRUE         
##  3 acaudina molpa… SCIE… Acaudina     molpadioides    TRUE   TRUE         
##  4 acipenser dabr… SCIE… Acipenser    dabryanus       TRUE   TRUE         
##  5 acipenser fulv… SCIE… Acipenser    fulvescens      TRUE   TRUE         
##  6 acipenser mika… SCIE… Acipenser    mikadoi         TRUE   TRUE         
##  7 acipenser oxyr… SCIE… Acipenser    oxyrinchus      TRUE   TRUE         
##  8 acipenser ruth… SCIE… Acipenser    ruthenus        TRUE   TRUE         
##  9 acipenser schr… SCIE… Acipenser    schrencki       TRUE   TRUE         
## 10 acrocalanus gr… SCIE… Acrocalanus  gracilis        TRUE   TRUE         
## # ... with 8,390 more rows, and 44 more variables: canonicalname &amp;lt;chr&amp;gt;,
## #   canonicalnamewithmarker &amp;lt;chr&amp;gt;, canonicalnamecomplete &amp;lt;chr&amp;gt;,
## #   rankmarker &amp;lt;chr&amp;gt;, gbif_id &amp;lt;chr&amp;gt;, db &amp;lt;chr&amp;gt;, match &amp;lt;chr&amp;gt;,
## #   multiple_matches &amp;lt;chr&amp;gt;, pattern_match &amp;lt;chr&amp;gt;, uri &amp;lt;chr&amp;gt;, kingdom &amp;lt;chr&amp;gt;,
## #   phylum &amp;lt;chr&amp;gt;, class &amp;lt;chr&amp;gt;, order &amp;lt;chr&amp;gt;, family &amp;lt;chr&amp;gt;, genus &amp;lt;chr&amp;gt;,
## #   species &amp;lt;chr&amp;gt;, kingdom_id &amp;lt;chr&amp;gt;, phylum_id &amp;lt;chr&amp;gt;, class_id &amp;lt;chr&amp;gt;,
## #   order_id &amp;lt;chr&amp;gt;, family_id &amp;lt;chr&amp;gt;, genus_id &amp;lt;chr&amp;gt;, species_id &amp;lt;chr&amp;gt;,
## #   query &amp;lt;chr&amp;gt;, scientificname1 &amp;lt;chr&amp;gt;, required_fields_check &amp;lt;chr&amp;gt;,
## #   environment_aphia_worms &amp;lt;chr&amp;gt;, name_aphia_worms &amp;lt;chr&amp;gt;,
## #   aphiaid_worms &amp;lt;chr&amp;gt;, accepted_name_aphia_worms &amp;lt;chr&amp;gt;,
## #   valid_aphiaid_worms &amp;lt;chr&amp;gt;, status_aphia_worms &amp;lt;chr&amp;gt;,
## #   taxonmatch_matchcount_worms &amp;lt;chr&amp;gt;, taxonmatch_note_worms &amp;lt;chr&amp;gt;,
## #   species1 &amp;lt;chr&amp;gt;, match1 &amp;lt;chr&amp;gt;, environment &amp;lt;chr&amp;gt;, marine &amp;lt;chr&amp;gt;,
## #   brackish &amp;lt;chr&amp;gt;, freshwater &amp;lt;chr&amp;gt;, terrestrial &amp;lt;chr&amp;gt;,
## #   noenvironment &amp;lt;chr&amp;gt;, worms_id &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With our toy dataset we could easily change the columns that are our target as needed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df$a &amp;lt;- as.numeric(df$a)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dates can be troublesome and in cases where you need to format date fields the &lt;a href=&#34;http://lubridate.tidyverse.org/&#34;&gt;&lt;code&gt;lubridate&lt;/code&gt;&lt;/a&gt; package will really make your life a whole lot easier.&lt;/p&gt;
&lt;p&gt;In our toy dataset while the dates are all in YYYYMMDD format (and those in your dataset may not be) the separators are different. Using &lt;code&gt;as.Date()&lt;/code&gt; won’t work for the second and third dates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.Date(df$d)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, this problem is easily handled by &lt;code&gt;lubridate::as_date&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lubridate)
df$e &amp;lt;- lubridate::as_date(df$d)
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 5
##       a b     c     d          e         
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;lgl&amp;gt; &amp;lt;chr&amp;gt;      &amp;lt;date&amp;gt;    
## 1    1. a     TRUE  2017-12-10 2017-12-10
## 2    2. b     FALSE 20170815   2017-08-15
## 3    3. c     TRUE  2017_06_12 2017-06-12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Charlotte Wickham offers an incredibly useful DataCamp course &lt;a href=&#34;https://www.datacamp.com/courses/working-with-dates-and-times-in-r&#34;&gt;Working with Dates and Times in R&lt;/a&gt; that will have you up and running in no time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reading-an-excel-file-from-a-url&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reading an Excel file from a URL&lt;/h2&gt;
&lt;p&gt;In the 2015 post on importing Excel I wrote:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“It is faster to simply download the file to your drive, or swim the Atlantic ocean, than to successfully download an excel file on http: or, in particular https:. So maybe ask yourself what is the path of least resistance and run with that.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As far as I can tell the situation is not radically different now. However, this is something that lots of people have logically wanted to do. By the power of Stack Overflow, a solution can be found. &lt;a href=&#34;https://stackoverflow.com/users/1327739/lukea&#34;&gt;Luke A&lt;/a&gt; provided the following answer to this &lt;a href=&#34;https://stackoverflow.com/questions/41368628/read-excel-file-from-a-url-using-the-readxl-package&#34;&gt;question&lt;/a&gt; on downloading excel files.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readxl)
library(httr)
packageVersion(&amp;quot;readxl&amp;quot;)
# [1] ‘0.1.1’

GET(url1, write_disk(tf &amp;lt;- tempfile(fileext = &amp;quot;.xls&amp;quot;)))
df &amp;lt;- read_excel(tf, 2L)
str(df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This code uses the &lt;code&gt;httr&lt;/code&gt; package to read in a .xls file from a url that is written to disk and then passed to &lt;code&gt;readxl&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can wrap this into a small function with some adjustments. In this case we use &lt;code&gt;str_detect&lt;/code&gt; to detect whether the file type is included in the URL. Note that this will not address those cases (such as Google Drive) where the Excel file type is not included (see the &lt;a href=&#34;https://github.com/tidyverse/googledrive&#34;&gt;googledrive package&lt;/a&gt;). Nor will it detect other Excel file types such as &lt;code&gt;.xlsm&lt;/code&gt; for macro enabled workbooks. As this suggests the task is more complex than it might at first appear. This small function addresses common use cases but will not address all use cases.&lt;/p&gt;
&lt;p&gt;The function assumes that the file extension is contained in the URL and will spot that for us, in the case of a zip extension it will download and attempt to extract the file and if all else fails, we can provide the file extension. the &lt;code&gt;...&lt;/code&gt; informs us that other arguments such as &lt;code&gt;col_types =&lt;/code&gt; can be passed to the function and will be picked up by &lt;code&gt;read_excel&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;readxl_online &amp;lt;- function(url, type = NULL, ...) {
    test &amp;lt;- stringr::str_detect(url, &amp;quot;[.]xls|[.]zip&amp;quot;)
    if (test == FALSE) {
        print(message(&amp;quot;Expecting file extension of type .xlsx, .xls or .zip. Check the URL or the data source for the correct file extension and use the type argument&amp;quot;))
    }
    # test for individual file extensions for xls use look forward, xls not
    # followed by x
    t1 &amp;lt;- stringr::str_detect(url, &amp;quot;[.]xlsx&amp;quot;)
    t2 &amp;lt;- stringr::str_detect(url, &amp;quot;[.]xls(?!x)&amp;quot;)
    tz &amp;lt;- stringr::str_detect(url, &amp;quot;[.]zip&amp;quot;)
    if (t1 == TRUE) {
        type = &amp;quot;.xlsx&amp;quot;
    }
    if (t2 == TRUE) {
        type = &amp;quot;.xls&amp;quot;
    }
    if (tz == TRUE) {
        httr::GET(url, write_disk(&amp;quot;tmp.zip&amp;quot;, overwrite = TRUE))
        tmp &amp;lt;- unzip(&amp;quot;tmp.zip&amp;quot;)
        # On osx more than one file name is returned, select first element.
        df &amp;lt;- readxl::read_excel(tmp[[1]])
        return(df)
    }
    if (!is.null(type)) {
        type = type
        
    }
    df &amp;lt;- httr::GET(url, write_disk(paste0(&amp;quot;tmp&amp;quot;, type), overwrite = TRUE))
    df &amp;lt;- readxl::read_excel(paste0(&amp;quot;tmp&amp;quot;, type))
    
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is not perfect, but it is a start. We can now run a test on different data types to see if it will work. These urls are all from excel files on Github. Github file urls are actually placeholders and so we need to follow the link and copy the Raw file url (see raw=true in the url). Note also that these urls are all &lt;code&gt;https:&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The .xls case:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dfxls &amp;lt;- readxl_online(&amp;quot;https://github.com/wipo-analytics/opensource-patent-analytics/blob/master/2_datasets/pizza_all_24294/patentscope_pizza_1940_2005_9659.xls?raw=true&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The xlsx case:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dfxlsx &amp;lt;- readxl_online(&amp;quot;https://github.com/wipo-analytics/opensource-patent-analytics/blob/master/2_datasets/ewaste/ewaste.xlsx?raw=true&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The zip file case:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dfzip &amp;lt;- readxl_online(&amp;quot;https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/taxonomy_final.zip?raw=true&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is always a good thing if functions fail fast and provide a helpful message.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;error &amp;lt;- readxl_online(&amp;quot;https://www.google.co.uk/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This prints the expected message.&lt;/p&gt;
&lt;p&gt;“Expecting file extension of type .xlsx, .xls or .zip. Check the URL or the data source for the correct file extension and use the type argument”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidying-column-names-with-janitor&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidying column names with &lt;code&gt;janitor&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;One issue once you have your data in R is that column names in excel files often contain mixed case names and spaces or other characters such as brackets that can be awkward to work with in R. To solve that an easy option is to use the recent &lt;code&gt;janitor&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;janitor&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this we need an excel worksheet with noisy names. For R coding Blue Peter fans…“Here is one we prepared earlier”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;noisycols &amp;lt;- read_excel(&amp;quot;/Users/pauloldham17inch/blog/content/post/noisydf.xlsx&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;noisy(yes)&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;really_,Noisy;!&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;EVEN noisier !?*$!&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;OMG- I_can’t-***//believe?it|&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;these&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;are&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;the&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;noisiest&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;column&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;names&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;in&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;the&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;world,&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;just&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;a&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;tribute&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NANA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NANANANA&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(janitor)
noisycols1 &amp;lt;- janitor::clean_names(noisycols)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;noisy_yes&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;really_noisy&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;even_noisier&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;omg_i_can_t_believe_it&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;these&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;are&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;not&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;the&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;noisiest&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;column&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;names&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;in&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;the&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;world,&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;just&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;a&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;tribute&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NANA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NANANANA&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This does a very good job of cleaning up names but may not always catch everything. If you have particular needs the &lt;a href=&#34;https://github.com/tidyverse/stringr&#34;&gt;&lt;code&gt;stringr&lt;/code&gt;&lt;/a&gt; package (now installed with the &lt;code&gt;tidyverse&lt;/code&gt;) is the go to package. Try the &lt;code&gt;str_replace_all&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;If you need more help try the &lt;a href=&#34;https://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf&#34;&gt;Basic Regular Expressions Cheatsheet in R&lt;/a&gt; or the chapter on strings in Hadley Wickham’s book &lt;a href=&#34;http://r4ds.had.co.nz/strings.html&#34;&gt;R for Data Science&lt;/a&gt;. Charlotte Wickham also offers a Data Camp course on &lt;a href=&#34;https://www.datacamp.com/courses/string-manipulation-in-r-with-stringr&#34;&gt;String Manipulation in R with stringr&lt;/a&gt;. When it comes to working with strings &lt;code&gt;stringr&lt;/code&gt; is your friend and if that doesn’t solve the problem then try the &lt;code&gt;stringi&lt;/code&gt; package that powers &lt;code&gt;stringr&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exporting-to-excel&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exporting to Excel&lt;/h2&gt;
&lt;p&gt;In the earlier post I wrote about using &lt;code&gt;write.xlsx()&lt;/code&gt; from the &lt;code&gt;xlsx&lt;/code&gt; package. That is still a very good option. However, as a personal preference I have now switched over to the &lt;code&gt;writexl&lt;/code&gt; package as I find it easier to remember and use. It is also an &lt;a href=&#34;https://github.com/ropensci/writexl&#34;&gt;ROpenSci package&lt;/a&gt; and I use a lot of ROpenSci packages. &lt;code&gt;writexl&lt;/code&gt; has the added bonus that Clippy appears in the documentation to brighten up your day… or drive you insane… as the case may be. So, if you prefer to be a curmudgeon about Clippy you may want to use the xlsx package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;writexl&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We simply specify the file and the name of the file we want to write. An additional argument &lt;code&gt;col_names = TRUE&lt;/code&gt; is set to TRUE by default so you only need to specify that if you want the value to be FALSE.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(writexl)
writexl::write_xlsx(df, path = &amp;quot;df.xlsx&amp;quot;, col_names = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also now write multiple sheets by specifying the data frames in a list and passing them to write_xlsx.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(writexl)
tmp &amp;lt;- list(df, noisycols1)
write_xlsx(tmp, &amp;quot;tmp.xlsx&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round Up&lt;/h2&gt;
&lt;p&gt;There we have it. Reading and writing Excel files in R is now way way easier than it was just a couple of years ago thanks to the dedicated work of those behind &lt;code&gt;readxl&lt;/code&gt; (Hadley Wickham and Jenny Bryan) and &lt;code&gt;writexl&lt;/code&gt; by Jeroen Ooms and John McNamara. Other packages will take you to the same place but these are my go to packages. Community contributions are helping to solve the mystery of reading Excel files from urls and we might hope that at some point &lt;code&gt;readxl&lt;/code&gt; may address this problem.&lt;/p&gt;
&lt;p&gt;If you would like to learn more on importing data into R then try the DataCamp course on &lt;a href=&#34;https://www.datacamp.com/tracks/importing-cleaning-data-with-r&#34;&gt;Importing Data &amp;amp; Cleaning with R&lt;/a&gt; that covers Excel.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dr. Evil meets the robotstxt package</title>
      <link>/using-robotstxt-in-r/</link>
      <pubDate>Mon, 08 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/using-robotstxt-in-r/</guid>
      <description>&lt;p&gt;I am fairly new to webscraping in R using &lt;a href=&#34;https://github.com/hadley/rvest&#34;&gt;rvest&lt;/a&gt; and one question is whether a site gives permission for scraping. This information is often contained in the robots.txt file on a website. So, I’m briefly going to explore the &lt;a href=&#34;https://ropensci.org/&#34;&gt;ROpenSci&lt;/a&gt; &lt;a href=&#34;https://github.com/ropenscilabs/robotstxt&#34;&gt;robotstxt&lt;/a&gt; package by &lt;a href=&#34;https://github.com/petermeissnerpackage&#34;&gt;Peter Meissner&lt;/a&gt;. &lt;a href=&#34;https://github.com/ropenscilabs/robotstxt&#34;&gt;robotstxt&lt;/a&gt; provides easy access to the robots.txt file for a domain from R.&lt;/p&gt;
&lt;p&gt;I’m slowly working on a new R data package for underwater geographic feature names as part of a Norwegian Research Council funded project &lt;code&gt;biospolar&lt;/code&gt; on innovation involving biodiversity in marine polar areas. One of the main data sources for the package is the &lt;a href=&#34;https://www.gebco.net/data_and_products/undersea_feature_names/&#34;&gt;General Bathymetric Chart of the Oceans or GEBCO Gazeteer&lt;/a&gt;. I’m also going to be bringing in data from the &lt;a href=&#34;https://vents-data.interridge.org/&#34;&gt;Interridge database of hydrothermal vents&lt;/a&gt; and so wanted to understand whether I am just free to go ahead.&lt;/p&gt;
&lt;p&gt;The robots.txt content is advisory, and well we could always choose to be Dr. Evil. If my wife would let me have a cat it would definitely be called Mr. Bigglesworth. But it strikes me that building a package for a data source that tries to prohibit scraping might not be a brilliant idea.&lt;/p&gt;
&lt;p&gt;There are a bunch of functions in the &lt;code&gt;robotstxt&lt;/code&gt; package but I’m just going to use the main one &lt;code&gt;robotstxt()&lt;/code&gt;. Take a look at the &lt;a href=&#34;https://cran.r-project.org/web/packages/robotstxt/vignettes/using_robotstxt.html&#34;&gt;vignette&lt;/a&gt; for more information. For a very quick check on whether scraping on a path is allowed try the &lt;code&gt;paths_allowed()&lt;/code&gt; function. I’ll come back to that at the end.&lt;/p&gt;
&lt;p&gt;The first place I am going to look is the main GEBCO domain.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(robotstxt)
gebco &amp;lt;- robotstxt(&amp;quot;https://www.gebco.net&amp;quot;)
gebco&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $domain
## [1] &amp;quot;https://www.gebco.net&amp;quot;
## 
## $text
## [1] &amp;quot;Sitemap: https://www.gebco.net/sitemap.xml \r\n\r\nUser-agent: *\r\nHost: www.gebco.net\r\nDisallow: /cgi-bin/\r\nDisallow: /perl/\r\nDisallow: /css/\r\nDisallow: /js/\r\nDisallow: /_mm/\r\nDisallow: /_notes/\r\n\n[... 36 lines omitted ...]&amp;quot;
## 
## $bots
## [1] &amp;quot;*&amp;quot;                &amp;quot;Googlebot&amp;quot;        &amp;quot;Googlebot-Image&amp;quot; 
## [4] &amp;quot;Googlebot-Mobile&amp;quot;
## 
## $comments
## [1] line    comment
## &amp;lt;0 rows&amp;gt; (or 0-length row.names)
## 
## $permissions
##                         field useragent     value
## 1                    Disallow         * /cgi-bin/
## 2                    Disallow         *    /perl/
## 3                    Disallow         *     /css/
## 4                    Disallow         *      /js/
## 5                    Disallow         *     /_mm/
## 6                    Disallow         *  /_notes/
## 7                                                
## 8 [...  31 items omitted ...]                    
## 
## $crawl_delay
## [1] field     useragent value    
## &amp;lt;0 rows&amp;gt; (or 0-length row.names)
## 
## $host
##   field useragent         value
## 1  Host         * www.gebco.net
## 
## $sitemap
##     field useragent                             value
## 1 Sitemap         * https://www.gebco.net/sitemap.xml
## 
## $other
## [1] field     useragent value    
## &amp;lt;0 rows&amp;gt; (or 0-length row.names)
## 
## $robexclobj
## &amp;lt;Robots Exclusion Protocol Object&amp;gt;
## $check
## function (paths = &amp;quot;/&amp;quot;, bot = &amp;quot;*&amp;quot;) 
## {
##     spiderbar::can_fetch(obj = self$robexclobj, path = paths, 
##         user_agent = bot)
## }
## &amp;lt;bytecode: 0x7fc3af22a750&amp;gt;
## &amp;lt;environment: 0x7fc3af24bef8&amp;gt;
## 
## attr(,&amp;quot;class&amp;quot;)
## [1] &amp;quot;robotstxt&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This returns a list from the robots txt where the main bit I am interested in is the data frame under gebco$permissions.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;field&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;useragent&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;*&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/cgi-bin/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;*&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/perl/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;*&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/css/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;*&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/js/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;*&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/_mm/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;*&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/_notes/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;*&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/_baks/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;*&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/MMWIP/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/cgi-bin/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/perl/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/css/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/js/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/_mm/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/_notes/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/_baks/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/MMWIP/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/*templates&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;*/log.gif&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/*_baks&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/*_notes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/js&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;*.csi&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;*.vcf&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot-Image&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/cgi-bin/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot-Image&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/perl/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot-Image&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/css/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot-Image&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/js/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot-Image&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/_mm/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot-Image&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/_notes/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot-Image&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/_baks/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot-Image&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/MMWIP/&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot-Image&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;*/log.gif&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot-Mobile&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/*templates&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot-Mobile&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;*/log.gif&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot-Mobile&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/*_baks&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disallow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Googlebot-Mobile&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/*_notes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;What is of interest here are the entries under Value which can be a bit difficult to interpret. With the help of the handy &lt;a href=&#34;https://en.wikipedia.org/wiki/Robots_exclusion_standard&#34;&gt;Wikipedia article on the Robots Exclusion Standard&lt;/a&gt; I can see that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Disallow + *&lt;/code&gt; means to stay out of the website altogether.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Disallow + /xyz&lt;/code&gt; means to stay out of the specific directories.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Disallow Googlebot&lt;/code&gt; means that the named bot should stay out of either the website or (as in this case) specific directories. Note that Googlebot appears to be in the naughty seat because the site is more specific about what it should stay out of while others would be free to enter?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, the GEBCO data files that I am interested in are not hosted on the gebco.net domain but on the &lt;a href=&#34;https://www.ngdc.noaa.gov/&#34;&gt;NOAA National Centers for Environmental Information domain&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;noaa &amp;lt;- robotstxt(domain = &amp;quot;https://www.ngdc.noaa.gov&amp;quot;)
noaa&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $domain
## [1] &amp;quot;https://www.ngdc.noaa.gov&amp;quot;
## 
## $text
## [1] &amp;quot;User-agent: *\nCrawl-delay: 60\nDisallow: /cgi-bin\nDisallow: /dmsp/cgi-bin\nDisallow: /dmsp/data\nDisallow: /dmsp/include\nDisallow: /dmsp/protected\nDisallow: /eog\nDisallow: /geomag/cdroms\nDisallow: /geomag/data\n\n[... 67 lines omitted ...]&amp;quot;
## 
## $bots
## [1] &amp;quot;*&amp;quot;                                                                                            
## [2] &amp;quot;LinkChecker&amp;quot;                                                                                  
## [3] &amp;quot;siteimprove&amp;quot;                                                                                  
## [4] &amp;quot;Mozilla/5.0(compatible;MSIE10.0;WindowsNT6.1;Trident/6.0)LinkCheckbySiteimprove.com&amp;quot;          
## [5] &amp;quot;Mozilla/5.0(compatible;MSIE10.0;WindowsNT6.1;Trident/6.0)SiteCheck-sitecrawlbySiteimprove.com&amp;quot;
## [6] &amp;quot;HTMLValidatorbysiteimprove.com/1.3&amp;quot;                                                           
## 
## $comments
## [1] line    comment
## &amp;lt;0 rows&amp;gt; (or 0-length row.names)
## 
## $permissions
##                         field useragent           value
## 1                    Disallow         *        /cgi-bin
## 2                    Disallow         *   /dmsp/cgi-bin
## 3                    Disallow         *      /dmsp/data
## 4                    Disallow         *   /dmsp/include
## 5                    Disallow         * /dmsp/protected
## 6                    Disallow         *            /eog
## 7                                                      
## 8 [...  73 items omitted ...]                          
## 
## $crawl_delay
##         field useragent value
## 1 Crawl-delay         *    60
## 
## $host
## [1] field     useragent value    
## &amp;lt;0 rows&amp;gt; (or 0-length row.names)
## 
## $sitemap
## [1] field     useragent value    
## &amp;lt;0 rows&amp;gt; (or 0-length row.names)
## 
## $other
## [1] field     useragent value    
## &amp;lt;0 rows&amp;gt; (or 0-length row.names)
## 
## $robexclobj
## &amp;lt;Robots Exclusion Protocol Object&amp;gt;
## $check
## function (paths = &amp;quot;/&amp;quot;, bot = &amp;quot;*&amp;quot;) 
## {
##     spiderbar::can_fetch(obj = self$robexclobj, path = paths, 
##         user_agent = bot)
## }
## &amp;lt;bytecode: 0x7fc3af22a750&amp;gt;
## &amp;lt;environment: 0x7fc3aee6a4e0&amp;gt;
## 
## attr(,&amp;quot;class&amp;quot;)
## [1] &amp;quot;robotstxt&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The NOAA robotstxt provides some different information. For example, NOAA specifies a crawl delay of 60 seconds which tells me to build in a delay of 60 seconds to a call.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;noaa$text&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## User-agent: *
## Crawl-delay: 60
## Disallow: /cgi-bin
## Disallow: /dmsp/cgi-bin
## Disallow: /dmsp/data
## Disallow: /dmsp/include
## Disallow: /dmsp/protected
## Disallow: /eog
## Disallow: /geomag/cdroms
## Disallow: /geomag/data
## Disallow: /geomag/EMM/data
## Disallow: /geomag/pmag/datafiles
## Disallow: /geomag/WMM/data
## Disallow: /globe
## Disallow: /hazard/data
## Disallow: /hazard/img 
## Disallow: /IAGA/cgi-bin
## Disallow: /idb
## Disallow: /ionosonde
## Disallow: /mgg/cgi-bin
## Disallow: /mgg/curator/data
## Disallow: /mgg/curator/userfiles
## Disallow: /mgg/dat
## Disallow: /mgg/ecs/data
## Disallow: /mgg/gdas/data
## Disallow: /mgg/geology/data
## Disallow: /mgg/geology/odp/data
## Disallow: /mgg/grids/data
## Disallow: /mgg/oracle
## Disallow: /mgg/tmp
## Disallow: /mgg/trk
## Disallow: /ngdc/cgi-bin
## Disallow: /ngdc/hn
## Disallow: /ngdc/Counter
## Disallow: /ngdc/NOAAServer/adm
## Disallow: /ngdc/NOAAServer/converters
## Disallow: /ngdc/NOAAServer/gif
## Disallow: /ngdc/NOAAServer/java
## Disallow: /ngdc/NOAAServer/lib
## Disallow: /ngdc/NOAAServer_N
## Disallow: /ngdc/Store
## Disallow: /nmmr
## Disallow: /nndc
## Disallow: /paleo
## Disallow: /riwebapp/rest
## Disallow: /seg/cgi-bin
## Disallow: /stp/bin
## Disallow: /stp/cgi-bin
## Disallow: /stp/drap/data
## Disallow: /stp/include
## Disallow: /stp/image
## Disallow: /stp/images
## Disallow: /stp/include
## Disallow: /stp/iono/drap
## Disallow: /stp/iono/ustec/products
## Disallow: /stp/satellite/poes/dataaccess.html
## Disallow: /stp/satellite/goes/dataaccess.html
## Disallow: /sxi/servlet/sxibrowse
## Disallow: /sxi/servlet/sximovie
## Disallow: /sxi/servlet/sxisearch
## Disallow: /stp/IONO/ionosonde
## Disallow: /thredds
## Disallow: /wdc/cgi-bin
## 
## 
## User-agent: LinkChecker
## Disallow:
## 
## User-agent: siteimprove
## Disallow: /
## User-agent: Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0) LinkCheck by Siteimprove.com
## Disallow: /
## User-agent: Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0) SiteCheck-sitecrawl by Siteimprove.com
## Disallow: /
## User-agent: HTML Validator by siteimprove.com/1.3
## Disallow: /&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then see a list of disallowed directories and in this case I am interested in the &lt;code&gt;https://www.ngdc.noaa.gov/gazetteer/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The dir I am interested in for the package is not on the list so I think I am free to go ahead… yay!&lt;/p&gt;
&lt;p&gt;If I wanted to do this more quickly I would use the &lt;code&gt;paths_allowed()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;paths_allowed(&amp;quot;https://www.ngdc.noaa.gov/gazetteer/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, there we have it. If we prefer to be good web scraping citizens rather than the Dr. Evil of web scraping in R then the &lt;code&gt;robotstxt&lt;/code&gt; package will help us out. On the other hand we could just be evil and see what happens. I’m off to stroke Mr. Bigglesworth.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>&lt;p&gt;My name is Paul Oldham, I hold a PhD in Social Anthropology from the London School of Economics and I&amp;rsquo;m the lead at  &lt;a href=&#34;http://www.oneworldanalytics.com.&#34;&gt;One World Analytics&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;My major interest is the conservation of biodiversity and the rights of indigenous peoples. For much of the 1990s and into the 2000s I worked with a people called the Piaroa in Amazonas in Southern Venezuela. As part of that work I used my research grant to help to establish the Regional Indigenous Peoples of Amazonas (ORPIA) to defend the rights of indigenous peoples in the region. Nearly 25 years on ORPIA is still going.&lt;/p&gt;

&lt;p&gt;I remain passionate about the rights of indigenous peoples but I grew dissatisfied with the inability, or unwillingness, of anthropology to address issues of scale. So, as part of my research focusing on debates leading to the &lt;a href=&#34;https://www.cbd.int/abs/&#34;&gt;Nagoya Protocol on Access and Benefit-Sharing&lt;/a&gt; at the &lt;a href=&#34;https://www.cbd.int/&#34;&gt;UN Convention on Biological Diversity&lt;/a&gt; I moved into large scale analytics.&lt;/p&gt;

&lt;p&gt;My work now combines my passion for biodiversity with scientometrics/bibliometrics and patent analytics. So, most of the stuff I write about involves combinations of these topics. I have also been working in R for the last three years and trying to write about it. I am very far away from being expert in R but I really enjoy the enthusiasm and helpfulness of the R community. I am also a big fan of open source and initiative such as &lt;a href=&#34;https://ropensci.org/&#34;&gt;ROpenSci&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the work I do now I write for the &lt;a href=&#34;http://www.wipo.int/patentscope/en/programs/patent_landscapes/&#34;&gt;World Intellectual Property Organisation&lt;/a&gt; on &lt;a href=&#34;https://github.com/wipo-analytics&#34;&gt;Patent Analytics&lt;/a&gt; and work with the &lt;a href=&#34;https://www.giz.de/en/html/index.html&#34;&gt;German Technical Cooperation (GIZ)&lt;/a&gt; and &lt;a href=&#34;http://www.abs-initiative.info/&#34;&gt;ABS Initiative&lt;/a&gt; in capacity building projects to implement the Nagoya Protocol, mainly on Monitoring. This is challenging but fun and gets me out of the house. Most of the posts on the blog are part of work in progress for projects.&lt;/p&gt;

&lt;p&gt;Having worked for 10 years at the ESRC Centre for Economic and Social Aspects of Genomics (Cesagen) my involvement in academia is through a senior visiting fellowship with the Institute for Advanced Study of Sustainability at United Nations University and as an Industrial Fellow with the Manchester Institute for Innovation Research at Manchester Business School. The Manchester group has a great team working on analytics and Responsibile Research and Innovation that I am happy to be a part of.&lt;/p&gt;

&lt;p&gt;In my spare time I try to keep a 1972 VW Bay Window camper van and a 1970 VW convertible beetle on the road. I love gardening but am rubbish at growning tomatoes.&lt;/p&gt;

&lt;p&gt;This blog is written in &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;RStudio&lt;/a&gt; using the great &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;blogdown&lt;/a&gt;  package by &lt;a href=&#34;https://github.com/yihui&#34;&gt;Yihui Xie&lt;/a&gt; and built in &lt;a href=&#34;https://gohugo.io/&#34;&gt;Hugo&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating an Infographic with infogram</title>
      <link>/infographics/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/infographics/</guid>
      <description>&lt;p&gt;In this article we will use RStudio to prepare patent data for visualisation in an infographic using the online software tool &lt;a href=&#34;https://infogram.com/?rc=paid0sem0branded0search0&amp;amp;gclid=EAIaIQobChMIw6KgvMiq2AIViLvtCh2fpgxhEAAYASAAEgKR2PD_BwE&#34;&gt;infogram&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Infographics are a popular way of presenting data in a way that is easy for a reader to understand without reading a long report. Infographics are well suited to presenting summaries of data with simple messages about key findings. A good infographic can encourage the audience to read a detailed report and is a tool for engagement with audiences during presentations of the findings of patent research.&lt;/p&gt;
&lt;p&gt;Some patent offices have already been creating infographics as part of their reports to policy makers and other clients. The Instituto Nacional de Propiedade Industrial (INPI) in Brazil produces regular two page &lt;a href=&#34;http://www.inpi.gov.br/menu-servicos/informacao/radares-tecnologicos&#34;&gt;Technology Radar&lt;/a&gt; (Radar Tecnologico) consisting of charts and maps that briefly summarise more detailed research on subjects such as &lt;a href=&#34;http://www.inpi.gov.br/menu-servicos/arquivos-cedin/n08_radar_tecnologico_nano_residuos_versao_resumida_ingles_atualizada_20160122.pdf&#34;&gt;Nanotechnology in Waste Management&lt;/a&gt;. &lt;a href=&#34;http://www.wipo.int/patentscope/en/programs/patent_landscapes/&#34;&gt;WIPO Patent Landscape Reports&lt;/a&gt;, which go into depth on patent activity for a particular area, are accompanied by one page infographics that have proved very popular such as the infographic accompanying a recent report on &lt;a href=&#34;http://www.wipo.int/export/sites/www/patentscope/en/programs/patent_landscapes/reports/documents/assistivedevices_infographic.pdf&#34;&gt;assistive devices&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A growing number of companies are offering online infographic software services such as &lt;a href=&#34;https://infogr.am/app/#/library&#34;&gt;infogr.am&lt;/a&gt;,&lt;a href=&#34;http://www.easel.ly&#34;&gt;easel.ly&lt;/a&gt; &lt;a href=&#34;https://magic.piktochart.com/templates&#34;&gt;piktochart.com&lt;/a&gt;, &lt;a href=&#34;https://www.canva.com/create/infographics/&#34;&gt;canva.com&lt;/a&gt; or &lt;a href=&#34;https://venngage.com&#34;&gt;venngage.com&lt;/a&gt; to mention only a selection of the offerings out there. The &lt;a href=&#34;http://www.coolinfographics.com/tools/&#34;&gt;Cool Infographics website&lt;/a&gt; provides a useful overview of available tools.&lt;/p&gt;
&lt;p&gt;One feature of many of these services is that they are based on a freemium model. Creating graphics is free but the ability to export files and the available formats for export of your masterpiece (e.g. high resolution or .pdf) often depend on upgrading to a monthly account at varying prices. In this chapter we test drive &lt;a href=&#34;https://infogr.am/app/#/library&#34;&gt;infogr.am&lt;/a&gt; as a chart friendly service, albeit with export options that depend on a paid account.&lt;/p&gt;
&lt;p&gt;This article is divided into two sections.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In part 1 we focus on using RStudio to prepare patent data for visualisation in infographics software using the &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;tidyr&lt;/code&gt; and &lt;code&gt;stringr&lt;/code&gt; packages. This involves dealing with common problems with patent data such as concatenated fields, white space and creating counts of data fields. Part 1 is intended for those starting out using R and assumes no prior knowledge of R.&lt;/li&gt;
&lt;li&gt;In part 2 we produce an infographic from the data using &lt;a href=&#34;https://infogr.am/app/#/library&#34;&gt;infogr.am&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This article is now a chapter in the &lt;a href=&#34;https://wipo-analytics.github.io/&#34;&gt;WIPO Manual on Open Source Patent Analytics&lt;/a&gt;. You can read the chapter in electronic book format &lt;a href=&#34;https://wipo-analytics.github.io/patent-infographics-with-r.html&#34;&gt;here&lt;/a&gt; and find all the materials including presentations at the &lt;a href=&#34;https://github.com/wipo-analytics&#34;&gt;WIPO Analytics Github homepage&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;getting-started&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;To start with we need to ensure that RStudio and R for your operating system are installed by following the instructions on the RStudio website &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/&#34;&gt;here&lt;/a&gt;. Do not forget to follow the link to also &lt;a href=&#34;https://cran.rstudio.com&#34;&gt;install R for your operating system&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When working in RStudio it is good practice to work with projects. This will keep all of the files for a project in the same folder. To create a project go to File, New Project and create a project. Call the project something like infographic. Any file you create and save for the project will now be listed under the Files tab in RStudio.&lt;/p&gt;
&lt;p&gt;R works using packages (libraries) and there are around 7,490 of them for a whole range of purposes. We will use just a few of them. To install a package we use the following. Copy and paste the code into the Console and press enter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;tidyverse&amp;quot;)  # the group of packages you will need&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Packages can also be installed by selecting the Packages tab and typing the name of the package.&lt;/p&gt;
&lt;p&gt;To load the packages (libraries) use the following or check the tick box in the Packages pane.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now ready to go.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;load-a-.csv-file-using-readr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Load a .csv file using &lt;code&gt;readr&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;We will work with the &lt;code&gt;pizza_medium_clean&lt;/code&gt; dataset in the online &lt;a href=&#34;https://github.com/wipo-analytics/opensource-patent-analytics/tree/master/2_datasets&#34;&gt;Github Manual repository&lt;/a&gt;. If manually downloading a file remember to click on the file name and select &lt;code&gt;Raw&lt;/code&gt; to download the actual file.&lt;/p&gt;
&lt;p&gt;We can use the easy to use &lt;code&gt;read_csv()&lt;/code&gt; function from the &lt;code&gt;readr&lt;/code&gt; package to quickly read in our pizza data directly from the Github repository. Note the &lt;code&gt;raw&lt;/code&gt; at the beginning of the filename.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
pizza &amp;lt;- read_csv(&amp;quot;https://github.com/wipo-analytics/opensource-patent-analytics/blob/master/2_datasets/pizza_medium_clean/pizza.csv?raw=true&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;readr&lt;/code&gt; will display a warning for the file arising from its efforts to parse publication dates on import. We will ignore this as we will not be using this field.&lt;/p&gt;
&lt;p&gt;As an alternative to importing directly from Github download the file and in RStudio use &lt;code&gt;File &amp;gt; Import Dataset &amp;gt; From .csv&lt;/code&gt;. If you experience problems with direct import of a file the File &amp;gt; Import Dataset approach will give you a range of easy to use controls for figuring this out (e.g. where .csv is actually a tab separated file).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;viewing-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Viewing Data&lt;/h2&gt;
&lt;p&gt;We can view data in a variety of ways.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In the console:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pizza&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9,996 x 31
##    applicants_cleaned    applicants_clean… applicants_orga… applicants_original
##    &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;              
##  1 &amp;lt;NA&amp;gt;                  People            &amp;lt;NA&amp;gt;             &amp;lt;NA&amp;gt;               
##  2 Ventimeglia Jamie Jo… People            &amp;lt;NA&amp;gt;             Ventimeglia Jamie …
##  3 Cordova Robert; Mart… People            &amp;lt;NA&amp;gt;             Cordova Robert;Mar…
##  4 Lazarillo De Tormes … Corporate         Lazarillo De To… LAZARILLO DE TORME…
##  5 &amp;lt;NA&amp;gt;                  People            &amp;lt;NA&amp;gt;             &amp;lt;NA&amp;gt;               
##  6 Depoortere, Thomas    People            &amp;lt;NA&amp;gt;             DEPOORTERE, Thomas 
##  7 Frisco Findus Ag      Corporate         Frisco Findus Ag FRISCO-FINDUS AG   
##  8 Bicycle Tools Incorp… Corporate         Bicycle Tools I… Bicycle Tools Inco…
##  9 Castiglioni, Carlo    People            &amp;lt;NA&amp;gt;             CASTIGLIONI, CARLO 
## 10 &amp;lt;NA&amp;gt;                  People            &amp;lt;NA&amp;gt;             &amp;lt;NA&amp;gt;               
## # ... with 9,986 more rows, and 27 more variables: inventors_cleaned &amp;lt;chr&amp;gt;,
## #   inventors_original &amp;lt;chr&amp;gt;, ipc_class &amp;lt;chr&amp;gt;, ipc_codes &amp;lt;chr&amp;gt;,
## #   ipc_names &amp;lt;chr&amp;gt;, ipc_original &amp;lt;chr&amp;gt;, ipc_subclass_codes &amp;lt;chr&amp;gt;,
## #   ipc_subclass_detail &amp;lt;chr&amp;gt;, ipc_subclass_names &amp;lt;chr&amp;gt;,
## #   priority_country_code &amp;lt;chr&amp;gt;, priority_country_code_names &amp;lt;chr&amp;gt;,
## #   priority_data_original &amp;lt;chr&amp;gt;, priority_date &amp;lt;chr&amp;gt;,
## #   publication_country_code &amp;lt;chr&amp;gt;, publication_country_name &amp;lt;chr&amp;gt;,
## #   publication_date &amp;lt;chr&amp;gt;, publication_date_original &amp;lt;chr&amp;gt;,
## #   publication_day &amp;lt;int&amp;gt;, publication_month &amp;lt;int&amp;gt;, publication_number &amp;lt;chr&amp;gt;,
## #   publication_number_espacenet_links &amp;lt;chr&amp;gt;, publication_year &amp;lt;int&amp;gt;,
## #   title_cleaned &amp;lt;chr&amp;gt;, title_nlp_cleaned &amp;lt;chr&amp;gt;,
## #   title_nlp_multiword_phrases &amp;lt;chr&amp;gt;, title_nlp_raw &amp;lt;chr&amp;gt;,
## #   title_original &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;In Environment click on the blue arrow to see in the environment. Keep clicking to open a new window with the data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;View()&lt;/code&gt; command (for data.frames and tables)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;View(pizza)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If possible use the View() command or environment. The difficulty with the console is that large amounts of data will simply stream past.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;identifying-types-of-object&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Identifying Types of Object&lt;/h2&gt;
&lt;p&gt;We often want to know what type of object we are working with and more details about the object so we know what to do later. Here are some of the most common commands for obtaining information about objects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(pizza)  ## type of object
names(pizza)  ## names of variables
str(pizza)  ## structure of object
dim(pizza)  ## dimensions of the object&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most useful command in this list is &lt;code&gt;str()&lt;/code&gt; because this allows us to access the structure of the object and see its type.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(pizza, max.level = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;str()&lt;/code&gt; is particularly useful because we can see the names of the fields (vectors) and their type. Most patent data is a character vector with dates forming integers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;working-with-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Working with Data&lt;/h2&gt;
&lt;p&gt;We will often want to select aspects of our data to focus on a specific set of columns or to create a graph. We might also want to add information, notably numeric counts.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;dplyr&lt;/code&gt; package provides a set of very handy functions for selecting, adding and counting data. The &lt;code&gt;tidyr&lt;/code&gt; and &lt;code&gt;stringr&lt;/code&gt; packages are sister packages that contain a range of other useful functions for working with our data. We have covered some of these in other chapters on graphing using R but will go through them quickly and then pull them together into a function that we can use across our dataset.&lt;/p&gt;
&lt;div id=&#34;select&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Select&lt;/h3&gt;
&lt;p&gt;In this case we will start by using the &lt;code&gt;select()&lt;/code&gt; function to limit the data to specific columns. We can do this using their names or their numeric position (best for large number of columns e.g. 1:31). In &lt;code&gt;dplyr&lt;/code&gt;, unlike most R packages, existing character columns do not require &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
pizza_number &amp;lt;- select(pizza, publication_number, publication_year)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a new data.frame that contains two columns. One with the year and one with the publication number. Note that we have created a new object called pizza_number using &lt;code&gt;&amp;lt;-&lt;/code&gt; and that after &lt;code&gt;select()&lt;/code&gt; we have named our original data and the columns we want. A fundamental feature of select is that it will drop columns that we do not name. So it is best to create a new object using &lt;code&gt;&amp;lt;-&lt;/code&gt; if you want to keep your original data for later work.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-data-with-mutate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding data with &lt;code&gt;mutate()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;mutate()&lt;/code&gt; is a &lt;code&gt;dplyr&lt;/code&gt; function that allows us to add data based on existing data in our data frame, for example to perform a calculation. In the case of patent data we normally lack a numeric field to use for counts. We can however assign a value to our publication field by using sum() and the number 1 as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
pizza_number &amp;lt;- mutate(pizza_number, n = sum(publication_number = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we view &lt;code&gt;pizza_number&lt;/code&gt; we now have a value of 1 in the column &lt;code&gt;n&lt;/code&gt; for each publication number.&lt;/p&gt;
&lt;p&gt;Note that in patent data a priority, application, publication or family number may occur multiple times and we would want to reduce the dataset to distinct records. For that we would use &lt;code&gt;n_distinct(pizza_number$publication_number)&lt;/code&gt; from &lt;code&gt;dplyr&lt;/code&gt; or &lt;code&gt;unique(pizza_number$publication_number)&lt;/code&gt; from base R. Because the publication numbers are unique we can proceed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;counting-data-using-count&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Counting data using &lt;code&gt;count()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;At the moment, we have multiple instances of the same year (where a patent publication occurs in that year). We now want to calculate how many of our documents were published in each year. To do that we will use the &lt;code&gt;dplyr&lt;/code&gt; function &lt;code&gt;count()&lt;/code&gt;. We will use the publication_year and add &lt;code&gt;wt =&lt;/code&gt; (for weight) with &lt;code&gt;n&lt;/code&gt; as the value to count.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
pizza_total &amp;lt;- count(pizza_number, publication_year, wt = n)
pizza_total&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 58 x 2
##    publication_year    nn
##               &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
##  1             1940    1.
##  2             1954    1.
##  3             1956    1.
##  4             1957    1.
##  5             1959    1.
##  6             1962    1.
##  7             1964    2.
##  8             1966    1.
##  9             1967    1.
## 10             1968    8.
## # ... with 48 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we now examine pizza_total, we will see the publication year and a summed value for the records in that year.&lt;/p&gt;
&lt;p&gt;This raises the question of how we know that R has calculated the count correctly. We already know that there are 9996 records in the pizza dataset. To check our count is correct we can simply use sum and select the column we want to sum using &lt;code&gt;$&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
sum(pizza_total$nn)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 9996&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, all is good and we can move on. The &lt;code&gt;$&lt;/code&gt; sign is one of the main ways of subsetting to tell R that we want to work with a specific column (the others are “[” and “[[”).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rename-a-field-with-rename&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Rename a field with &lt;code&gt;rename()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Next we will use &lt;code&gt;rename()&lt;/code&gt; from &lt;code&gt;dplyr&lt;/code&gt; to rename the fields. Note that understanding which field require quote marks can take some effort. In this case renaming the character vector publication_year as “pubyear” requires quotes while renaming the numeric vector “n” does not.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
pizza_total &amp;lt;- rename(pizza_total, pubyear = publication_year, publications = nn) %&amp;gt;% 
    print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 58 x 2
##    pubyear publications
##      &amp;lt;int&amp;gt;        &amp;lt;dbl&amp;gt;
##  1    1940           1.
##  2    1954           1.
##  3    1956           1.
##  4    1957           1.
##  5    1959           1.
##  6    1962           1.
##  7    1964           2.
##  8    1966           1.
##  9    1967           1.
## 10    1968           8.
## # ... with 48 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;make-a-quickplot-with-qplot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Make a quickplot with &lt;code&gt;qplot()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Using the &lt;code&gt;qplot()&lt;/code&gt; function in &lt;code&gt;ggplot2&lt;/code&gt; we can now draw a quick line graph. Note that qplot() is unusual in R because the data (pizza_total) appears after the coordinates. We will specify that we want a line using &lt;code&gt;geom =&lt;/code&gt; (if geom is left out it will be a scatter plot). This will give us an idea of what our plot might look like in our infographic and actions we might want to take on the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
qplot(x = pubyear, y = publications, data = pizza_total, geom = &amp;quot;line&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-04-20-infographics_files/figure-html/qplot-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![](images_foot/infogram/fig1_infographic.png)---&gt;
&lt;p&gt;The plot reveals a data cliff in recent years. This normally reflects a lack of data for the last 2-3 years as recent documents feed through the system en route to publication.&lt;/p&gt;
&lt;p&gt;It is a good idea to remove the data cliff by cutting the data 2-3 years prior to the present. In some cases two years is sufficient, but 3 years is a good rule of thumb.&lt;/p&gt;
&lt;p&gt;We also have long tail of data with limited data from 1940 until the late 1970s. Depending on our purposes with the analysis we might want to keep this data (for historical analysis) or to focus in on a more recent period.&lt;/p&gt;
&lt;p&gt;We will limit our data to specific values using the &lt;code&gt;dplyr&lt;/code&gt; function &lt;code&gt;filter()&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;filter-data-using-filter&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Filter data using &lt;code&gt;filter()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;In contrast with &lt;code&gt;select()&lt;/code&gt; which works with columns, &lt;code&gt;filter()&lt;/code&gt; in &lt;code&gt;dplyr&lt;/code&gt; works with rows. In this case we need to filter on the values in the pubyear column. To remove the data prior to 1990 we will use the greater than or equal to operator &lt;code&gt;&amp;gt;=&lt;/code&gt; on the pubyear column and we will use the less than or equal to &lt;code&gt;&amp;lt;=&lt;/code&gt; operator on the values after 2012.&lt;/p&gt;
&lt;p&gt;One strength of &lt;code&gt;filter()&lt;/code&gt; in &lt;code&gt;dplyr&lt;/code&gt; is that it is easy to filter on multiple values in the same expression (unlike the very similar filter function in base R). The use of &lt;code&gt;filter()&lt;/code&gt; will also remove the 30 records where the year is recorded as NA (Not Available). We will write this file to disk using the simple &lt;code&gt;write_csv()&lt;/code&gt; from &lt;code&gt;readr&lt;/code&gt;. To use &lt;code&gt;write_csv()&lt;/code&gt; we first name our data (&lt;code&gt;pizza_total&lt;/code&gt;) and then provide a file name with a .csv extension. In this case and other examples below we have used a descriptive file name bearing in mind that Windows systems have limitations on the length and type of characters that can be used in file names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(readr)
pizza_total &amp;lt;- filter(pizza_total, pubyear &amp;gt;= 1990, pubyear &amp;lt;= 2012)
write_csv(pizza_total, &amp;quot;pizza_total_1990_2012.csv&amp;quot;)
pizza_total&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 23 x 2
##    pubyear publications
##      &amp;lt;int&amp;gt;        &amp;lt;dbl&amp;gt;
##  1    1990         139.
##  2    1991         154.
##  3    1992         212.
##  4    1993         201.
##  5    1994         162.
##  6    1995         173.
##  7    1996         180.
##  8    1997         186.
##  9    1998         212.
## 10    1999         290.
## # ... with 13 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we print pizza_total to the console we will see that the data now covers the period 1990-2012. When using &lt;code&gt;filter()&lt;/code&gt; on values in this way it is important to remember to apply this filter to any subsequent operations on the data (such as applicants) so that it matches the same data period.&lt;/p&gt;
&lt;p&gt;To see our .csv file we can head over to the Files tab and, assuming that we have created a project, the file will now appear in the list of project files. Clicking on the file name will display the raw unformatted data in RStudio.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;simplify-code-with-pipes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simplify code with pipes &lt;code&gt;%&amp;gt;%&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;So far we have handled the code one line at a time. But, one of the great strengths of using a programming language is that we can run multiple lines of code together. There are two basic ways that we can do this.&lt;/p&gt;
&lt;p&gt;We will create a new temporary object &lt;code&gt;df&lt;/code&gt; to demonstrate this.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The standard way&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)
df &amp;lt;- select(pizza, publication_number, publication_year)
df &amp;lt;- mutate(df, n = sum(publication_number = 1))
df &amp;lt;- count(df, publication_year, wt = n)
df &amp;lt;- rename(df, pubyear = publication_year, publications = nn)
df &amp;lt;- filter(df, pubyear &amp;gt;= 1990, pubyear &amp;lt;= 2012)
qplot(x = pubyear, y = publications, data = df, geom = &amp;quot;line&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code we have just created is six lines long. If we select all of this code and run it in one go it will produce our graph.&lt;/p&gt;
&lt;p&gt;One feature of this code is that each time we run a function on the object total we name it at the start of each function (e.g. mutate(df…)) and then we overwrite the object.&lt;/p&gt;
&lt;p&gt;We can save quite a lot of typing and reduce the complexity of the code using the pipe operator introduced by the the &lt;code&gt;magrittr&lt;/code&gt; package and then adopted in Hadley Wickham’s data wrangling and tidying packages.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Using pipes &lt;code&gt;%&amp;gt;%&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Pipes are now a very popular way of writing R code because they simplify writing R code and speed it up. The most popular pipe is &lt;code&gt;%&amp;gt;%&lt;/code&gt; which means “this” then “that”. In this case we are going to create a new temporary object &lt;code&gt;df1&lt;/code&gt; by first applying select to pizza, then mutate, count, rename and filter. Note that we only name our dataset once (in &lt;code&gt;select()&lt;/code&gt;) and we do not need to keep overwriting the object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)
df1 &amp;lt;- select(pizza, publication_number, publication_year) %&amp;gt;% mutate(n = sum(publication_number = 1)) %&amp;gt;% 
    count(publication_year, wt = n) %&amp;gt;% rename(pubyear = publication_year, publications = nn) %&amp;gt;% 
    filter(pubyear &amp;gt;= 1990, pubyear &amp;lt;= 2012) %&amp;gt;% qplot(x = pubyear, y = publications, 
    data = ., geom = &amp;quot;line&amp;quot;) %&amp;gt;% print()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-04-20-infographics_files/figure-html/piped-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![](images_foot/infogram/fig2_infographic_qplot.png)---&gt;
&lt;p&gt;In the standard code we typed &lt;code&gt;df&lt;/code&gt; nine times to arrive at the same result. Using pipes we typed df1 once. Of greater importance is that the use of pipes simplifies the structure of R code by introducing a basic “this” then “that” logic which makes it easier to understand.&lt;/p&gt;
&lt;p&gt;One point to note about this code is that &lt;code&gt;qplot()&lt;/code&gt; requires us to name our data (in this case &lt;code&gt;df1&lt;/code&gt;). However, &lt;code&gt;df1&lt;/code&gt; is actually the final output of the code and does not exist as an input object before the final line is run. So, if we attempt to use &lt;code&gt;data = df1&lt;/code&gt; in &lt;code&gt;qplot()&lt;/code&gt; we will receive an error message. The way around this is to use &lt;code&gt;.&lt;/code&gt; in place of our data object. That way &lt;code&gt;qplot()&lt;/code&gt; will know we want to graph the outputs of the earlier code. Finally, we need to add an explicit call to &lt;code&gt;print()&lt;/code&gt; to display the graph (without this the code will work but we will not see the graph).&lt;/p&gt;
&lt;p&gt;If we now inspect the structure of the df1 object (using &lt;code&gt;str(df1)&lt;/code&gt;) in the console, it will be a list. The reason for this is that it is an object with mixed components, including a data.frame with our data plus additional data setting out the contents of the plot. As there is no direct link between R and our infographics software this will create problems for us later because the infographics software won’t know how to interpret the list object. So, it is generally a good idea to use a straight data.frame by excluding the call to &lt;code&gt;qplot&lt;/code&gt; and adding it later when needed as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)
df2 &amp;lt;- select(pizza, publication_number, publication_year) %&amp;gt;% mutate(n = sum(publication_number = 1)) %&amp;gt;% 
    count(publication_year, wt = n) %&amp;gt;% rename(pubyear = publication_year, publications = nn) %&amp;gt;% 
    filter(pubyear &amp;gt;= 1990, pubyear &amp;lt;= 2012) %&amp;gt;% print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 23 x 2
##    pubyear publications
##      &amp;lt;int&amp;gt;        &amp;lt;dbl&amp;gt;
##  1    1990         139.
##  2    1991         154.
##  3    1992         212.
##  4    1993         201.
##  5    1994         162.
##  6    1995         173.
##  7    1996         180.
##  8    1997         186.
##  9    1998         212.
## 10    1999         290.
## # ... with 13 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that in this case the only change is that we need to explicitly include the reference to the df2 data frame as the data argument in the call to &lt;code&gt;qplot()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
qplot(x = pubyear, y = publications, data = df2, geom = &amp;quot;line&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-04-20-infographics_files/figure-html/df2_qplot-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;harmonising-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Harmonising data&lt;/h2&gt;
&lt;p&gt;One challenge with creating multiple tables from a baseline dataset is keeping track of subdatasets. At the moment we have two basic objects we will be working with:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;pizza&lt;/code&gt; - our raw dataset&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pizza_total&lt;/code&gt; - created via &lt;code&gt;pizza_number&lt;/code&gt; limited to 1990_2012.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the remainder of the chapter we will want to create some additional datasets from our pizza dataset. These are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Country trends&lt;/li&gt;
&lt;li&gt;Applicants&lt;/li&gt;
&lt;li&gt;International Patent Classification (IPC) Class&lt;/li&gt;
&lt;li&gt;Phrases&lt;/li&gt;
&lt;li&gt;Google&lt;/li&gt;
&lt;li&gt;Google IPC&lt;/li&gt;
&lt;li&gt;Google phrases&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We need to make sure that any data that we generate from our raw dataset matches the period for the &lt;code&gt;pizza_total&lt;/code&gt; dataset. If we do not do this there is a risk that we will generate subdatasets with counts for the raw pizza dataset.&lt;/p&gt;
&lt;p&gt;To handle this we will use &lt;code&gt;filter()&lt;/code&gt; to create a new baseline dataset with an unambiguous name.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
pizza_1990_2012 &amp;lt;- rename(pizza, pubyear = publication_year) %&amp;gt;% filter(pubyear &amp;gt;= 
    1990, pubyear &amp;lt;= 2012)
pizza_1990_2012&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8,262 x 31
##    applicants_cleaned  applicants_clean… applicants_organ… applicants_original 
##    &amp;lt;chr&amp;gt;               &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;               
##  1 &amp;lt;NA&amp;gt;                People            &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;                
##  2 Lazarillo De Torme… Corporate         Lazarillo De Tor… LAZARILLO DE TORMES…
##  3 &amp;lt;NA&amp;gt;                People            &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;                
##  4 Depoortere, Thomas  People            &amp;lt;NA&amp;gt;              DEPOORTERE, Thomas  
##  5 Frisco Findus Ag    Corporate         Frisco Findus Ag  FRISCO-FINDUS AG    
##  6 Bicycle Tools Inco… Corporate         Bicycle Tools In… Bicycle Tools Incor…
##  7 Castiglioni, Carlo  People            &amp;lt;NA&amp;gt;              CASTIGLIONI, CARLO  
##  8 &amp;lt;NA&amp;gt;                People            &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;                
##  9 Bujalski, Wlodzimi… People            &amp;lt;NA&amp;gt;              BUJALSKI, WLODZIMIE…
## 10 Ehrno Flexible A/S… Corporate; People Ehrno Flexible A… &amp;quot;EHRNO FLEXIBLE A/S…
## # ... with 8,252 more rows, and 27 more variables: inventors_cleaned &amp;lt;chr&amp;gt;,
## #   inventors_original &amp;lt;chr&amp;gt;, ipc_class &amp;lt;chr&amp;gt;, ipc_codes &amp;lt;chr&amp;gt;,
## #   ipc_names &amp;lt;chr&amp;gt;, ipc_original &amp;lt;chr&amp;gt;, ipc_subclass_codes &amp;lt;chr&amp;gt;,
## #   ipc_subclass_detail &amp;lt;chr&amp;gt;, ipc_subclass_names &amp;lt;chr&amp;gt;,
## #   priority_country_code &amp;lt;chr&amp;gt;, priority_country_code_names &amp;lt;chr&amp;gt;,
## #   priority_data_original &amp;lt;chr&amp;gt;, priority_date &amp;lt;chr&amp;gt;,
## #   publication_country_code &amp;lt;chr&amp;gt;, publication_country_name &amp;lt;chr&amp;gt;,
## #   publication_date &amp;lt;chr&amp;gt;, publication_date_original &amp;lt;chr&amp;gt;,
## #   publication_day &amp;lt;int&amp;gt;, publication_month &amp;lt;int&amp;gt;, publication_number &amp;lt;chr&amp;gt;,
## #   publication_number_espacenet_links &amp;lt;chr&amp;gt;, pubyear &amp;lt;int&amp;gt;,
## #   title_cleaned &amp;lt;chr&amp;gt;, title_nlp_cleaned &amp;lt;chr&amp;gt;,
## #   title_nlp_multiword_phrases &amp;lt;chr&amp;gt;, title_nlp_raw &amp;lt;chr&amp;gt;,
## #   title_original &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case we start with a call to &lt;code&gt;rename()&lt;/code&gt; to make this consistent with our pizza_total table and then use a pipe to filter the data on the year. Note here that when filtering raw data on a set of values it is important to inspect it first to check that the field is clean (e.g. not concatenated). If for some reason your data is concatenated (which happens quite a lot with patent data) then lookup &lt;code&gt;?tidyr::separate_rows&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We are now in a position to create our country trends table.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;country-trends-using-spread&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Country Trends using &lt;code&gt;spread()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;There are two basic data formats: long and wide. Our pizza dataset is in long format because each column is a variable (e.g. &lt;code&gt;publication_country&lt;/code&gt;) and each row in &lt;code&gt;publication_country&lt;/code&gt; contains a country name. This is the most common and useful data format.&lt;/p&gt;
&lt;p&gt;However, in some cases, such as &lt;code&gt;infogr.am&lt;/code&gt; our visualisation software will expect the data to be in wide format. In this case each country name would become a variable (column name) with the years forming the rows and the number of records per year the observations. The key to this is the &lt;code&gt;tidyr()&lt;/code&gt; function &lt;code&gt;spread()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As above we will start off by using &lt;code&gt;select()&lt;/code&gt; to create a table with the fields that we want. We will then use &lt;code&gt;mutate()&lt;/code&gt; to add a numeric field and then count up that data. To illustrate the process run this code (we will not create an object).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
select(pizza_1990_2012, publication_country_name, publication_number, pubyear) %&amp;gt;% 
    mutate(n = sum(publication_number = 1)) %&amp;gt;% count(publication_country_name, pubyear, 
    wt = n) %&amp;gt;% print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 223 x 3
##    publication_country_name pubyear    nn
##    &amp;lt;chr&amp;gt;                      &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
##  1 Canada                      1990   19.
##  2 Canada                      1991   49.
##  3 Canada                      1992   66.
##  4 Canada                      1993   59.
##  5 Canada                      1994   50.
##  6 Canada                      1995   39.
##  7 Canada                      1996   36.
##  8 Canada                      1997   45.
##  9 Canada                      1998   46.
## 10 Canada                      1999   47.
## # ... with 213 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we run this code we will see the results in long format. We now want to take our &lt;code&gt;publication_country_name&lt;/code&gt; column and spread it to form columns with &lt;code&gt;nn&lt;/code&gt; as the values.&lt;/p&gt;
&lt;p&gt;In using spread note that it takes a data argument (&lt;code&gt;pizza_1990_2012&lt;/code&gt;), a key (&lt;code&gt;publication_country_name&lt;/code&gt;), and value column (&lt;code&gt;nn&lt;/code&gt;) (created from &lt;code&gt;count()&lt;/code&gt;). We are using pipes so the data only needs to be named in the first line. For additional arguments see &lt;code&gt;?spread()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidyr)
country_totals &amp;lt;- select(pizza_1990_2012, publication_country_name, publication_number, 
    pubyear) %&amp;gt;% mutate(n = sum(publication_number = 1)) %&amp;gt;% count(publication_country_name, 
    pubyear, wt = n) %&amp;gt;% spread(publication_country_name, nn)
country_totals&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 23 x 17
##    pubyear Canada China `Eurasian Patent… `European Paten… Germany Israel Japan
##      &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1    1990    19.   NA                NA               22.      2.    NA    NA 
##  2    1991    49.   NA                NA               29.      2.    NA    NA 
##  3    1992    66.   NA                NA               36.      2.     1.   NA 
##  4    1993    59.   NA                NA               29.      2.    NA    NA 
##  5    1994    50.   NA                NA               26.      5.    NA    NA 
##  6    1995    39.   NA                NA               29.      2.     1.   NA 
##  7    1996    36.    1.               NA               27.      1.     1.   NA 
##  8    1997    45.   NA                NA               34.      1.    NA    NA 
##  9    1998    46.   NA                NA               36.      1.    NA    17.
## 10    1999    47.    2.                2.              60.      4.    NA    26.
## # ... with 13 more rows, and 9 more variables: `Korea, Republic of` &amp;lt;dbl&amp;gt;,
## #   Mexico &amp;lt;dbl&amp;gt;, `Patent Co-operation Treaty` &amp;lt;dbl&amp;gt;, Portugal &amp;lt;dbl&amp;gt;, `Russian
## #   Federation` &amp;lt;dbl&amp;gt;, Singapore &amp;lt;dbl&amp;gt;, `South Africa` &amp;lt;dbl&amp;gt;, Spain &amp;lt;dbl&amp;gt;,
## #   `United States of America` &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have data in wide format.&lt;/p&gt;
&lt;p&gt;In some cases, such as infogr.am, visualisation software may expect the country names to be the name of rows and the column names to be years . We can modify our call to &lt;code&gt;spread()&lt;/code&gt; by replacing the &lt;code&gt;publication_country_name&lt;/code&gt; with &lt;code&gt;pubyear&lt;/code&gt;. Then we will write the data to disk for use in our infographic.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(readr)
country_totals &amp;lt;- select(pizza_1990_2012, publication_country_name, publication_number, pubyear) %&amp;gt;%
  mutate(n = sum(publication_number = 1)) %&amp;gt;% 
  count(publication_country_name, pubyear, wt = n) %&amp;gt;% # note n
  spread(pubyear, nn) # note nn
country_totals&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 16 x 24
##    publication_country… `1990` `1991` `1992` `1993` `1994` `1995` `1996` `1997`
##    &amp;lt;chr&amp;gt;                 &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1 Canada                  19.    49.    66.    59.    50.    39.    36.    45.
##  2 China                   NA     NA     NA     NA     NA     NA      1.    NA 
##  3 Eurasian Patent Org…    NA     NA     NA     NA     NA     NA     NA     NA 
##  4 European Patent Off…    22.    29.    36.    29.    26.    29.    27.    34.
##  5 Germany                  2.     2.     2.     2.     5.     2.     1.     1.
##  6 Israel                  NA     NA      1.    NA     NA      1.     1.    NA 
##  7 Japan                   NA     NA     NA     NA     NA     NA     NA     NA 
##  8 Korea, Republic of      NA     NA     NA      1.    NA     NA      1.     1.
##  9 Mexico                  NA     NA     NA     NA     NA     NA     NA     NA 
## 10 Patent Co-operation…     8.    13.    31.    16.    20.    22.    23.    26.
## 11 Portugal                NA     NA     NA     NA     NA     NA     NA     NA 
## 12 Russian Federation      NA     NA     NA     NA     NA     NA     NA      5.
## 13 Singapore               NA     NA     NA     NA     NA     NA     NA     NA 
## 14 South Africa             2.     3.     3.     3.     3.     1.     9.     7.
## 15 Spain                   NA     NA     NA     NA     NA     NA     NA     NA 
## 16 United States of Am…    86.    58.    73.    91.    58.    79.    81.    67.
## # ... with 15 more variables: `1998` &amp;lt;dbl&amp;gt;, `1999` &amp;lt;dbl&amp;gt;, `2000` &amp;lt;dbl&amp;gt;,
## #   `2001` &amp;lt;dbl&amp;gt;, `2002` &amp;lt;dbl&amp;gt;, `2003` &amp;lt;dbl&amp;gt;, `2004` &amp;lt;dbl&amp;gt;, `2005` &amp;lt;dbl&amp;gt;,
## #   `2006` &amp;lt;dbl&amp;gt;, `2007` &amp;lt;dbl&amp;gt;, `2008` &amp;lt;dbl&amp;gt;, `2009` &amp;lt;dbl&amp;gt;, `2010` &amp;lt;dbl&amp;gt;,
## #   `2011` &amp;lt;dbl&amp;gt;, `2012` &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(country_totals, &amp;quot;pizza_country_1990_2012.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To restore the data to long format we would need to use &lt;code&gt;gather()&lt;/code&gt; as the counterpart to &lt;code&gt;spread()&lt;/code&gt;. &lt;code&gt;gather()&lt;/code&gt; takes a dataset, a key for the name of the column we want to gather the countries into, a value for the numeric count (in this case n), and finally the positions of the columns to gather in. Note here that we need to look up the column positions in &lt;code&gt;country_totals&lt;/code&gt; (e.g. using &lt;code&gt;View()&lt;/code&gt;) or count the columns using &lt;code&gt;ncol(country_totals)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
gather(country_totals, year, n, 2:24) %&amp;gt;% print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 368 x 3
##    publication_country_name     year      n
##    &amp;lt;chr&amp;gt;                        &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
##  1 Canada                       1990    19.
##  2 China                        1990    NA 
##  3 Eurasian Patent Organization 1990    NA 
##  4 European Patent Office       1990    22.
##  5 Germany                      1990     2.
##  6 Israel                       1990    NA 
##  7 Japan                        1990    NA 
##  8 Korea, Republic of           1990    NA 
##  9 Mexico                       1990    NA 
## 10 Patent Co-operation Treaty   1990     8.
## # ... with 358 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The combination of spread and gather work really well to prepare data in formats that are expected by other software. However, one of the main issues we encounter with patent data is that our data is not tidy because various fields are concatenated.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidying-data---separating-and-gathering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidying data - Separating and Gathering&lt;/h2&gt;
&lt;p&gt;In patent data we often see concatenated fields with a separator (normally a &lt;code&gt;;&lt;/code&gt;). These are typically applicant names, inventor names, International Patent Classification (IPC) codes, or document numbers (priority numbers, family numbers). We need to &lt;code&gt;tidy&lt;/code&gt; this data prior to data cleaning (such as cleaning names) or to prepare for analysis and visualisation. For more on the concept of tidy data read &lt;a href=&#34;http://vita.had.co.nz/papers/tidy-data.pdf&#34;&gt;Hadley Wickham’s Tidy Data article&lt;/a&gt;. The new &lt;a href=&#34;http://r4ds.had.co.nz/tidy-data.html&#34;&gt;R for Data Science book&lt;/a&gt; by Garrett Grolemund and Hadley Wickham (see Chapter 12) is also strongly recommended.&lt;/p&gt;
&lt;p&gt;To tidy patent data we will typically need to do two things.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Separate the data so that each cell contains a unique data point (e.g. a name, code or publication number). This normally involves separating data into columns.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Gathering the data back in. This involves transforming the data in the columns we have created into rows.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Separating data into columns is very easy in tools such as Excel. However, gathering the data back into separate rows is remarkably difficult. Happily, this is very easy to do in R with the &lt;code&gt;tidyr&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;tidyr&lt;/code&gt; package contains three functions that are very useful when working with patent data. When dealing with concatenated fields in columns the key function is &lt;code&gt;separate_rows&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here we will work with the &lt;code&gt;applicants_cleaned&lt;/code&gt; field in the pizza dataset. This field contains concatenated names with a &lt;code&gt;;&lt;/code&gt; as the separator. For example, on lines 1_9 there are single applicant names or NA values. However, on lines 10 and line 59 we see:&lt;/p&gt;
&lt;p&gt;Ehrno Flexible A/S; Stergaard, Ole
Farrell Brian; Mcnulty John; Vishoot Lisa&lt;/p&gt;
&lt;p&gt;The problem here is that when we are dealing with thousands of lines of applicant names we don’t know how many names might be concatenated into each cell as a basis for separating the data into columns. Once we had split the columns (for example using Text to Columns in Excel) we would then need to work out how to gather the columns into rows. The &lt;code&gt;separate_rows()&lt;/code&gt; function from &lt;code&gt;tidyr&lt;/code&gt; makes light work of this problem. To use the function we name the dataset, the column we want to separate into rows and the separator (sep).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidyr)
pizza1 &amp;lt;- separate_rows(pizza_1990_2012, applicants_cleaned, sep = &amp;quot;;&amp;quot;)
pizza1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12,729 x 31
##    applicants_cleaned applicants_cleane… applicants_organ… applicants_original 
##    &amp;lt;chr&amp;gt;              &amp;lt;chr&amp;gt;              &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;               
##  1 &amp;lt;NA&amp;gt;               People             &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;                
##  2 Lazarillo De Torm… Corporate          Lazarillo De Tor… LAZARILLO DE TORMES…
##  3 &amp;lt;NA&amp;gt;               People             &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;                
##  4 Depoortere, Thomas People             &amp;lt;NA&amp;gt;              DEPOORTERE, Thomas  
##  5 Frisco Findus Ag   Corporate          Frisco Findus Ag  FRISCO-FINDUS AG    
##  6 Bicycle Tools Inc… Corporate          Bicycle Tools In… Bicycle Tools Incor…
##  7 Castiglioni, Carlo People             &amp;lt;NA&amp;gt;              CASTIGLIONI, CARLO  
##  8 &amp;lt;NA&amp;gt;               People             &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;                
##  9 Bujalski, Wlodzim… People             &amp;lt;NA&amp;gt;              BUJALSKI, WLODZIMIE…
## 10 Ehrno Flexible A/S Corporate; People  Ehrno Flexible A… &amp;quot;EHRNO FLEXIBLE A/S…
## # ... with 12,719 more rows, and 27 more variables: inventors_cleaned &amp;lt;chr&amp;gt;,
## #   inventors_original &amp;lt;chr&amp;gt;, ipc_class &amp;lt;chr&amp;gt;, ipc_codes &amp;lt;chr&amp;gt;,
## #   ipc_names &amp;lt;chr&amp;gt;, ipc_original &amp;lt;chr&amp;gt;, ipc_subclass_codes &amp;lt;chr&amp;gt;,
## #   ipc_subclass_detail &amp;lt;chr&amp;gt;, ipc_subclass_names &amp;lt;chr&amp;gt;,
## #   priority_country_code &amp;lt;chr&amp;gt;, priority_country_code_names &amp;lt;chr&amp;gt;,
## #   priority_data_original &amp;lt;chr&amp;gt;, priority_date &amp;lt;chr&amp;gt;,
## #   publication_country_code &amp;lt;chr&amp;gt;, publication_country_name &amp;lt;chr&amp;gt;,
## #   publication_date &amp;lt;chr&amp;gt;, publication_date_original &amp;lt;chr&amp;gt;,
## #   publication_day &amp;lt;int&amp;gt;, publication_month &amp;lt;int&amp;gt;, publication_number &amp;lt;chr&amp;gt;,
## #   publication_number_espacenet_links &amp;lt;chr&amp;gt;, pubyear &amp;lt;int&amp;gt;,
## #   title_cleaned &amp;lt;chr&amp;gt;, title_nlp_cleaned &amp;lt;chr&amp;gt;,
## #   title_nlp_multiword_phrases &amp;lt;chr&amp;gt;, title_nlp_raw &amp;lt;chr&amp;gt;,
## #   title_original &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our original dataset contained 8,262 rows. Our new dataset split on applicant names contains 12,729 rows. The function has moved our target column from column 1 to column 31 in the data frame. We can easily move it back to inspect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
pizza1 &amp;lt;- select(pizza1, 31, 1:30)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;separate_rows()&lt;/code&gt; has done a great job but one of the problems with concatenated names is extra white space around the separator. We will deal with this next.&lt;/p&gt;
&lt;div id=&#34;trimming-with-stringr&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Trimming with &lt;code&gt;stringr&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;If we inspect the bottom of the column by subsetting into it using &lt;code&gt;$&lt;/code&gt; we will see that a lot of the names have a leading whitespace space. This results from the separate exercise where the &lt;code&gt;;&lt;/code&gt; is actually &lt;code&gt;;space&lt;/code&gt;. Take a look at the last few rows of the data using &lt;code&gt;tail()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tail(pizza1$applicants_cleaned, 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Yahoo! Inc&amp;quot;                        &amp;quot;Clarcor Inc&amp;quot;                      
##  [3] &amp;quot;Holden Jeffrey A&amp;quot;                  &amp;quot; Vengroff Darren E&amp;quot;               
##  [5] &amp;quot;Casper Jeffrey L&amp;quot;                  &amp;quot; Erickson Braden J&amp;quot;               
##  [7] &amp;quot; Oppenheimer Alan A&amp;quot;               &amp;quot; Ray Madonna M&amp;quot;                   
##  [9] &amp;quot; Weber Jean L&amp;quot;                     &amp;quot;Pandey Neena&amp;quot;                     
## [11] &amp;quot; Sharma Sudhanshu&amp;quot;                 &amp;quot; Verizon Patent And Licensing Inc&amp;quot;
## [13] &amp;quot;Pandey Neena&amp;quot;                      &amp;quot; Sharma Sudhanshu&amp;quot;                
## [15] &amp;quot;Brown Michael&amp;quot;                     &amp;quot; Urban Scott&amp;quot;                     
## [17] &amp;quot;Brown Michael&amp;quot;                     &amp;quot; Urban Scott&amp;quot;                     
## [19] &amp;quot;Cole Lorin R&amp;quot;                      &amp;quot; Middleton Scott W&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a big issue because any counts that we make later on using the applicants_cleaned field will treat “Oppenheimer Alan A” and &amp;quot; Oppenheimer Alan A&amp;quot; as separate names when they should be grouped together.&lt;/p&gt;
&lt;p&gt;We can address this in a couple of ways. One approach is to recognise that actually our separator is not a simple &lt;code&gt;&amp;quot;;&amp;quot;&lt;/code&gt; but &lt;code&gt;&amp;quot;;space&amp;quot;&lt;/code&gt; in our call to &lt;code&gt;separate_rows()&lt;/code&gt;. In that case the call to &lt;code&gt;separate_rows()&lt;/code&gt; would actually be &lt;code&gt;sep = &amp;quot;; &amp;quot;&lt;/code&gt;. We will add a line of code to illustrate the impact of this change.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmp &amp;lt;- separate_rows(pizza_1990_2012, applicants_cleaned, sep = &amp;quot;; &amp;quot;)
tail(tmp$applicants_cleaned, 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Yahoo! Inc&amp;quot;                       &amp;quot;Clarcor Inc&amp;quot;                     
##  [3] &amp;quot;Holden Jeffrey A&amp;quot;                 &amp;quot;Vengroff Darren E&amp;quot;               
##  [5] &amp;quot;Casper Jeffrey L&amp;quot;                 &amp;quot;Erickson Braden J&amp;quot;               
##  [7] &amp;quot;Oppenheimer Alan A&amp;quot;               &amp;quot;Ray Madonna M&amp;quot;                   
##  [9] &amp;quot;Weber Jean L&amp;quot;                     &amp;quot;Pandey Neena&amp;quot;                    
## [11] &amp;quot;Sharma Sudhanshu&amp;quot;                 &amp;quot;Verizon Patent And Licensing Inc&amp;quot;
## [13] &amp;quot;Pandey Neena&amp;quot;                     &amp;quot;Sharma Sudhanshu&amp;quot;                
## [15] &amp;quot;Brown Michael&amp;quot;                    &amp;quot;Urban Scott&amp;quot;                     
## [17] &amp;quot;Brown Michael&amp;quot;                    &amp;quot;Urban Scott&amp;quot;                     
## [19] &amp;quot;Cole Lorin R&amp;quot;                     &amp;quot;Middleton Scott W&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to address this, is to use the &lt;code&gt;str_trim()&lt;/code&gt; function from the &lt;code&gt;stringr&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;We can address this problem using a function from the &lt;code&gt;stringr&lt;/code&gt; package &lt;code&gt;str_trim()&lt;/code&gt;. We have a choice with &lt;code&gt;str_trim()&lt;/code&gt; on whether to trim the whitespace on the right, left or both. Here we will choose both.&lt;/p&gt;
&lt;p&gt;Because we are seeking to modify an existing column (not to create a new vector or data.frame) we will use &lt;code&gt;$&lt;/code&gt; to select the column and as the data for the &lt;code&gt;str_trim()&lt;/code&gt; function. That will apply the function to the applicants column in pizza1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stringr)
pizza1$applicants_cleaned &amp;lt;- str_trim(pizza1$applicants_cleaned, side = &amp;quot;both&amp;quot;)
tail(pizza1$applicants_cleaned, 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Yahoo! Inc&amp;quot;                       &amp;quot;Clarcor Inc&amp;quot;                     
##  [3] &amp;quot;Holden Jeffrey A&amp;quot;                 &amp;quot;Vengroff Darren E&amp;quot;               
##  [5] &amp;quot;Casper Jeffrey L&amp;quot;                 &amp;quot;Erickson Braden J&amp;quot;               
##  [7] &amp;quot;Oppenheimer Alan A&amp;quot;               &amp;quot;Ray Madonna M&amp;quot;                   
##  [9] &amp;quot;Weber Jean L&amp;quot;                     &amp;quot;Pandey Neena&amp;quot;                    
## [11] &amp;quot;Sharma Sudhanshu&amp;quot;                 &amp;quot;Verizon Patent And Licensing Inc&amp;quot;
## [13] &amp;quot;Pandey Neena&amp;quot;                     &amp;quot;Sharma Sudhanshu&amp;quot;                
## [15] &amp;quot;Brown Michael&amp;quot;                    &amp;quot;Urban Scott&amp;quot;                     
## [17] &amp;quot;Brown Michael&amp;quot;                    &amp;quot;Urban Scott&amp;quot;                     
## [19] &amp;quot;Cole Lorin R&amp;quot;                     &amp;quot;Middleton Scott W&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that when using &lt;code&gt;str_trim()&lt;/code&gt; we use subsetting to modify the applicants column in place. There is possibly a more efficient way of doing this with pipes but this appears difficult because the data.frame needs to exist for &lt;code&gt;str_trim()&lt;/code&gt; to act on in place or we end up with a vector of applicant names rather than a data.frame. A solution to this problem is provided on Stack Overflow&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In practice, the most efficient solution in this case is to recognise that the separator for &lt;code&gt;separate_rows&lt;/code&gt; is &lt;code&gt;&amp;quot;;space&amp;quot;&lt;/code&gt;. However, that will not always be true making the tools in &lt;code&gt;stringr&lt;/code&gt; invaluable. To learn more about string manipulation in R try &lt;a href=&#34;http://r4ds.had.co.nz/strings.html&#34;&gt;Chapter 14 of R for Data Science by Garrett Grolemund and Hadley Wickham&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can tie the steps so far together using pipes into the following simpler code that we will become the applicants table for use in the infographic. We will add a call to rename and rename applicants_cleaned to tidy up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidyr)
library(stringr)
applicants &amp;lt;- rename(pizza, pubyear = publication_year) %&amp;gt;% filter(pubyear &amp;gt;= 1990, 
    pubyear &amp;lt;= 2012) %&amp;gt;% separate_rows(applicants_cleaned, sep = &amp;quot;; &amp;quot;) %&amp;gt;% rename(applicants = applicants_cleaned) %&amp;gt;% 
    select(31, 1:30)  # moves separated column to the beginning
applicants&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12,729 x 31
##    title_original applicants applicants_clea… applicants_orga… applicants_orig…
##    &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;           
##  1 PIZZA          &amp;lt;NA&amp;gt;       People           &amp;lt;NA&amp;gt;             &amp;lt;NA&amp;gt;            
##  2 IMPROVED PIZZA Lazarillo… Corporate        Lazarillo De To… LAZARILLO DE TO…
##  3 Pizza separat… &amp;lt;NA&amp;gt;       People           &amp;lt;NA&amp;gt;             &amp;lt;NA&amp;gt;            
##  4 Pizza separat… Depoorter… People           &amp;lt;NA&amp;gt;             DEPOORTERE, Tho…
##  5 PIZZA PREPARA… Frisco Fi… Corporate        Frisco Findus Ag FRISCO-FINDUS AG
##  6 Pizza Cutter   Bicycle T… Corporate        Bicycle Tools I… Bicycle Tools I…
##  7 PIZZA BOX      Castiglio… People           &amp;lt;NA&amp;gt;             CASTIGLIONI, CA…
##  8 PIZZA BOX      &amp;lt;NA&amp;gt;       People           &amp;lt;NA&amp;gt;             &amp;lt;NA&amp;gt;            
##  9 MORE ATTRACTI… Bujalski,… People           &amp;lt;NA&amp;gt;             BUJALSKI, WLODZ…
## 10 PIZZA PACKAGI… Ehrno Fle… Corporate; Peop… Ehrno Flexible … &amp;quot;EHRNO FLEXIBLE…
## # ... with 12,719 more rows, and 26 more variables: inventors_cleaned &amp;lt;chr&amp;gt;,
## #   inventors_original &amp;lt;chr&amp;gt;, ipc_class &amp;lt;chr&amp;gt;, ipc_codes &amp;lt;chr&amp;gt;,
## #   ipc_names &amp;lt;chr&amp;gt;, ipc_original &amp;lt;chr&amp;gt;, ipc_subclass_codes &amp;lt;chr&amp;gt;,
## #   ipc_subclass_detail &amp;lt;chr&amp;gt;, ipc_subclass_names &amp;lt;chr&amp;gt;,
## #   priority_country_code &amp;lt;chr&amp;gt;, priority_country_code_names &amp;lt;chr&amp;gt;,
## #   priority_data_original &amp;lt;chr&amp;gt;, priority_date &amp;lt;chr&amp;gt;,
## #   publication_country_code &amp;lt;chr&amp;gt;, publication_country_name &amp;lt;chr&amp;gt;,
## #   publication_date &amp;lt;chr&amp;gt;, publication_date_original &amp;lt;chr&amp;gt;,
## #   publication_day &amp;lt;int&amp;gt;, publication_month &amp;lt;int&amp;gt;, publication_number &amp;lt;chr&amp;gt;,
## #   publication_number_espacenet_links &amp;lt;chr&amp;gt;, pubyear &amp;lt;int&amp;gt;,
## #   title_cleaned &amp;lt;chr&amp;gt;, title_nlp_cleaned &amp;lt;chr&amp;gt;,
## #   title_nlp_multiword_phrases &amp;lt;chr&amp;gt;, title_nlp_raw &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will want to create a plot with the applicants data in our infographic software. For that we need to introduce a field to count on. We might also want to establish a cut off point based on the number of records per applicant.&lt;/p&gt;
&lt;p&gt;In this code we will simply print the applicants ranked in descending order. The second to last line of the code provides a filter on the number of records. This value can be changed after inspecting the data. The final line omits NA values (otherwise the top result) where an applicant name is not available.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)
library(dplyr)
applicant_count &amp;lt;- select(applicants, applicants, publication_number) %&amp;gt;% mutate(n = sum(publication_number = 1)) %&amp;gt;% 
    count(applicants, wt = n) %&amp;gt;% arrange(desc(nn)) %&amp;gt;% filter(nn &amp;gt;= 1) %&amp;gt;% na.omit()
applicant_count&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6,178 x 2
##    applicants                              nn
##    &amp;lt;chr&amp;gt;                                &amp;lt;dbl&amp;gt;
##  1 Graphic Packaging International, Inc  154.
##  2 Kraft Foods Holdings, Inc             132.
##  3 Google Inc                            123.
##  4 Microsoft Corporation                  88.
##  5 The Pillsbury Company                  83.
##  6 General Mills, Inc                     77.
##  7 Nestec                                 77.
##  8 The Procter &amp;amp; Gamble Company           59.
##  9 Pizza Hut, Inc                         57.
## 10 Yahoo! Inc                             54.
## # ... with 6,168 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we inspect applicant count using &lt;code&gt;View(applicant_count)&lt;/code&gt; we have 6,178 rows. That is far too many to display in an infographic. So, next we will filter the data on the value for the top ten (54). Then we will write the data to a .csv file using the simple &lt;code&gt;write_csv()&lt;/code&gt; from &lt;code&gt;readr&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidyr)
library(readr)
applicant_count &amp;lt;- select(applicants, applicants, publication_number) %&amp;gt;% mutate(n = sum(publication_number = 1)) %&amp;gt;% 
    count(applicants, wt = n) %&amp;gt;% arrange(desc(nn)) %&amp;gt;% filter(nn &amp;gt;= 54) %&amp;gt;% na.omit()
applicant_count&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##    applicants                              nn
##    &amp;lt;chr&amp;gt;                                &amp;lt;dbl&amp;gt;
##  1 Graphic Packaging International, Inc  154.
##  2 Kraft Foods Holdings, Inc             132.
##  3 Google Inc                            123.
##  4 Microsoft Corporation                  88.
##  5 The Pillsbury Company                  83.
##  6 General Mills, Inc                     77.
##  7 Nestec                                 77.
##  8 The Procter &amp;amp; Gamble Company           59.
##  9 Pizza Hut, Inc                         57.
## 10 Yahoo! Inc                             54.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(applicant_count, &amp;quot;pizza_applicants_1990_2012.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we inspect &lt;code&gt;applicant_count&lt;/code&gt; we will see that Graphic Packaging International is the top result with 154 results with Google ranking third with 123 results followed by Microsoft. This could suggest that Google and Microsoft are suddenly entering the market for online pizza sales or pizza making software or, as is more likely, that there are uses other uses of the word pizza in patent data that we are not aware of.&lt;/p&gt;
&lt;p&gt;As part of our infographic we will want to explore this intriguing result in more detail. We can do this by creating a subdataset for Google using &lt;code&gt;filter()&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;selecting-applicants-using-filter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Selecting applicants using &lt;code&gt;filter()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;As we saw above, while &lt;code&gt;select()&lt;/code&gt; functions with columns, &lt;code&gt;filter()&lt;/code&gt; from &lt;code&gt;dplyr&lt;/code&gt; works with rows. Here we will filter the data to select the rows in the applicants column that contain Google Inc. and then write that to a .csv for use in our infographic. Note the use of double &lt;code&gt;==&lt;/code&gt; and the quotes around “Google Inc”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(readr)
google &amp;lt;- filter(applicants, applicants == &amp;quot;Google Inc&amp;quot;)
google&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 123 x 31
##    title_original applicants applicants_clea… applicants_orga… applicants_orig…
##    &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;           
##  1 Location base… Google Inc Corporate; Peop… Google Inc       Hafsteinsson Gu…
##  2 AUTHORITATIVE… Google Inc Corporate        Google Inc       GOOGLE INC.     
##  3 Location-Base… Google Inc Corporate; Peop… Google Inc       GOOGLE INC.;HAF…
##  4 Controlling t… Google Inc Corporate; Peop… Google Inc       GOOGLE, INC.;BE…
##  5 METHOD AND SY… Google Inc Corporate        Google Inc       GOOGLE INC.     
##  6 Routing queri… Google Inc Corporate        Google Inc       GOOGLE INC.     
##  7 METHODS AND S… Google Inc Corporate        Google Inc       GOOGLE INC.     
##  8 Aspect-based … Google Inc Corporate; Peop… Google Inc       Reis George;Goo…
##  9 Interpreting … Google Inc Corporate        Google Inc       GOOGLE INC.     
## 10 Interpreting … Google Inc Corporate        Google Inc       GOOGLE INC.     
## # ... with 113 more rows, and 26 more variables: inventors_cleaned &amp;lt;chr&amp;gt;,
## #   inventors_original &amp;lt;chr&amp;gt;, ipc_class &amp;lt;chr&amp;gt;, ipc_codes &amp;lt;chr&amp;gt;,
## #   ipc_names &amp;lt;chr&amp;gt;, ipc_original &amp;lt;chr&amp;gt;, ipc_subclass_codes &amp;lt;chr&amp;gt;,
## #   ipc_subclass_detail &amp;lt;chr&amp;gt;, ipc_subclass_names &amp;lt;chr&amp;gt;,
## #   priority_country_code &amp;lt;chr&amp;gt;, priority_country_code_names &amp;lt;chr&amp;gt;,
## #   priority_data_original &amp;lt;chr&amp;gt;, priority_date &amp;lt;chr&amp;gt;,
## #   publication_country_code &amp;lt;chr&amp;gt;, publication_country_name &amp;lt;chr&amp;gt;,
## #   publication_date &amp;lt;chr&amp;gt;, publication_date_original &amp;lt;chr&amp;gt;,
## #   publication_day &amp;lt;int&amp;gt;, publication_month &amp;lt;int&amp;gt;, publication_number &amp;lt;chr&amp;gt;,
## #   publication_number_espacenet_links &amp;lt;chr&amp;gt;, pubyear &amp;lt;int&amp;gt;,
## #   title_cleaned &amp;lt;chr&amp;gt;, title_nlp_cleaned &amp;lt;chr&amp;gt;,
## #   title_nlp_multiword_phrases &amp;lt;chr&amp;gt;, title_nlp_raw &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(google, &amp;quot;google_1990_2012.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the correct result for the period 1990 to 2012 for Google is 123 records from 191 records across the whole pizza dataset. The correct result will be achieved only where you use the filtered, separated and trimmed data we created in the applicants data frame.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generating-ipc-tables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generating IPC Tables&lt;/h2&gt;
&lt;p&gt;In the next step we will want to generate two tables containing International Patent Classification (IPC) data. IPC codes and the Cooperative Patent Classification (CPC, not present in this dataset) provide information on the technologies involved in a patent document. The IPC is hierarchical and proceeds from the general class level to the detailed group and subgroup level. Experience reveals that the majority of patent documents receive more than one IPC code to more fully describe the technological aspects of patent documents.&lt;/p&gt;
&lt;p&gt;The pizza dataset contains IPC codes on the class and the subclass level in concatenated fields. One important consideration in using IPC data is that the descriptions are long and can be difficult for non-specialists to grasp. This can make visualising the data difficult and often requires manual efforts to edit labels for display.&lt;/p&gt;
&lt;p&gt;We now want to generate three IPC tables.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A general IPC table for the pizza dataset&lt;/li&gt;
&lt;li&gt;A general IPC table for the Google dataset&lt;/li&gt;
&lt;li&gt;A more detailed IPC subclass table for the Google dataset&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For ease of presentation in an infographic we will use the &lt;code&gt;ipc_class&lt;/code&gt; field. For many patent analytics purposes this will be too general. However it has the advantage of being easy to visualise.&lt;/p&gt;
&lt;p&gt;To generate the table we can use a generic function based on the code developed for dealing with the applicants data. We will call the function patent_count().&lt;/p&gt;
&lt;!--- updated to tidyeval in 2018. Note that the whitespace was not trimmed in the earlier version due to an oversight and counts will now be higher.. and correct... as a result. ---&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;patent_count &amp;lt;- function(data, col, count_col, sep, n_results) {
    p_count &amp;lt;- dplyr::select(data, !!col, !!count_col) %&amp;gt;% tidyr::separate_rows(col, 
        sep = sep) %&amp;gt;% dplyr::mutate(`:=`(!!col, stringr::str_trim(.[[col]], side = &amp;quot;both&amp;quot;))) %&amp;gt;% 
        dplyr::mutate(n = sum(count_col = 1)) %&amp;gt;% dplyr::group_by(`:=`(!!col, .[[col]])) %&amp;gt;% 
        dplyr::tally(sort = TRUE) %&amp;gt;% dplyr::rename(records = nn) %&amp;gt;% na.omit() %&amp;gt;% 
        head(n_results)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;patent_count()&lt;/code&gt; function is based on the the code we developed for applicants. It contains variations to make it work as a function. The function takes four arguments:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;col = the concatenated column that we want to split and gather back in&lt;/li&gt;
&lt;li&gt;col_count = a column for generating counts (in this dataset the publication_number)&lt;/li&gt;
&lt;li&gt;n_results = the number of results we want to see in the new table (typically 10 or 20 for visualisation). This is equivalent to the number of rows that you want to see.&lt;/li&gt;
&lt;li&gt;sep = the separator to use to separate the data in col. With patent data this is almost always “;” (as &lt;code&gt;;space&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To generate the &lt;code&gt;ipc_class&lt;/code&gt; data we can do the following and then write the file to .csv. Note that we have set the number of results &lt;code&gt;n_results&lt;/code&gt; to 10.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pizza_ipc_class &amp;lt;- patent_count(data = pizza_1990_2012, col = &amp;quot;ipc_class&amp;quot;, count_col = &amp;quot;publication_number&amp;quot;, 
    n_results = 10, sep = &amp;quot;;&amp;quot;)
pizza_ipc_class&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##    ipc_class                                           records
##    &amp;lt;chr&amp;gt;                                                 &amp;lt;dbl&amp;gt;
##  1 A21: Baking                                           2233.
##  2 A23: Foods Or Foodstuffs                              1843.
##  3 B65: Conveying                                        1383.
##  4 G06: Computing                                        1326.
##  5 A47: Furniture                                         932.
##  6 H04: Electric Communication Technique                  747.
##  7 H05: Electric Techniques Not Otherwise Provided For    613.
##  8 F24: Heating                                           512.
##  9 A61: Medical Or Veterinary Science                     318.
## 10 G07: Checking                                          226.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(pizza_ipc_class, &amp;quot;pizza_ipcclass_1990_2012.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that this dataset is based on the main &lt;code&gt;pizza_1990_2012&lt;/code&gt; dataset (including cases where no applicant name is available). The reason we have not used the applicants dataset is because that dataset will duplicate the IPC field for each split of an applicant name. As a result it will over count the IPCs by the number of applicants on a document name. As this suggests, it is important to be careful when working with data that has been tidied because of the impact on other counts.&lt;/p&gt;
&lt;p&gt;This problem does not apply in the case of our Google data because the only applicant listed in that data is Google (excluding co-applicants). We can therefore safely use the Google dataset to identify the IPC codes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;google_ipc_class &amp;lt;- patent_count(data = google, col = &amp;quot;ipc_class&amp;quot;, count_col = &amp;quot;publication_number&amp;quot;, 
    n_results = 10, sep = &amp;quot;;&amp;quot;)
google_ipc_class&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 2
##   ipc_class                             records
##   &amp;lt;chr&amp;gt;                                   &amp;lt;dbl&amp;gt;
## 1 G06: Computing                           105.
## 2 H04: Electric Communication Technique     17.
## 3 G01: Measuring                            14.
## 4 G09: Educating                            11.
## 5 G10: Musical Instruments                   7.
## 6 A63: Sports                                1.
## 7 G08: Signalling                            1.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(google_ipc_class, &amp;quot;google_ipcclass_1990_2012.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are only 7 classes and as we might expect they are dominated by computing. We might want to dig into this in a little more detail and so let’s also create an IPC subclass field.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;google_ipc_subclass &amp;lt;- patent_count(data = google, col = &amp;quot;ipc_subclass_detail&amp;quot;, count_col = &amp;quot;publication_number&amp;quot;, 
    n_results = 10, sep = &amp;quot;;&amp;quot;)
google_ipc_subclass&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##    ipc_subclass_detail                                                  records
##    &amp;lt;chr&amp;gt;                                                                  &amp;lt;dbl&amp;gt;
##  1 G06F: Electric Digital Data Processing                                   89.
##  2 G06Q: Data Processing Systems Or Methods, Specially Adapted For Adm…     24.
##  3 G01C: Measuring Distances, Levels Or Bearings                            14.
##  4 G09B: Educational Or Demonstration Appliances                             9.
##  5 G10L: Speech Analysis Or Synthesis                                        7.
##  6 H04W: Wireless Communication Networks                                     7.
##  7 G09G: Arrangements Or Circuits For Control Of Indicating Devices Us…      5.
##  8 H04B: Transmission                                                        4.
##  9 H04L: Transmission Of Digital Information, E.G. Telegraphic Communi…      4.
## 10 H04M: Telephonic Communication                                            4.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(google_ipc_subclass, &amp;quot;google_ipcsubclass_1990_2012.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have the data on technology areas that we need to understand our data. The next and final step is to generate data from the text fields.&lt;/p&gt;
&lt;div id=&#34;phrases-tables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Phrases Tables&lt;/h3&gt;
&lt;p&gt;We will be using data from words and phrases in the titles of patent documents for use in a word cloud in our infographic. It is possible to generate this type of data in R directly using the &lt;code&gt;tm&lt;/code&gt; and &lt;code&gt;NLP&lt;/code&gt; packages. Our pizza dataset already contains a title field broken down into phrases using Vantagepoint software and so we will use that. We will use the field &lt;code&gt;title_nlp_multiword_phrases&lt;/code&gt; as phrases are generally more informative than individual words. Once again we will use our general &lt;code&gt;patent_count()&lt;/code&gt; function although experimentation may be needed to identify the number of phrases that visualise well in a word cloud.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pizza_phrases &amp;lt;- patent_count(data = pizza_1990_2012, col = &amp;quot;title_nlp_multiword_phrases&amp;quot;, 
    count_col = &amp;quot;publication_number&amp;quot;, n_results = 15, sep = &amp;quot;;&amp;quot;)
pizza_phrases&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 15 x 2
##    title_nlp_multiword_phrases records
##    &amp;lt;chr&amp;gt;                         &amp;lt;dbl&amp;gt;
##  1 Food Product                   179.
##  2 Microwave Ovens                137.
##  3 Making Same                     48.
##  4 conveyor Oven                   46.
##  5 Crust Pizza                     44.
##  6 microwave Heating               41.
##  7 Bakery Product                  40.
##  8 pizza Box                       40.
##  9 Microwave Cooking               39.
## 10 Pizza Oven                      37.
## 11 pizza Dough                     35.
## 12 Cook Food                       34.
## 13 Baked Product                   33.
## 14 Related Method                  32.
## 15 Food Item                       29.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(pizza_phrases, &amp;quot;pizza_phrases_1990_2012.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we do the same with the Google data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;google_phrases &amp;lt;- patent_count(data = google, col = &amp;quot;title_nlp_multiword_phrases&amp;quot;, 
    count_col = &amp;quot;publication_number&amp;quot;, n_results = 15, sep = &amp;quot;;&amp;quot;)
google_phrases&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 15 x 2
##    title_nlp_multiword_phrases           records
##    &amp;lt;chr&amp;gt;                                   &amp;lt;dbl&amp;gt;
##  1 Digital Map System                        10.
##  2 conversion Path Performance Measures       9.
##  3 Mobile Device                              8.
##  4 Search Results                             8.
##  5 Geographical Relevance                     4.
##  6 Local Search Results                       4.
##  7 Location Prominence                        4.
##  8 Network Speech Recognizers                 4.
##  9 Processing Queries                         4.
## 10 Search Query                               4.
## 11 aspect-Based Sentiment Summarization       3.
## 12 authoritative Document Identification      3.
## 13 Business Listings Search                   3.
## 14 Content Providers                          3.
## 15 indexing Documents                         3.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(google_phrases, &amp;quot;google_phrases_1990_2012.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have the following .csv files.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;pizza_total_1990_2012&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pizza_country_1990_2012&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pizza_applicants_1990_2012&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pizza_ipcclass_1990_2012&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pizza_phrases_1990_2012&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Google_1990_2012&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Google_ipclass_1990_2012&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Google_ipcsubclass_1990_2012&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Google_phrases-1990_2012&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-an-infographic-in-infogr.am&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating an infographic in infogr.am&lt;/h2&gt;
&lt;p&gt;If you are starting this chapter here then download the datasets we will be using as a single zip file from the Manual repository &lt;a href=&#34;https://github.com/wipo-analytics/opensource-patent-analytics/blob/master/2_datasets/infographic/infographic.zip?raw=true&#34;&gt;here&lt;/a&gt; and then unzip the file.&lt;/p&gt;
&lt;p&gt;We first need to sign up for a free account with &lt;a href=&#34;https://infogr.am/&#34;&gt;infogr.am&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/infogram/fig1_infogram_front.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will then see a page with some sample infographics to provide ideas to get you started.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/infogram/fig2_infogram_login.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
Click on one of the infograms with a graph such as Trends in Something and then click inside the graph box itself and select the edit button in the top right.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/infogram/fig3_infogram_findedit.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This will open up a data panel with the toy data displayed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;images/infogram/fig4_infogram_datapanel.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We want to replace this data by choosing the upload button and selecting our &lt;code&gt;pizza_country_1990_2012.csv&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/infogram/fig5_infogram_panelgraph.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We now have a decent looking graph for our country trends data where we can see the number of records per country and year by hovering over the relevant data points. While some of the countries with low frequency data are crunched at the bottom (and would be better displayed in a separate graph), hovering over the data or over a country name will display the relevant country activity. We will therefore live with this.&lt;/p&gt;
&lt;p&gt;We now want to start adding story elements by clicking on the edit button in the title. Next we can start adding new boxes using the menu icons on the right. Here we have changed the title, added a simple body text for the data credit and then a quote from someone describing themselves as the Head of Pizza Analytics.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/infogram/fig6_infogram_paneltext.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next we need to start digging into the data using our IPC, applicants and phrases data.&lt;/p&gt;
&lt;p&gt;To work with our IPC class data we will add a bar chart and load the data. To do this select the graph icon in the right and then Bar. Once again we will choose edit and then load our &lt;code&gt;pizza_ipcclass_1990_2012&lt;/code&gt; dataset. Then we can add a descriptive text box. We can then continue to add elements as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;applicants bar chart&lt;/li&gt;
&lt;li&gt;pizza phrases by selecting graph and word cloud&lt;/li&gt;
&lt;li&gt;Google ipc-subclass&lt;/li&gt;
&lt;li&gt;Google word cloud.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One useful approach to developing an infographic is to start by adding the images and then add titles and text boxes to raise key points. In infogram new text boxes appear below existing boxes but can be repositioned by dragging and dropping boxes onto each other.&lt;/p&gt;
&lt;p&gt;One nice feature of infogram is that it is easy to share the infographic with others through a url, an embed code or on facebook or via twitter.&lt;/p&gt;
&lt;p&gt;At the end of the infographic it is a good idea to provide a link where the reader can obtain more information, such as the full report or the underlying data. In this case we will add a link to the Tableau workbook on pizza patent activity that we developed in an earlier &lt;a href=&#34;https://public.tableau.com/profile/wipo.open.source.patent.analytics.manual#!/vizhome/pizzapatents/Overview&#34;&gt;chapter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Our final infographic should look something like &lt;a href=&#34;https://infogr.am/trends_in_something&#34;&gt;this&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Round Up&lt;/h3&gt;
&lt;p&gt;In this chapter we have concentrated on using R to tidy patent data in order to create an online infographic using free software. Using our trusty pizza patent data from WIPO Patentscope we walked through the process of wrangling and tidying patent data first using short lines of code that we then combined into a reusable function. As this introduction to tidying data in R has hopefully revealed, R and packages such as &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;tidyr&lt;/code&gt; and &lt;code&gt;stringr&lt;/code&gt; provide very useful tools for working with patent data, and they are free and well supported.&lt;/p&gt;
&lt;p&gt;In the final part of the chapter we used the data we had generated in RStudio to create an infographic using infogr.am that we then shared online. Infogram is just one of a number of online infographic services and it is well worth trying other services such as &lt;a href=&#34;https://www.easel.ly&#34;&gt;easel.ly&lt;/a&gt; to find a service that meets your needs.&lt;/p&gt;
&lt;p&gt;As regular users of R will already know, it is already possible to produce all of these graphics (such as word clouds) directly in R using tools such as &lt;code&gt;ggplot2&lt;/code&gt;, &lt;code&gt;plotly&lt;/code&gt; and word clouds using packages such as &lt;code&gt;wordcloud&lt;/code&gt;. Some of these topics have been covered in other chapters and for more on text mining and word clouds in R see this recent article on &lt;a href=&#34;http://www.r-bloggers.com/building-wordclouds-in-r/&#34;&gt;R-bloggers&lt;/a&gt;. None of the infographic services we viewed appeared to offer an API that would enable a direct connection with R. There also seems to be a gap in R’s packages where infographics might sit with this &lt;a href=&#34;http://www.r-bloggers.com/r-how-to-layout-and-design-an-infographic/&#34;&gt;2015 R-bloggers article&lt;/a&gt; providing a walk through on how to create a basic infographic.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://stackoverflow.com/questions/25975827/how-to-feed-the-result-of-a-pipe-chain-magrittr-to-an-object&#34; class=&#34;uri&#34;&gt;http://stackoverflow.com/questions/25975827/how-to-feed-the-result-of-a-pipe-chain-magrittr-to-an-object&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exploring Scientific Literature with rplos</title>
      <link>/rplos-walkthrough/</link>
      <pubDate>Tue, 27 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>/rplos-walkthrough/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this chapter we look at the use of the &lt;a href=&#34;https://github.com/ropensci/rplos&#34;&gt;&lt;code&gt;rplos&lt;/code&gt;&lt;/a&gt; package from &lt;a href=&#34;https://ropensci.org&#34;&gt;rOpenSci&lt;/a&gt; to access the scientific literature from the &lt;a href=&#34;https://www.plos.org&#34;&gt;Public Library of Science&lt;/a&gt; using the &lt;a href=&#34;http://api.plos.org/solr/faq/&#34;&gt;PLOS Search API&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Public Library of Science (PLOS) is the main champion of open access peer reviewed scientific publications and has published somewhere in the region of 140,000 articles. These articles are a fantastic resource. PLOS includes the following titles.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PLOS ONE&lt;/li&gt;
&lt;li&gt;PLOS Biology&lt;/li&gt;
&lt;li&gt;PLOS Medicine&lt;/li&gt;
&lt;li&gt;PLOS Computational Biology&lt;/li&gt;
&lt;li&gt;PLOS Genetics&lt;/li&gt;
&lt;li&gt;PLOS Pathogens&lt;/li&gt;
&lt;li&gt;PLOS Neglected Tropical Diseases&lt;/li&gt;
&lt;li&gt;PLOS Clinical Trials ()&lt;/li&gt;
&lt;li&gt;PLOS Collections (collections of articles)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PLOS is important because it provides open access to the full text of peer reviewed research. For researchers interested in working with R, &lt;code&gt;rplos&lt;/code&gt; and its bigger sister package, the &lt;a href=&#34;https://ropensci.org/tutorials/fulltext_tutorial.html&#34;&gt;rOpenSci &lt;code&gt;fulltext&lt;/code&gt; package&lt;/a&gt; are very important tools for gaining access to research.&lt;/p&gt;
&lt;p&gt;This article is part of work in progress for the WIPO Manual on Open Source Patent Analytics. The Manual is intended to introduce open source analytics tools to patent researchers in developing countries and to be of wider use to the science and technology research community. An important part of patent research is being able to access and analyse the scientific literature.&lt;/p&gt;
&lt;p&gt;This article makes no assumptions about knowledge of R or programming. &lt;code&gt;rplos&lt;/code&gt; is a good place to start with learning how to access scientific literature in R using Application Programming Interfaces (APIs). Because &lt;code&gt;rplos&lt;/code&gt; is well organised and the data is very clean it is also a good place to learn some of the basics of working with data in R. This provides a good basis for working with the ROpenSci &lt;a href=&#34;https://github.com/ropensci/fulltext&#34;&gt;fulltext package&lt;/a&gt;. &lt;code&gt;fulltext&lt;/code&gt; allows you to retrieve scientific literature from multiple data sources and we will deal with that next.&lt;/p&gt;
&lt;p&gt;We will also use this as an opportunity to introduce some of the popular packages for working with data in R, notably the family of packages for tidying and wrangling data developed by Hadley Wickham at RStudio (namely, &lt;code&gt;plyr&lt;/code&gt;, &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;stringr&lt;/code&gt; and &lt;code&gt;tidyr&lt;/code&gt;). We will only touch on these but we include then as everyday working packages that you will find useful in learning more about R.&lt;/p&gt;
&lt;p&gt;The first step is to make sure that you have R and RStudio.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;install-r-and-rstudio&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Install R and RStudio&lt;/h2&gt;
&lt;p&gt;To get up and running you need to install a version of R for your operating system. You can do that from &lt;a href=&#34;http://cran.rstudio.com/&#34;&gt;here&lt;/a&gt;. Then download RStudio Desktop for your operating system from &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/&#34;&gt;here&lt;/a&gt; using the installer for your system. Then open RStudio.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-a-project&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Create A Project&lt;/h2&gt;
&lt;p&gt;Projects are probably the best way of organising your work in RStudio. To create a new project select the dropdown menu in to top right where you see the blue R icon. Navigate to where you want to keep your R materials and give your project a name (e.g. rplos). Now you will be able to save you work into an rplos project folder and R will keep everything together when you save the project.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;install-packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Install Packages&lt;/h2&gt;
&lt;p&gt;First we need to install some packages to help us work with the data. This list of packages are common “go to” packages for daily use.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;rplos&amp;quot;)  #the main event
install.packages(&amp;quot;readr&amp;quot;)  #for reading data
install.packages(&amp;quot;plyr&amp;quot;)  #for wrangling data
install.packages(&amp;quot;dplyr&amp;quot;)  #for wrangling data
install.packages(&amp;quot;tidyr&amp;quot;)  #for tidying data
install.packages(&amp;quot;stringr&amp;quot;)  #for manipulating strings
install.packages(&amp;quot;tm&amp;quot;)  #for text mining
install.packages(&amp;quot;XML&amp;quot;)  #for dealing with text in xml&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we load the libraries. Note that &lt;code&gt;rplos&lt;/code&gt; will install and load any other packages that it needs (in this case ggplot2 for graphing) so we don’t need to worry about that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rplos)
library(readr)
library(plyr)  # load before dplyr to avoid errors
library(dplyr)
library(tidyr)
library(stringr)
library(tm)
library(XML)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next let’s take a look at the wide range of functions that are available for searching using &lt;code&gt;rplos&lt;/code&gt; by moving over to the Packages tab in RStudio and clicking on &lt;code&gt;rplos&lt;/code&gt;. A very useful tutorial on using &lt;code&gt;rplos&lt;/code&gt; can be found &lt;a href=&#34;https://ropensci.org/tutorials/rplos_tutorial.html&#34;&gt;here&lt;/a&gt; and can be cited as “Scott Chamberlain, Carl Boettiger and Karthik Ram (2015). rplos: Interface to PLOS Journals search API. R package version 0.5.0 &lt;a href=&#34;https://github.com/ropensci/rplos&#34; class=&#34;uri&#34;&gt;https://github.com/ropensci/rplos&lt;/a&gt;”. If you are already comfortable working in R you might want to head to that introductory tutorial as this article contains a lot more in the way of explanation. However, we will also add some new examples and code for working with the results to add to the resource base for &lt;code&gt;rplos&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;key-functions-in-rplos&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Key functions in rplos&lt;/h2&gt;
&lt;p&gt;R is an object oriented language meaning that it works on objects such as a vector, table, list, or matrix. These are easy to create. We then apply functions to the data from &lt;code&gt;base R&lt;/code&gt; or from packages we have installed for particular tasks.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;searchplos()&lt;/code&gt;, the basic function for searching plos&lt;/li&gt;
&lt;li&gt;&lt;code&gt;plosauthor()&lt;/code&gt;, search on author name&lt;/li&gt;
&lt;li&gt;&lt;code&gt;plostitle()&lt;/code&gt;, search the title&lt;/li&gt;
&lt;li&gt;&lt;code&gt;plosabstract()&lt;/code&gt;, search the abstract&lt;/li&gt;
&lt;li&gt;&lt;code&gt;plossubject()&lt;/code&gt;, search by subject&lt;/li&gt;
&lt;li&gt;&lt;code&gt;citations()&lt;/code&gt;, search the &lt;a href=&#34;http://api.richcitations.org/&#34;&gt;PLOS Rich Citations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;plos_fulltext()&lt;/code&gt;, retrieve full text using a DOI&lt;/li&gt;
&lt;li&gt;&lt;code&gt;highplos()&lt;/code&gt;, highlight search terms in the results.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;highbrow()&lt;/code&gt;, browse search terms in a browser with hyperlinks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Functions in R take (accept) arguments which are options for the type of data we want to obtain when using an API or the calculations that we want to run on the data. For &lt;code&gt;rplos&lt;/code&gt; we will mainly use arguments setting out our search query, the fields that we want to search, and the number of results.&lt;/p&gt;
&lt;p&gt;If you are new to R this will typically takes the form of a short piece of code that is structured like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;newobject &amp;lt;- function(yourdata, argument1, argument2, other_arguments)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A new object is likely to be a table or list containing data. the sign &lt;code&gt;&amp;lt;-&lt;/code&gt; gets or passes the results of the function (such as seachplos) to the new object. To specify what we want we first include our data (&lt;code&gt;yourdata&lt;/code&gt;) and then one or more arguments which control what we get, such as the number of records or the title etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-fields-in-rplos&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Fields in rplos&lt;/h2&gt;
&lt;p&gt;There are quite a number of fields that can be searched with &lt;code&gt;rplos&lt;/code&gt; or used to refine a search. We will only use a few of them. To see the range of fields type &lt;code&gt;plosfields&lt;/code&gt; into the console and press Enter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plosfields&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example, if we wanted to search the title, abstract and conclusions we would use these fields in building the query (see below). If we wanted to search everything but those fields we would probably use body. If we wanted to retrieve the references then we would include &lt;code&gt;reference&lt;/code&gt; in the fields. In &lt;code&gt;rplos&lt;/code&gt; a field is denoted by &lt;code&gt;fl =&lt;/code&gt; with the fields in quotes such as &lt;code&gt;fl = &amp;quot;title&amp;quot;&lt;/code&gt; and so on as we will see below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-searching-using-searchplos-navigating-and-exporting-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basic Searching using &lt;code&gt;searchplos()&lt;/code&gt;, Navigating and Exporting Data&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;searchplos()&lt;/code&gt; is the basic &lt;code&gt;rplos&lt;/code&gt; search function and returns a list of document identifiers (DOIs) or other data fields. The basic search result is a set of DOIs that can be used for further work. To get help for a function, or to find working examples, use &lt;code&gt;?&lt;/code&gt; in front of the function in the console:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;`?`(searchplos)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will bring up the help page for that function with a description of the arguments that are available and with examples at the bottom of the page.&lt;/p&gt;
&lt;p&gt;The examples are there to help you. In &lt;code&gt;rplos&lt;/code&gt; they presently focus on the use of single search terms such as ecology. However, as we will see below, it is possible to use phrases in searching and to use multiple terms. There are quite a number of arguments (options) available for refining the results and we will include some of these in the examples.&lt;/p&gt;
&lt;p&gt;The author of this article is a big fan of pizza. So, in the first example we will carry out a simple search for the term pizza and then specify the results we want to see using the argument &lt;code&gt;fl =&lt;/code&gt; (for fields) and the number of results that we want to see using &lt;code&gt;limit = 20&lt;/code&gt;. In specifying the fields we will use &lt;code&gt;c()&lt;/code&gt; to combine them together.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- searchplos(q = &amp;quot;pizza&amp;quot;, fl = c(&amp;quot;id&amp;quot;,&amp;quot;publication_date&amp;quot;, &amp;quot;title&amp;quot;, &amp;quot;abstract&amp;quot;), limit = 20)
p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What &lt;code&gt;searchplos()&lt;/code&gt; has done in the background is to send a request to the PLOS API to bring back the id, publication_date, title and abstract for 20 records across the PLOS journals. To see the results type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Results in R are stored in objects (in this case the object is a list). To see the type of object in R use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When working with R it is generally more useful to understand the structure of the data so that you can work out how to access it. That can be done using &lt;code&gt;str()&lt;/code&gt; for structure. This is one of the most useful functions in R and well worth writing down.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results might seem a little confusing at first but what this is telling us is that we have an R object that is a list consisting of two components. The first is an item called &lt;code&gt;meta&lt;/code&gt; that reports the number of records found and the type of object (a data.frame). The second is &lt;code&gt;data&lt;/code&gt; which contains the information on the two results in the form of a data frame (basically a table) containing the id, date, title and abstract information that we asked PLOS for.&lt;/p&gt;
&lt;p&gt;Note that the list contains a marker &lt;code&gt;$&lt;/code&gt; for the beginning of the two lists with the data they contain appearing as &lt;code&gt;..$&lt;/code&gt; signifying that they are nested under &lt;code&gt;meta&lt;/code&gt; or &lt;code&gt;data&lt;/code&gt;. This hierarchy helps us with accessing the data using subsetting in R. For example, if we wanted to access the &lt;code&gt;meta&lt;/code&gt; data (and we do) we can use the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p$meta&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That will just print the full &lt;code&gt;meta&lt;/code&gt; data entries. If we wanted to just access the number of records (num$Found) then we would extend this a little by moving to that position in the hierarchy with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p$meta$numFound&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That will print out just the number of records returned by our search. An alternative way of subsetting is to use the “[” and “[[” and the numeric position in the list. In &lt;a href=&#34;http://www.amazon.com/Hands-On-Programming-Write-Functions-Simulations/dp/1449359019&#34;&gt;Hands on Programming with R&lt;/a&gt; Garrett Grolemund compares this to a train with numbered carriages where “[]” selects the train carriage e.g. [1] and “[[1]]” selects the contents of carriage number 1. We don’t need to worry about this but it is very helpful as a way of remembering the difference. For example the following selects the contents of the first item in our list (&lt;code&gt;meta&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and is the same as &lt;code&gt;p$meta&lt;/code&gt;. While:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p[[1]][[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;is the same as &lt;code&gt;p$meta$numFound&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Subsetting the data by its numeric position rather than its name makes life much easier when working with lists with lots of items. As we will see below, when applying a function to a list with multiple items we can also use “[[”, 2. This will retrieve the second item in each of our line of train carriages.&lt;/p&gt;
&lt;p&gt;Another useful tip for navigating the data in RStudio is using autocomplete. Try typing the following into the console.&lt;/p&gt;
&lt;p&gt;p&lt;span class=&#34;math inline&#34;&gt;\(meta\)&lt;/span&gt; #type me in the console, do not cut and paste&lt;/p&gt;
&lt;p&gt;When we type the $ a popup will appear and display two entries as tables for &lt;code&gt;meta&lt;/code&gt; and &lt;code&gt;data&lt;/code&gt;. Click on meta, then add another $ sign at the end. It will now display three items in purple (for vectors). Select &lt;code&gt;numFound&lt;/code&gt; and hey presto! As you work with RStudio you will notice that when you start to type a function name, lists of names will start to pop up. Type &lt;code&gt;search&lt;/code&gt; into the console but do not press enter and wait a moment. A list with three items should pop up with search {base}, searchpaths {base}, and searchplos {rplos}. This is really helpful because it saves a lot of typing. As you become more familiar with R it also helpfully displays what a function does and a reminder of its arguments. The soft brackets around {base} indicate the package where the function can be found (this can be useful for discovering functions when you get stuck).&lt;/p&gt;
&lt;p&gt;Finally, you can also see the items in your project in the Environment pane. Click on the blue arrow for &lt;code&gt;p&lt;/code&gt; in the Environment pane under Values and you will see the structure of the data in &lt;code&gt;p&lt;/code&gt; and some of its content.&lt;/p&gt;
&lt;div id=&#34;creating-a-new-object-and-writing-to-file&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating a New Object and Writing to File&lt;/h3&gt;
&lt;p&gt;Ok so we have a list with some results containing &lt;code&gt;meta&lt;/code&gt; and &lt;code&gt;data&lt;/code&gt;. We now want to export &lt;code&gt;data&lt;/code&gt; to a .csv file that we can work with in Excel or another programme.&lt;/p&gt;
&lt;p&gt;While we will want to make a note of the total number of results in &lt;code&gt;meta&lt;/code&gt;, what we really want will be in &lt;code&gt;data&lt;/code&gt;. We can simply create a new object using the code above and assign it to a name using &lt;code&gt;&amp;lt;-&lt;/code&gt;. Note that there is no space here and &lt;code&gt;&amp;lt; -&lt;/code&gt; will not work.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- p$data
dat&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we look at the class of this object (&lt;code&gt;class(dat)&lt;/code&gt;) we now have a data.frame (a table) that we can write to a .csv file to use later. We can do this easily using &lt;code&gt;write.csv()&lt;/code&gt; and start by naming the object we want to write (&lt;code&gt;dat&lt;/code&gt;) and then giving it a file name. Because we created an &lt;code&gt;rplos&lt;/code&gt; project in RStudio earlier (didn’t we), the file will be saved into the project folder. If you didn’t create a project or want to check the directory then use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;getwd()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will show your current working directory. If you do not see the name of your &lt;code&gt;rplos&lt;/code&gt; project then copy the full file path so that it looks something like this (don’t forget the &amp;quot;&amp;quot; around the path):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;setwd(&amp;quot;/Users/pauloldham/Desktop/open_source_master/rplos&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, we now know where we are. So, let’s save the file.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write.csv(dat, &amp;quot;dat.csv&amp;quot;, row.names = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we open this up in Excel or Open Office Calc then we will see two blank entries in the abstract fields. Blank cells can create calculation problems. Inside R we can handle this by filling in the blanks with NA as follows [2]. In this case we are subsetting into dat and then asking R to identify those cells that exactly match &lt;code&gt;==&lt;/code&gt; with &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;. We then fill those cells in dat with NA (for Not Available).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat[dat == &amp;quot;&amp;quot;] &amp;lt;- NA
dat&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then simply write the file as before. If we wanted to remove the NAs we have just introduced then we could use &lt;code&gt;write.csv(dat, &amp;quot;dat.csv&amp;quot;, row.names = FALSE, na = &amp;quot;&amp;quot;)&lt;/code&gt; which will convert them back to blank spaces.&lt;/p&gt;
&lt;p&gt;A faster way to deal with writing files is to use the recent &lt;code&gt;readr&lt;/code&gt; package as this will not add row numbers to exported files. Here we will use the &lt;code&gt;write_csv()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(dat, &amp;quot;dat.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The advantage of &lt;code&gt;readr&lt;/code&gt; is that it is fast and does not require the same number of arguments as the standard &lt;code&gt;write.csv&lt;/code&gt; such as specifying row names or with &lt;code&gt;read.csv&lt;/code&gt; specifying stringsAsFactors = FALSE.&lt;/p&gt;
&lt;p&gt;Finally, if we wanted to write the entire list &lt;code&gt;p&lt;/code&gt;, including &lt;code&gt;meta&lt;/code&gt; to file then we could use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write.csv(p, &amp;quot;p.csv&amp;quot;, row.names = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have now retrieved some data containing pizza through the PLOS API using &lt;code&gt;rplos&lt;/code&gt; and we have written the data to a file as a table we can use later. We will now move on to some more sophisticated things we can do with &lt;code&gt;rplos&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;limit-by-journal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Limit by journal&lt;/h2&gt;
&lt;p&gt;As we have seen above, PLOS contains 7 journals and in &lt;code&gt;rplos&lt;/code&gt; the results for a search can be limited to specific journals such as PLOS ONE or PLOS Biology. Note that the short journal names appear to use the old format for PLOS consisting of mixed upper and lowercase characters (e.g. PLoSONE not PLOSONE). A nice easy way to find the short journal names is to use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;journalnamekey()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we will limit the search to PLOS ONE by adding &lt;code&gt;fq =&lt;/code&gt; to the arguments and then the &lt;code&gt;cross_published_journal_key&lt;/code&gt; argument. Note that the &lt;code&gt;fq=&lt;/code&gt; argument takes the same options as &lt;code&gt;fl=&lt;/code&gt;. But, &lt;code&gt;fq =&lt;/code&gt; filters the results returned by PLOS to only those specified in &lt;code&gt;fq =&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pizza &amp;lt;- searchplos(q = &amp;quot;pizza&amp;quot;, fl = c(&amp;quot;id&amp;quot;, &amp;quot;publication_date&amp;quot;, &amp;quot;title&amp;quot;, &amp;quot;abstract&amp;quot;), fq = &amp;#39;cross_published_journal_key:PLoSONE&amp;#39;, start = 0, limit = 20)
head(pizza$data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have retrieved 20 records here using &lt;code&gt;limit = 20&lt;/code&gt; (the default is 10). It is generally a good idea to start with a small number of results to test that we are getting what we expect back rather than lots of irrelevant data. What if we wanted to retrieve all of the results? Here we will need to do a bit more work using the meta field.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;obtaining-the-full-number-of-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Obtaining the full number of results&lt;/h2&gt;
&lt;p&gt;One way to do this is to take our original number of results and then subset in to the data and create a new object containing the value for the number of records in &lt;code&gt;numFound&lt;/code&gt;. Note that the number of records for a particular query below may well have gone up by the time that you read this article.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r &amp;lt;- pizza$meta$numFound&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To run a new search we can now insert &lt;code&gt;r&lt;/code&gt; into the limit = value. This will be interpreted as the numeric value of &lt;code&gt;r&lt;/code&gt; (210).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pizza &amp;lt;- searchplos(q = &amp;quot;pizza&amp;quot;, fl = c(&amp;quot;id&amp;quot;, &amp;quot;publication_date&amp;quot;, &amp;quot;title&amp;quot;, &amp;quot;abstract&amp;quot;), fq = &amp;#39;cross_published_journal_key:PLoSONE&amp;#39;, start = 0, limit = r)
head(pizza$data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An alternative way of doing this is to make life a bit easier for ourselves by first running our query and setting the limit as &lt;code&gt;limit = 0&lt;/code&gt;. This will only return the &lt;code&gt;meta&lt;/code&gt; data. We then add the subset for number found to the end of the code as &lt;code&gt;$meta$numFound&lt;/code&gt;. That will pull back the value directly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r &amp;lt;- searchplos(q = &amp;quot;pizza&amp;quot;, fq = &amp;quot;cross_published_journal_key:PLoSONE&amp;quot;, limit = 0)$meta$numFound
r&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then run the query again using the value of &lt;code&gt;r&lt;/code&gt; in limit = :&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pizza &amp;lt;- searchplos(q = &amp;quot;pizza&amp;quot;, fl = c(&amp;quot;id&amp;quot;, &amp;quot;publication_date&amp;quot;, &amp;quot;title&amp;quot;, &amp;quot;abstract&amp;quot;), fq = &amp;#39;cross_published_journal_key:PLoSONE&amp;#39;, start = 0, limit = r) 
head(pizza$data)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;obtaining-the-number-of-records-across-plos-journals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Obtaining the number of records across PLOS Journals&lt;/h2&gt;
&lt;p&gt;That has returned the full 210 results for PLOS ONE. We could attempt to make life even easier by first getting the results across all PLOS journals. We do this by removing the &lt;code&gt;fq =&lt;/code&gt; argument limiting the data to PLOS ONE and saving the result in and object we will call &lt;code&gt;r1&lt;/code&gt;. Note that the number of records will probably have gone up by the time you read this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r1 &amp;lt;- searchplos(&amp;quot;pizza&amp;quot;, limit = 0)$meta$numFound
r1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This produces 352 results at the time of writing. What happens now if we run our original query using the value of &lt;code&gt;r1&lt;/code&gt; (352 records) but limiting the results only to PLOS ONE?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pizza &amp;lt;- searchplos(q = &amp;quot;pizza&amp;quot;, fl = c(&amp;quot;id&amp;quot;, &amp;quot;publication_date&amp;quot;, &amp;quot;title&amp;quot;, &amp;quot;abstract&amp;quot;), fq = &amp;#39;cross_published_journal_key:PLoSONE&amp;#39;, start = 0, limit = r1)
pizza$meta$numFound&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The answer is that the 210 results in PLOS ONE are returned from the total of 244 across the PLOS journals. Why? The reason this works is that &lt;code&gt;searchplos()&lt;/code&gt; initially pulls back all of the data from the PLOS API and then applies our entry in &lt;code&gt;fq =&lt;/code&gt; as a filter. So, in reality the full 244 records are fetched and then filtered down to the 210 from PLOS ONE. In this case, this makes our lives easier because we can use the results across PLOS journals and then restrict the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;writing-the-results-and-using-a-codebook&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Writing the results and using a codebook&lt;/h2&gt;
&lt;p&gt;We now have a total of 210 results for pizza. We can simply write the results to a .csv file.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write.csv(pizza, &amp;quot;plosone_pizza.csv&amp;quot;, row.names = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As this illustrates, it is very easy to use &lt;code&gt;rplos()&lt;/code&gt; and rapidly create a file that can be used for other purposes.&lt;/p&gt;
&lt;p&gt;When working in R you will often create multiple tables and take multiple steps. To keep track of what you do it is a good idea to create a text file as a codebook. Use the codebook to note down the important steps you take. The idea of a codebook is taken from Jeffrey Leek’s &lt;a href=&#34;https://leanpub.com/datastyle&#34;&gt;Elements of Data Analytic Sytle&lt;/a&gt; which provides a very accessible introduction to staying organised. To create a codebook in RStudio simply use &lt;code&gt;File &amp;gt; New File &amp;gt; Text File&lt;/code&gt;. This will open a text file that can be saved with your project. The codebook allows you to recall what actions you performed on the data months or years later. It also allows others to follow and reproduce your results and is important for &lt;a href=&#34;https://ropensci.org/blog/2014/06/09/reproducibility/&#34;&gt;reproducible research&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;proximity-searching&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Proximity Searching&lt;/h2&gt;
&lt;p&gt;We will typically want to carry out a search by first retrieving a rough working set of results to get a feel for the data and then experimenting until we are happy with the data to noise ratio (see this &lt;a href=&#34;http://rsta.royalsocietypublishing.org/content/372/2031/20140065&#34;&gt;article&lt;/a&gt; for an example).&lt;/p&gt;
&lt;p&gt;In thinking about ways to refine our search criteria we can also use proximity searching. Proximity searching focuses on the distance between words that we are interested in. To read more about this use &lt;code&gt;?searchplos&lt;/code&gt; in the console and scroll down to example seven in the help list. We reproduce that example here using the words synthetic and biology as our terms.&lt;/p&gt;
&lt;p&gt;We can set the proximity of terms using tilde &lt;code&gt;~&lt;/code&gt; and a value. For example, &lt;code&gt;~15&lt;/code&gt; will find instances of the terms synthetic and biology within 15 words of each other in the full texts of PLOS articles.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;searchplos(q = &amp;quot;everything:\&amp;quot;synthetic biology\&amp;quot;~15&amp;quot;, fl = &amp;quot;title&amp;quot;, fq = &amp;quot;doc_type:full&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that while synthetic and biology appear inside quotes (suggesting they are a phrase to be searched) in reality the API will treat this as synthetic AND biology. That is, the query will look first for documents that contain the words synthetic AND biology and then for those cases where the words appear within 15 words of each other. In this case we get 1,684 results across PLOS (everything) and full texts (&lt;code&gt;fq = &amp;quot;doc_type:full&lt;/code&gt;) as we can see from this code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;searchplos(q = &amp;quot;everything:\&amp;quot;synthetic biology\&amp;quot;~15&amp;quot;, fl = &amp;quot;title&amp;quot;, fq = &amp;quot;doc_type:full&amp;quot;)$meta$numFound&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can narrow the search horizon to ~1 to capture those cases where the terms appear next to each other (within 1 word either to the left or the right) which produces 1001 results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;searchplos(q = &amp;quot;everything:\&amp;quot;synthetic biology\&amp;quot;~1&amp;quot;, fl = &amp;quot;title&amp;quot;, fq = &amp;quot;doc_type:full&amp;quot;)$meta$numFound&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is actually about 10 records higher than the total returned on an exact match for the phrase suggesting that there could be cases of “biology synthetic” or other issues (such as punctuation) or API performance that account for the variance. As noted in the &lt;code&gt;searchplos()&lt;/code&gt; documentation:&lt;/p&gt;
&lt;p&gt;“Don’t be surprised if queries you perform in a scripting language, like using rplos in R, give different results than when searching for articles on the PLOS website. I am not sure what exact defaults they use on their website.”&lt;/p&gt;
&lt;p&gt;As a result, it is a good idea to try different approaches. Even if it is not possible to get to the bottom of any variance it is very useful to note it down in your codebook to highlight the issue to others who may try and repeat your work.&lt;/p&gt;
&lt;p&gt;It is also important to emphasise that when using &lt;code&gt;rplos()&lt;/code&gt; it is possible to return a fragment of the text with the highlighted terms using &lt;code&gt;highplos()&lt;/code&gt; and the &lt;code&gt;hl.fragsize&lt;/code&gt; argument to set the horizon for the fragment of text around the search. This is particularly useful for text mining.&lt;/p&gt;
&lt;p&gt;In many cases the most useful information comes from searching using phrases and multiple terms. Unlike words, phrases can articulate concepts. This generally makes them more useful than single words for searching for information.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;searching-using-multiple-phrases&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Searching Using Multiple Phrases&lt;/h2&gt;
&lt;p&gt;To search by phrases we start by creating an object containing our phrases and put the phrases inside double quotation marks. If we do not use double quotation marks the search will look for documents containing both words rather than the complete phrase (e.g. synthetic AND biology rather than “synthetic biology”). Note that the code below will display &amp;quot;&amp;quot; as &amp;quot;&amp;quot; but you don’t need to enter the &lt;code&gt;\&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We will use the search query developed in this &lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0034368&#34;&gt;PLOS ONE article on synthetic biology&lt;/a&gt; in this example and retrieve the id, data, author, title and abstract across the PLOS journals.&lt;/p&gt;
&lt;p&gt;First we create the search query. Note that we use &lt;code&gt;c()&lt;/code&gt;, for combine, to combine the list of terms into a vector inside the object called &lt;code&gt;s&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- c(&amp;quot;\&amp;quot;synthetic biology\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;synthetic genomics\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;synthetic genome\&amp;quot;&amp;quot;, 
    &amp;quot;\&amp;quot;synthetic genomes\&amp;quot;&amp;quot;)
s&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now want to get the maximum number of results returned by one of the search terms. This is slightly tricky because &lt;code&gt;rplos&lt;/code&gt; will return a list containing four list items (one for each of our search terms). Each of those lists will contain &lt;code&gt;meta&lt;/code&gt; and &lt;code&gt;data&lt;/code&gt; items. What we want to do is find out which of the search terms returns the highest number of results inside &lt;code&gt;meta&lt;/code&gt; in &lt;code&gt;numFound&lt;/code&gt;. Then we can use that number as our limit.&lt;/p&gt;
&lt;p&gt;This involves more than one step.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;First we need to fetch the data.&lt;/li&gt;
&lt;li&gt;Then we need to extract &lt;code&gt;meta&lt;/code&gt; from each list.&lt;/li&gt;
&lt;li&gt;Then we need to select &lt;code&gt;numFound&lt;/code&gt; and find and return the maximum value across the lists of results.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The easiest way to do this is to create a small function that we will call &lt;code&gt;plos_records&lt;/code&gt;. To load the function into your Environment copy it and paste it into your console and press enter. The comments following &lt;code&gt;#&lt;/code&gt; explain what is happening will be ignored when the function runs. When you have done this if you move over to Environment you will see &lt;code&gt;plos_records&lt;/code&gt; under Functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plos_records &amp;lt;- function(q) {
  library(plyr) #for ldply
  library(dplyr) #for pipes, select and filter
    lapply(q, function(x) searchplos(x, limit = 0)) %&amp;gt;%
    ldply(&amp;quot;[[&amp;quot;, 1) %&amp;gt;% #get meta from the lists
    select(numFound) %&amp;gt;% #select numFound column of meta
    filter(numFound == max(numFound)) %&amp;gt;% #filter on max numFound
      print() #print max value of numFound
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can run the following code using &lt;code&gt;s&lt;/code&gt; as our query (q = s) in the function. If all goes well a result will be printed in the console with the maximum number of results. It can take a few moments for the results to come back from the API.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r2 &amp;lt;- plos_records(q = s)
r2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should now see a number around 1151 (at the time of writing). Yay!&lt;/p&gt;
&lt;p&gt;Now we can use &lt;code&gt;r2&lt;/code&gt; in the limit to return all of the records. We will write this in the standard way and then display a simpler way using pipes &lt;code&gt;%&amp;gt;%&lt;/code&gt; below. Note that we use &lt;code&gt;s&lt;/code&gt; as our search terms (see &lt;code&gt;q = s&lt;/code&gt;) and we have used &lt;code&gt;r2&lt;/code&gt; for the limit (limit = r2). Because we are calling a chunk of data this can take around a minute to run.&lt;/p&gt;
&lt;p&gt;Note that at each step in the code below we are creating and then overwriting an object called &lt;code&gt;results&lt;/code&gt;. We are also naming &lt;code&gt;results&lt;/code&gt; as the first argument in each step. This can take a few moments to run.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plyr)
results &amp;lt;- lapply(s, function(x) searchplos(x, fl = c(&amp;#39;id&amp;#39;,&amp;#39;author&amp;#39;, &amp;#39;publication_date&amp;#39;, &amp;#39;title&amp;#39;, &amp;#39;abstract&amp;#39;), limit = r2))
results &amp;lt;- setNames(results, s) #add query terms to the relevant results in the list
results &amp;lt;- ldply(results, &amp;quot;[[&amp;quot;, 2) #extract the data into a single data.frame&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can make life simpler by using pipes &lt;code&gt;%&amp;gt;%&lt;/code&gt; to simplify the code. The advantage of using pipes is that we do not have to keep creating and overwriting temporary objects (see above for &lt;code&gt;results&lt;/code&gt;). The code is also much easier to read and faster. To learn more about using pipes see this article from &lt;a href=&#34;http://seananderson.ca/2014/09/13/dplyr-intro.html&#34;&gt;Sean Anderson&lt;/a&gt;. Again the query might be a bit slow as the data is fetched back.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plyr)
library(dplyr)
results &amp;lt;- lapply(s, function(x) searchplos(x, fl = c(&amp;#39;id&amp;#39;, &amp;#39;author&amp;#39;, &amp;#39;publication_date&amp;#39;, &amp;#39;title&amp;#39;, &amp;#39;abstract&amp;#39;), limit = r2)) %&amp;gt;%
    setNames(s) %&amp;gt;% 
    ldply(&amp;quot;[[&amp;quot;, 2)
results&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pipes are a relatively recent innovation in R (see the &lt;code&gt;magrittr&lt;/code&gt;, &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;tidyr&lt;/code&gt; packages) and most code you will see will be written in the traditional way. However, pipes make R code faster and much easier to follow. While you will need to be familiar with regular R code to follow most existing work, pipes are becoming increasingly popular because the code is simpler and has a clearer logic (e.g. do this then that).&lt;/p&gt;
&lt;p&gt;We now have our data consisting of 1,405 records in a single data frame that we can view.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;View(results)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could now simply write this to a .csv file. But there are a number of things that we might want to do first. Most of these tasks fall into the category of wrangling and tidying up data so that we can carry on working with it in R or other software such as Excel.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidying-and-organising-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidying and Organising the Data&lt;/h2&gt;
&lt;p&gt;Many useful data cleaning and organisational tasks can be easily performed using the &lt;code&gt;dplyr()&lt;/code&gt; and &lt;code&gt;tidyr()&lt;/code&gt; packages developed by Hadley Wickham at RStudio. Other important packages include &lt;code&gt;stringr()&lt;/code&gt;(for working with text strings), &lt;code&gt;plyr()&lt;/code&gt; and &lt;code&gt;reshape2()&lt;/code&gt; (general wrangling) and &lt;code&gt;lubridate()&lt;/code&gt; (for working with dates). These packages were developed by Hadley Wickham and colleagues with the specific aim of making it easier to work with data in R in a consistent way. We will mainly use &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;tidyr&lt;/code&gt; in the examples below and a very useful &lt;a href=&#34;https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf&#34;&gt;RStudio cheatsheet&lt;/a&gt; can help you with working with &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;tidyr&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;renaming-a-column&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Renaming a column&lt;/h3&gt;
&lt;p&gt;First we might want to tidy up by renaming a column. For example we might want to rename &lt;code&gt;.id&lt;/code&gt; to something more meaningful. We can use &lt;code&gt;rename()&lt;/code&gt; from &lt;code&gt;dplyr()&lt;/code&gt; to do that (see &lt;code&gt;?rename&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results &amp;lt;- rename(results, search_terms = .id)
results&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;filling-blank-spaces&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Filling Blank Spaces&lt;/h2&gt;
&lt;p&gt;It is good practice to fill blank cells with NA for “Not Available”&amp;quot; to avoid calculation problems. For example, as in the earlier example, we have some blank cells in the abstract field and there may be others somewhere else. Following this &lt;a href=&#34;http://r.789695.n4.nabble.com/How-to-convert-blanks-to-NA-td895155.html&#34;&gt;StackOverflow answer&lt;/a&gt; we can do this easily.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results[results == &amp;quot;&amp;quot;] &amp;lt;- NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If for some reason we wanted to remove the NA values we can handle that at the time of exporting to a file (see above).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;converting-dates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Converting Dates&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;publication_date&lt;/code&gt; field is a character vector. We can easily turn this into a Date format that can be used in R and drop the T00:00:00 for time information using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results$publication_date &amp;lt;- as.Date(results$publication_date)
head(results$publication_date)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-columns&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding columns&lt;/h2&gt;
&lt;p&gt;When dealing with dates we might want to simply split the &lt;code&gt;publication_date&lt;/code&gt; field into three columns for year, month and day. We can do that using &lt;code&gt;separate()&lt;/code&gt; from &lt;code&gt;tidyr&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results &amp;lt;- separate(results, publication_date, c(&amp;quot;year&amp;quot;, &amp;quot;month&amp;quot;, &amp;quot;day&amp;quot;), sep = &amp;quot;-&amp;quot;, 
    remove = FALSE)
head(select(results, year, month, day))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we have specified the data (results), the column we want to separate (results) and then the three new columns that we want to create by closing them in &lt;code&gt;c()&lt;/code&gt; and placing them in quotes. This creates three new columns. The &lt;code&gt;remove&lt;/code&gt; argument specifies whether we want to remove the original column (the default is TRUE) or keep it.&lt;/p&gt;
&lt;p&gt;Because working with dates can be quite awkward (to put it mildly) it makes sense to have a range of options available to you early on in working with your data rather than having to go back to the beginning much later on.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;add-a-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Add a count&lt;/h2&gt;
&lt;p&gt;One feature of pulling back literature from an API for scientific literature is that the fields tend to be character fields rather than numeric. Character vectors in R are quoted with &amp;quot;&amp;quot;. This can make life awkward if we want to start counting things later on. To add a count column we can use &lt;code&gt;mutate&lt;/code&gt; from the &lt;code&gt;dplyr()&lt;/code&gt; package to create a new column &lt;code&gt;number&lt;/code&gt;. &lt;code&gt;number&lt;/code&gt; is based on assigning the value 1 to the id columns using &lt;code&gt;mutate()&lt;/code&gt;. We are avoiding the term count because it is the name of a function &lt;code&gt;count()&lt;/code&gt;. There are other ways of doing this but this approach points to the very useful &lt;code&gt;mutate()&lt;/code&gt; function in &lt;code&gt;dplyr&lt;/code&gt; for adding a new variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
results &amp;lt;- mutate(results, number = sum(id = 1))
head(select(results, title, number))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we view results we will now see a new column number that contains the value 1 for each entry.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;remove-a-column&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Remove a column&lt;/h2&gt;
&lt;p&gt;We will often end up with more data than we want, or create more columns than we need. The standard way to remove a column is to use the trusty &lt;code&gt;$&lt;/code&gt; to select the column and assign it to NULL.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results$columnname &amp;lt;- NULL  #dummy example&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way of doing this, which can be used for multiple columns, is to use &lt;code&gt;select()&lt;/code&gt; from &lt;code&gt;dplyr&lt;/code&gt; (see &lt;code&gt;?select()&lt;/code&gt;). Select will only keep the columns that we name. We can do this using the column names or position. For example the following will keep the first 8 columns (1:8) but will drop the unnamed 9th column because the default is to drop columns that are not named. We could also write out the column names but using the position numbers is faster in this case.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test &amp;lt;- select(results, 1:8)
length(test)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could also drop columns by position using the following (to remove column 5 and 6). This approach is useful when there are lots of columns to deal with.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test &amp;lt;- select(results, 1:4, 7:9)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An easier approach in this case is to explicitly drop columns using &lt;code&gt;-&lt;/code&gt; and keep the others.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test &amp;lt;- select(results, -month, -day)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Select is also very useful for reordering columns. Let’s imagine that we wanted to move the &lt;code&gt;id&lt;/code&gt; column to the first column. We can simply put &lt;code&gt;id&lt;/code&gt; as the first entry in &lt;code&gt;select()&lt;/code&gt; and then the total columns to reorder.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test &amp;lt;- select(results, id, 1:9)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The select function is incredibly useful for rapidly organising data as we will see below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;arranging-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Arranging the Data&lt;/h2&gt;
&lt;p&gt;We might want to arrange our rows (which can be quite difficult to do in base R). The &lt;code&gt;arrange()&lt;/code&gt; function in &lt;code&gt;dplyr&lt;/code&gt; makes this easy and arranges a column’s values in ascending order by default. Here we will specify descending &lt;code&gt;desc()&lt;/code&gt; because we want to see the most recent publications that mention our search terms at the top.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results &amp;lt;- arrange(results, desc(publication_date))
head(results$publication_date)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we use &lt;code&gt;View(results)&lt;/code&gt; we will see that the most recent data is at the top. We will also see that some of the titles towards the top are duplicates of the same article because they include all the terms in our search. So, the next thing we will want to do is to address duplicates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dealing-with-duplicates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dealing with Duplicates&lt;/h2&gt;
&lt;p&gt;How you deal with duplicates depends on what you are trying to achieve. If you are attempting to develop data on trends then duplicates will result in overcounting unless you take steps to count only distinct records. Duplicates of the same data will also distort text mining of the frequencies of terms. So, from that perspective duplicates are bad. On the other hand. If we are interested in the use of terms over time within an emerging area of science and technology, then we might well want to look in detail at the use of particular terms. For example, synthetic genomics is an alternative term for synthetic biology favoured by the J. Craig Venter group. We could look at whether this term is more widely used. Do synthetic biologists also use terms such as engineering biology, genome engineering or the fashionable new genome editing technique? In these cases duplicate records using terms are good because shifts in language can be mapped over time. This suggests a need for a strategy that uses different data tables to answer different questions.&lt;/p&gt;
&lt;p&gt;As we have already seen, it is very easy in R to create new objects (typically data.frames), take some kind of action, and write the data to a file. In thinking about duplicates we would probably first want to find out what we are dealing with by identifying unique records. There are multiple ways to do this, here are two:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unique(results$id)  #displays unique DOIs (base R)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_distinct(results$id)  #displays the count of distinct DOIs (dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tells us there are 1,098 unique DOIs meaning there were 307 duplicates at the time of writing.&lt;/p&gt;
&lt;p&gt;Next we have two main options.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We can spread the duplicate results across the table&lt;/li&gt;
&lt;li&gt;We can identify and delete the duplicates.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;spreading-data-using-spread-from-tidyr&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Spreading data using &lt;code&gt;spread()&lt;/code&gt; from &lt;code&gt;tidyr&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Rather than simply deleting our duplicate DOIs, we could create new columns for each search term and its associated DOI. This will be useful because it will tell us which terms are associated with which records over time. This is easy to do with &lt;code&gt;spread()&lt;/code&gt; by providing a &lt;code&gt;key&lt;/code&gt; and a &lt;code&gt;value&lt;/code&gt; in the arguments. In this case, we want to use &lt;code&gt;search_terms&lt;/code&gt; as the &lt;code&gt;key&lt;/code&gt; (column names) to spread across the table and the DOIs in the &lt;code&gt;id&lt;/code&gt; column as the &lt;code&gt;value&lt;/code&gt; for the rows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spread_results &amp;lt;- spread(results, search_terms, id)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates a column for each search term with the relevant DOIs as the values. Note that the default is to drop the original column (in this case &lt;code&gt;search_terms&lt;/code&gt;) when creating the new columns. Things will go badly wrong if you try to keep the existing column because R will be simultaneously trying to spread the data, thus reducing the size of the table, and keep the table in the same size. So, we will leave the default to drop the column as is.&lt;/p&gt;
&lt;p&gt;We now have a data.frame with 1098 rows and the search terms identified in each column. If we briefly inspect &lt;code&gt;spread_results&lt;/code&gt; on the terms at the end we can detect a potentially interesting pattern where some documents are only using terms such as synthetic genome or synthetic genomics while others are using only synthetic biology or a mix of terms.&lt;/p&gt;
&lt;p&gt;We have now reduced our data to unique records while preserving our search terms as reference points. The limitation of this approach is that by spreading the DOIs across 4 columns we no longer have a tidy single column of DOIs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;deleting-duplicates&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Deleting Duplicates&lt;/h3&gt;
&lt;p&gt;As an alternative, or complement, to spread we can use a logical TRUE/FALSE test to filter our dataset. There are a number of functions that perform logical tests in R (see also &lt;code&gt;which()&lt;/code&gt;, &lt;code&gt;%in%&lt;/code&gt;, &lt;code&gt;within()&lt;/code&gt;). In this case the most appropriate choice is probably &lt;code&gt;duplicated()&lt;/code&gt;. &lt;code&gt;duplicated()&lt;/code&gt; will mark duplicate records as TRUE and non-duplicated records as FALSE. We will add a column to our data using the trusty &lt;code&gt;$&lt;/code&gt; when creating the new column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results$duplicate &amp;lt;- duplicated(results$id)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we use View(results) a new column will have been added to results. Records that are not duplicates are marked FALSE while records that are duplicates are marked TRUE. We now want to filter that table down to the results that are not duplicated (are FALSE) from our logical test. We will use &lt;code&gt;filter()&lt;/code&gt; from &lt;code&gt;dplyr&lt;/code&gt; (see above). While &lt;code&gt;select()&lt;/code&gt; works exclusively with columns &lt;code&gt;filter()&lt;/code&gt; works with rows and allows us to easily filter the data on the values contained in a row.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unique_results &amp;lt;- filter(results, duplicate == FALSE) %&amp;gt;%
  select(- search_terms) 
#drop search_terms column&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we have asked &lt;code&gt;filter()&lt;/code&gt; to show us only those values in the duplicate column that exactly match with FALSE. We now have a data from with 1097 unique results with the DOIs in one column.&lt;/p&gt;
&lt;p&gt;The creation of logical TRUE/FALSE vectors is very useful in creating conditions to filter data. In this case however, in the process note that we will lose information from the &lt;code&gt;search_terms&lt;/code&gt; column which will become incomplete. To avoid potential confusion later on we drop the &lt;code&gt;search_terms&lt;/code&gt; column using &lt;code&gt;select(- search_terms)&lt;/code&gt; in the code above. If we wanted to keep the terms we would use the spread method above.&lt;/p&gt;
&lt;p&gt;We now have three data.frames, &lt;code&gt;results&lt;/code&gt;, &lt;code&gt;spread_results&lt;/code&gt;, and &lt;code&gt;unique_results&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;results&lt;/code&gt; is our core or reference set. If we planned to do a significant amount of work with this data we would save a copy of &lt;code&gt;results&lt;/code&gt; to .csv and label it as &lt;code&gt;raw&lt;/code&gt; with notes in our codebook on its origins and the actions taken to generate it. It can be a good idea to &lt;code&gt;.zip&lt;/code&gt; a raw file so that it is more difficult to access by accident.&lt;/p&gt;
&lt;p&gt;Going forward we would use the &lt;code&gt;spread_results&lt;/code&gt; and &lt;code&gt;unique_results&lt;/code&gt; for further work.&lt;/p&gt;
&lt;p&gt;As we did earlier, use either write.csv(x, “x.csv”, row.names = FALSE) or the simpler and faster &lt;code&gt;write_csv()&lt;/code&gt;. R can write multiple files in a blink. This will write all three files to the rplos project folder (use &lt;code&gt;getwd()&lt;/code&gt; and &lt;code&gt;setwd()&lt;/code&gt; if you want to do something different).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(results, &amp;quot;results.csv&amp;quot;)
write_csv(spread_results, &amp;quot;spread_results.csv&amp;quot;)
write_csv(unique_results, &amp;quot;unique_results.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, so we have now a dataset containing the records for a range of terms and we have come a long way. Quite a lot of this has been about what to do with PLOS data once we have accessed it in terms of turning it into tables that we can work with. In the next section we will look at how to restrict searches by section.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;restricting-searches-by-section&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Restricting searches by section&lt;/h2&gt;
&lt;p&gt;The default for searching with &lt;code&gt;rplos&lt;/code&gt; is to search everything. This can produce many passing results and be overwhelming. There are quite a number of options for restricting searches in &lt;code&gt;rplos&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;by-author&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;By author&lt;/h2&gt;
&lt;p&gt;In creating the results dataset above we included the &lt;code&gt;author&lt;/code&gt; field. However, there are some complexities to searching with author names and working with author data that it is important to understand. We will start by searching on author names and then look at how to process the data.&lt;/p&gt;
&lt;p&gt;To restrict a search by author name we can use either the full name or the surname:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plosauthor(q = &amp;quot;Paul Oldham&amp;quot;, fl = c(&amp;quot;author&amp;quot;, &amp;quot;id&amp;quot;), fq = &amp;quot;doc_type:full&amp;quot;, 
    limit = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example we have specified &lt;code&gt;doc_type:full&lt;/code&gt; to return only the results for full articles. If you do not use this then the search will return a large number of repeated results based on article sections. So, in this case, Paul Oldham - the author of this article on &lt;code&gt;rplos&lt;/code&gt; - has published two articles in PLOS ONE. If &lt;code&gt;doc_type:full&lt;/code&gt; isn’t specified more than 20 results are returned that display different sections of the two articles. This will create a duplication issue later on, so a sensible default approach is to use &lt;code&gt;doc_type:full&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As a general observation, considerable caution should be exercised when working with author names because of problems with the lumping of names and splitting of names as described in this &lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0070299&#34;&gt;PLOS ONE article&lt;/a&gt;. If a large number of results are encountered on a single author name consider using match criteria from other available data fields to ensure that separate persons are not being lumped together by name. Above all, do not assume that simply because a name is the same, or very similar to the target name, that the name designates the same person.&lt;/p&gt;
&lt;p&gt;The next issue we need to address is what to do with the author data when we have retrieved it. The reason for this is that the author field in the results is generally a concatenated field containing the names of the authors of a particular article. We will start with the &lt;code&gt;oldham&lt;/code&gt; results set.&lt;/p&gt;
&lt;p&gt;In this case we will make the call to &lt;code&gt;plosauthor()&lt;/code&gt; and then use &lt;code&gt;ldply()&lt;/code&gt; from &lt;code&gt;plyr&lt;/code&gt; to return a data frame containing &lt;code&gt;meta&lt;/code&gt; and &lt;code&gt;data&lt;/code&gt;. Then we will use &lt;code&gt;fill&lt;/code&gt; from &lt;code&gt;tidyr&lt;/code&gt; to take the &lt;code&gt;numFound&lt;/code&gt; and fill down that column. We will remove the start column using &lt;code&gt;select()&lt;/code&gt; and finally &lt;code&gt;filter()&lt;/code&gt; to limit the table to data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;oldham &amp;lt;- plosauthor(q = &amp;quot;Paul Oldham&amp;quot;, fl = c(&amp;quot;author&amp;quot;, &amp;quot;id&amp;quot;), fq = &amp;quot;doc_type:full&amp;quot;, limit = 20) %&amp;gt;%
  ldply(&amp;quot;[&amp;quot;, 1:2) %&amp;gt;%
  fill(numFound, start) %&amp;gt;%
  select(- start) %&amp;gt;%
  filter(.id == &amp;quot;data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a two records with the author and id (DOI) data. The next thing we want to do is to separate the author names out. We can do this using &lt;code&gt;separate()&lt;/code&gt;. Note that &lt;code&gt;separate()&lt;/code&gt; will need to know the number of names involved before hand. In the oldham data case there are three authors of each article. We will deal with how to calculate the number of author names shortly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;oldham &amp;lt;- separate(oldham, author, 1:3, sep = &amp;quot;;&amp;quot;, remove = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have some other choices. We could simply keep only the first author name. To do that, in this particular case, we could use &lt;code&gt;select()&lt;/code&gt; and the numeric position of the columns that we want to remove.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;first_author &amp;lt;- select(oldham, -7, -8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an alternative, we could place each author name on its own row so that we can focus in on a specific author later. For that we can use &lt;code&gt;gather()&lt;/code&gt; from &lt;code&gt;tidyr&lt;/code&gt; and the column position numbers (not their names in this case) of the columns we want to gather.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;authors &amp;lt;- gather(oldham, number, authors, 5:7)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As above &lt;code&gt;gather()&lt;/code&gt; requires a key and value field. In this case we have used the number as our key and authors as our value. We have then specified that we want to gather columns 6 to 8 into the new column authors.&lt;/p&gt;
&lt;p&gt;That was easy because we are dealing with a small number of results with a uniform number of authors. However, our &lt;code&gt;results&lt;/code&gt; data is more complicated than this because we have multiple author names for each article and the number of authors for the articles could vary considerably.&lt;/p&gt;
&lt;p&gt;We will need to organise the data and to run some simple calculations to make this work. This will take six steps. The full working code is below.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We calculate the number of columns in our dataset. We do this because the number may vary depending on what fields we retrieve from &lt;code&gt;rplos&lt;/code&gt;. We will use &lt;code&gt;ncols()&lt;/code&gt; to make the calculation.&lt;/li&gt;
&lt;li&gt;We use a short function from &lt;code&gt;stringr&lt;/code&gt; to calculate the number of authors based on the author name separator “;” (+1 to capture the final names in the sequence). This gives us the maximum number of authors across the dataset that we need to split the data into (in this case 83 as the value of n). Copy and paste the function below into the console to access it.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;author_count &amp;lt;- function(data, col = &amp;quot;&amp;quot;, sep = &amp;quot;[^[:alnum:]]+&amp;quot;) {
    library(stringr)
    authcount &amp;lt;- str_count(data[[col]], pattern = sep)
    n &amp;lt;- as.integer(max(authcount) + 1)
    print(n)
}&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We use &lt;code&gt;select()&lt;/code&gt; from &lt;code&gt;dplyr&lt;/code&gt; to move our target column to the first column. This simply makes it easier to specify column positions in &lt;code&gt;separate()&lt;/code&gt; and &lt;code&gt;gather()&lt;/code&gt; later on.&lt;/li&gt;
&lt;li&gt;We use the value of &lt;code&gt;n&lt;/code&gt; to separate the author names into multiple columns&lt;/li&gt;
&lt;li&gt;We then gather them back in using the value of &lt;code&gt;n&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Splitting on a separator such as &lt;code&gt;;&lt;/code&gt; normally generates invisible leading and trailing white space. This will prevent author names from ranking correctly (e.g. in Excel or Tableau). The &lt;code&gt;str_trim()&lt;/code&gt; function from &lt;code&gt;stringr&lt;/code&gt; provides an easy way of removing the white space (specify side as right, left or both).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Copy and paste the code below and then hit Enter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#---calculations---
colno &amp;lt;- ncol(unique_results)  #calculate number of columns
n &amp;lt;- author_count(unique_results, &amp;quot;author&amp;quot;, &amp;quot;;&amp;quot;)  # See function above. Calculate n as an integer to meet requirement for separate()
#---select, separate and gather---
full_authors &amp;lt;- select(unique_results, author, 1:colno)  #bring author to the front
full_authors &amp;lt;- separate(full_authors, author, 1:n, sep = &amp;quot;;&amp;quot;, remove = TRUE, 
    convert = FALSE, extra = &amp;quot;merge&amp;quot;, fill = &amp;quot;right&amp;quot;)  #separate
full_authors &amp;lt;- gather(full_authors, value, authors, 1:n, na.rm = TRUE)  #gather
#---trim authors----
full_authors$authors &amp;lt;- str_trim(full_authors$authors, side = &amp;quot;both&amp;quot;)  #trim leading and trailing whitespace&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can simplify this with pipes to bring together the actions on the new full_authors object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#---calculations---
colno &amp;lt;- ncol(unique_results)
n &amp;lt;- author_count(unique_results, &amp;quot;author&amp;quot;, &amp;quot;;&amp;quot;)
#---select, separate, gather---
full_authors &amp;lt;- select(unique_results, author, 1:colno) %&amp;gt;% separate(author, 
    1:n, sep = &amp;quot;;&amp;quot;, remove = TRUE, convert = FALSE, extra = &amp;quot;merge&amp;quot;, fill = &amp;quot;right&amp;quot;) %&amp;gt;% 
    gather(value, authors, 1:n, na.rm = TRUE)
#---trim authors----
full_authors$authors &amp;lt;- str_trim(full_authors$authors, side = &amp;quot;both&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In running this code we will remove the original author column (column 1) by specifying &lt;code&gt;remove = TRUE&lt;/code&gt; in &lt;code&gt;separate()&lt;/code&gt;. &lt;code&gt;gather()&lt;/code&gt; will place the new &lt;code&gt;authors&lt;/code&gt; column at the end. So, make sure you scroll to the final column when viewing the results. We could also drop unwanted columns.&lt;/p&gt;
&lt;p&gt;We now have a complete list of individual author names that could be used to look up individual authors, to clean up author names for statistical use and for author network mapping. As a brief example, if we wanted to look up contributions by Jean Peccoud who leads the &lt;a href=&#34;http://blogs.plos.org/synbio/&#34;&gt;PLOS SynBio blog&lt;/a&gt; we might use the following based on this useful &lt;a href=&#34;http://stackoverflow.com/questions/22850026/filtering-row-which-contains-a-certain-string-using-dplyr&#34;&gt;Stack Overflow answer&lt;/a&gt;. See ?grepl for more info.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Peccoud &amp;lt;- filter(full_authors, grepl(&amp;quot;Peccoud&amp;quot;, authors))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will not go into depth on these topics, but generating this type of author list is an important step in enabling wider analytics and visualisation. While the code used to get to this list of authors may appear quite involved, once the basics are understood it can be used over and over again.&lt;/p&gt;
&lt;p&gt;Let’s write that data to a .csv file to explore later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(full_authors, &amp;quot;full_authors.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;title-search-using-plostitle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Title search using &lt;code&gt;plostitle()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;For a title search we can use &lt;code&gt;plostitle()&lt;/code&gt;. As above you may want to count the number of records first using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t &amp;lt;- plostitle(q = &amp;quot;synthetic biology&amp;quot;, limit = 0)$meta$numFound&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we run the search to return the number of results we would like. Here we have set it to the value of t above (11). We have limited the results to the data field by subsetting with $data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;title &amp;lt;- plostitle(q = &amp;quot;synthetic biology&amp;quot;, fl = &amp;quot;title&amp;quot;, limit = t)$data&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;abstract-search-using-plosabstract&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Abstract search using &lt;code&gt;plosabstract()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;For confining the searches to abstracts we can use &lt;code&gt;plosabstract()&lt;/code&gt;. We will start with a quick count of records.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- plosabstract(q = &amp;quot;synthetic biology&amp;quot;, limit = 0)$meta$numFound&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To retrieve the results we could use the value of &lt;code&gt;a&lt;/code&gt;. As an alternative we could set it arbitrarily high and the correct results will be returned. Of course if we don’t know what the total number of results are then we will be unsure whether we have captured the universe. But, an arbitrary number can be useful for exploration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abstract &amp;lt;- plosabstract(q = &amp;quot;synthetic biology&amp;quot;, fl = &amp;quot;id, title, abstract&amp;quot;, 
    limit = 200)
abstract$data&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As before, we can easily create a new object containing the data.frame. In this case we will also include the meta data and then use &lt;code&gt;fill()&lt;/code&gt; from &lt;code&gt;tidyr()&lt;/code&gt; to fill down the &lt;code&gt;numFound&lt;/code&gt; field and the start with 0. Note that &lt;code&gt;meta&lt;/code&gt; will appear at the top of the list and will create a largely blank row. To avoid this, while keeping number of records for reference, we will use filter from &lt;code&gt;tidyr()&lt;/code&gt;. This short code will do that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abstract_df &amp;lt;- ldply(abstract, &amp;quot;[&amp;quot;, 1:2) %&amp;gt;% fill(numFound, start) %&amp;gt;% filter(.id == 
    &amp;quot;data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;subject-area-using-plossubject&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Subject Area using &lt;code&gt;plossubject()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;To search by subject area use &lt;code&gt;plossubject&lt;/code&gt;. The default return is 10 results of the total results. So, try starting with a search such as this to get an idea of how many results there are. In this case the query has been limited to PLOS ONE and full text articles.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sa &amp;lt;- plossubject(q = &amp;quot;\&amp;quot;synthetic+biology\&amp;quot;&amp;quot;, fq = list(&amp;quot;cross_published_journal_key:PLoSONE&amp;quot;, 
    &amp;quot;doc_type:full&amp;quot;))$meta$numFound&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At the time of writing this returns 739 results. We will simply pull back 10 results. To pull back all of the results replace 10 with &lt;code&gt;sa&lt;/code&gt; above or type the number into &lt;code&gt;limit =&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plossubject(q = &amp;quot;\&amp;quot;synthetic+biology\&amp;quot;&amp;quot;, fl = &amp;quot;id&amp;quot;, fq = list(&amp;quot;cross_published_journal_key:PLoSONE&amp;quot;, 
    &amp;quot;doc_type:full&amp;quot;), limit = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As noted in the documentation, the results we return from the API and the results on the website are not necessarily the same because the settings used by PLOS on the website are not clear.&lt;/p&gt;
&lt;p&gt;In this case we return 740 results while, at the time of writing, PLOS ONE lists 417 articles in the &lt;a href=&#34;http://www.plosone.org/browse/Synthetic+biology?startPage=0&amp;amp;filterAuthors=&amp;amp;filterSubjectsDisjunction=&amp;amp;filterArticleTypes=&amp;amp;pageSize=13&amp;amp;filterKeyword=&amp;amp;filterJournals=PLoSONE&amp;amp;query=&amp;amp;ELocationId=&amp;amp;id=&amp;amp;resultView=&amp;amp;sortValue=&amp;amp;unformattedQuery=*%3A*&amp;amp;sortKey=Most+views%2C+all+time&amp;amp;filterSubjects=Synthetic%20biology&amp;amp;volume=&amp;amp;&#34;&gt;Synthetic Biology subject area&lt;/a&gt;. This will merit clarification of the criteria for counts used on the PLOS website and the API returns.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;highlighting-terms-and-text-fragments-with-highplos&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Highlighting terms and text fragments with highplos()&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;highplos()&lt;/code&gt; is a great function for research in PLOS, particularly when combined with opening results in a browser using &lt;code&gt;highbrow()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Highlighting will pull back a chunk of text with the search term highlighted with the emphasis &lt;em&gt; &lt;/em&gt; tag enclosing the individual words in a search phrase. It is possible that an entire phrase can be highlighted (see hl.usePhraseHighlighter) but this requires further exploration.&lt;/p&gt;
&lt;p&gt;In this example we will simply use the term synthetic biology and then highlight the terms in the abstract &lt;code&gt;hl.fl =&lt;/code&gt; and limit this to 10 rows of results. We will also add the function &lt;code&gt;highbrow()&lt;/code&gt; (for highlight browse) at the end. This will open the results in our browser. In the examples we use a pipe (%&amp;gt;%) meaning &lt;code&gt;this %then% that&lt;/code&gt;. This means that we do not have to enter the name snippet into the highbrow function and simplifies the code.&lt;/p&gt;
&lt;p&gt;When reviewing the results in a browser note that we can click on the DOI to see the full article. This is a really useful tool for assessing which articles we might want to take a closer look at.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;highplos(q = &amp;#39;&amp;quot;synthetic biology&amp;quot;&amp;#39;, hl.fl = &amp;#39;abstract&amp;#39;, fq = &amp;quot;doc_type:full&amp;quot;, rows = 10) %&amp;gt;%
  highbrow() #launches the browser&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that in some cases, even though we are restricting to &lt;code&gt;doc-type:full&lt;/code&gt;, we retrieve entries with no data. In one case this is because we are highlighting terms in the abstract when the term appears in the full text. In a second case we have picked up a correction where one of the authors is at a synthetic biology centre but neither the abstract or text mention synthetic biology. So, bear in mind that some further exploration may be required to understand why particular results are being returned. These issues are minor and this is a great tool.&lt;/p&gt;
&lt;p&gt;There are two additional options (arguments) for &lt;code&gt;highplos()&lt;/code&gt; that we can use. The first of these is snippets using &lt;code&gt;hl.snippets =&lt;/code&gt; and the second is &lt;code&gt;hl.fragsize =&lt;/code&gt;. Both can be used in conjunction with &lt;code&gt;highbrow()&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;snippets-using-hl.snippets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Snippets using hl.snippets&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;snippet &amp;lt;- highplos(q = &amp;#39;&amp;quot;synthetic biology&amp;quot;&amp;#39;, hl.fl = list(&amp;quot;title&amp;quot;, &amp;quot;abstract&amp;quot;), hl.snippets = 10, rows = 100) %&amp;gt;%
  highbrow()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The snippets argument is handy (the default value for a snippet is 1 but goes up to as many as you like). It become very interesting when we add &lt;code&gt;hl.mergeContiguous = &#39;true&#39;&lt;/code&gt;. This will display the entries captured in the order of the articles to provide a sense of its uses by the author(s).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;highplos(q=&amp;#39;&amp;quot;synthetic biology&amp;quot;&amp;#39;, hl.fl = &amp;quot;abstract&amp;quot;, hl.snippets = 10, hl.mergeContiguous = &amp;#39;true&amp;#39;, rows = 10) %&amp;gt;%
  highbrow()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fragment-size-using-hl.fragsize&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;fragment size using hl.fragsize&lt;/h3&gt;
&lt;p&gt;Greater control over what we are seeing is provided using the &lt;code&gt;hl.fragsize&lt;/code&gt; option. This allows us to specify the number of characters (including spaces) that we want to see in relation to our target terms.&lt;/p&gt;
&lt;p&gt;In the first example we will highlight the phrase synthetic biology in the titles and abstracts and set the fragment size (using hl.fragsize ) to a high 500. This will return the first 500 characters including spaces rather than words. We will set the number of rows to a somewhat arbitrary 200. This can easily be pushed a lot higher but expect to wait for a few moments if you move this to 1000 rows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;highplos(q = &amp;#39;&amp;quot;synthetic biology&amp;quot;&amp;#39;, hl.fl = list(&amp;quot;title&amp;quot;, &amp;quot;abstract&amp;quot;), hl.fragsize = 500, rows = 200) %&amp;gt;%
  highbrow()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also do the reverse of a larger search by reducing the fragment size to say up to 100 characters. At the moment it is unclear whether it is possible to control whether characters are selected to the right or the left of our target terms. Note that results will display up to 100 characters where they are available (short results will be for sentences such as titles that are less than 100 characters)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;highplos(q = &amp;#39;&amp;quot;synthetic biology&amp;quot;&amp;#39;, hl.fl = list(&amp;quot;title&amp;quot;, &amp;quot;abstract&amp;quot;), hl.fragsize = 100, rows = 200) %&amp;gt;%
  highbrow()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is great about this is that we can easily control the amount of text that we are seeing and then select articles of interest to read straight from the browser. We can also start to think about ways to use this information for text mining to identify terms used in conjunction with synthetic biology or types of synthetic biology.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;get-the-full-text-of-one-or-more-articles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Get the full text of one or more articles&lt;/h2&gt;
&lt;p&gt;We will finish this article by briefly demonstrating how to retrieve and save the full text of one or more articles. &lt;code&gt;rplos&lt;/code&gt; uses a combination of the &lt;code&gt;XML&lt;/code&gt; and the &lt;code&gt;tm&lt;/code&gt; (for text mining) package.&lt;/p&gt;
&lt;p&gt;Retrieving full text should initially be used rather sparingly because you could pull back a lot of data in XML format that you may then struggle to process. So, it is probably best to start small.&lt;/p&gt;
&lt;p&gt;Using the unique_results data that we created above we have a list of DOIs in the id field. We can create a vector of these using the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;doi &amp;lt;- unique_results$id&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That has created a vector of 1097 dois. To limit those results, let’s create a shorter version where we select five rows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;short_doi &amp;lt;- doi[1:5]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can use &lt;code&gt;plos_fulltext()&lt;/code&gt; to retrieve the full text.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ft &amp;lt;- plos_fulltext(short_doi)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we pull back the two articles an object is created of class &lt;code&gt;plosft&lt;/code&gt;. To see the full text of one of the individual articles we use the trusty &lt;code&gt;$&lt;/code&gt; and then select a doi.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ft$`10.1371/journal.pone.0140969`&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This displays a lot of the XML tags inside the text. We would now like to extract the text without the XML tags. The &lt;code&gt;rplos&lt;/code&gt; documentation for &lt;code&gt;plos_fulltext()&lt;/code&gt; helps us to do this using the following code. The first part of the code uses the XML package to parse the results removing the xml tags in the process.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tm)
library(XML)
ft_parsed &amp;lt;- lapply(ft, function(x) {
    xpathApply(xmlParse(x), &amp;quot;//body&amp;quot;, xmlValue)
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we type &lt;code&gt;ft_parsed&lt;/code&gt; we will now see the text (the body without title and abstract) fly by without all of the tags.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ft_parsed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The object returned by this is a list (use &lt;code&gt;class(ft_parsed)&lt;/code&gt;). Next, we can transform this into a corpus (a text or collection of texts) that we can save to disk using the following code from the &lt;code&gt;rplos&lt;/code&gt; &lt;code&gt;plos_fulltext()&lt;/code&gt; example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmcorpus &amp;lt;- Corpus(VectorSource(ft_parsed))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we type tmcorpus$ into the console then we will see 1 to 5 pop up, but this will return NULL if selected. The data is there but we need to use &lt;code&gt;str(tmcorpus&lt;/code&gt;) to see the structure of the corpus. If we want to view a text within the corpus we can use &lt;code&gt;writeLines()&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writeLines(as.character(tmcorpus[[2]]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also view the five texts in our corpus (be prepared for a lot of scrolling) by using lapply to read over the two texts as character.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(tmcorpus[1:5], as.character)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more information see the &lt;a href=&#34;https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf&#34;&gt;Ingo Feinerer (2015) Introduction to the tm package&lt;/a&gt; (also available in the tm documentation) from which the above is drawn.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;writing-a-corpus-to-disk&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Writing a corpus to disk&lt;/h2&gt;
&lt;p&gt;To write a corpus we first need to create a folder where the files will be housed (otherwise they will simply be written into your project folder with everything else).&lt;/p&gt;
&lt;p&gt;The easiest way to create a new folder is to head over to the Files Tab in RStudio (normally in the bottom right pane) and choose &lt;code&gt;New Folder&lt;/code&gt;. We will call it &lt;code&gt;tm&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now use &lt;code&gt;getwd()&lt;/code&gt; and copy the file path into the following function, from the writeCorpus examples, adding &lt;code&gt;/tm&lt;/code&gt; at the end. It will look something like this but replace the path with your own, not forgetting the &lt;code&gt;/tm&lt;/code&gt;. Then press Enter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writeCorpus(tmcorpus, path = &amp;quot;/Users/paul/Desktop/open_source_master/rplos/tm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When you look in the tm folder inside rplos (use the Files tab in RStudio) you will now see five texts with the names 1 to 5. For more details, such as naming files and specifying file types, see &lt;code&gt;?writeCorpus&lt;/code&gt; and the tm package documentation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round Up&lt;/h2&gt;
&lt;p&gt;In this chapter we have focused on using the &lt;code&gt;rplos&lt;/code&gt; package to access scientific articles from the Public Library of Science (PLOS). As we have seen, with short pieces of code it is easy to search and retrieve data from PLOS on a whole range of subjects whether it be pizza or synthetic biology.&lt;/p&gt;
&lt;p&gt;One of the most powerful features of R is that it is quite easy to access free online data using APIs. &lt;code&gt;rplos&lt;/code&gt; is a very good starting point for learning how to retrieve data using an API because it is well written and the data that comes back is remarkably clean.&lt;/p&gt;
&lt;p&gt;Perhaps the biggest challenge facing new users of R is what to do with data once you have retrieved it. This can result in many hours of frustration staring at a list or object with the data you need without the tools to access it and transform it into the format you need. In this article we have focused on using the &lt;code&gt;plyr&lt;/code&gt;, &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;tidyr&lt;/code&gt; and &lt;code&gt;stringr&lt;/code&gt; suite of packages to turn &lt;code&gt;rplos&lt;/code&gt; data into something you can use. These packages are rightly very popular for everyday work in R and becoming more familiar with them will reap rewards in learning R for practical work. At the close of the article we used the &lt;code&gt;tm&lt;/code&gt; (text mining) package to save the full text of articles. This is only a very small part of this package and &lt;code&gt;rplos&lt;/code&gt; provides some useful examples to begin text mining using &lt;code&gt;tm&lt;/code&gt; (see the &lt;code&gt;plos_fulltext()&lt;/code&gt; examples). R now has a rich range of text mining packages and we will address this in a future article.&lt;/p&gt;
&lt;p&gt;In the meantime, if you would like to learn more about R try the resources below. If you would like to learn R inside R then try the very useful Swirl package (details below).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;resources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Resources&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://ropensci.org/&#34;&gt;rOpenSci&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cookbook-r.com/&#34;&gt;Winston Chang’s R Cookbook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rstudio.com/resources/training/online-learning/&#34;&gt;RStudio Online Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.r-bloggers.com/&#34;&gt;r-bloggers.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/&#34;&gt;Datacamp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Swirl (developed by the &lt;a href=&#34;https://www.coursera.org/course/rprog&#34;&gt;free Coursea R Programming course&lt;/a&gt; team at John Hopkins University. If you would like to get started with Swirl run the code chunk below to install the package and load the library.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;swirl&amp;quot;)
library(swirl)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Patent Datasets</title>
      <link>/patent-datasets/</link>
      <pubDate>Fri, 07 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>/patent-datasets/</guid>
      <description>&lt;!---update the datasets locations---&gt;
&lt;p&gt;One problem for people seeking to learn patent analytics is a lack of access to patent data from different sources.&lt;/p&gt;
&lt;p&gt;In this article I introduce the patent datasets developed for the &lt;a href=&#34;https://github.com/wipo-analytics&#34;&gt;WIPO Open Source Patent Analytics Project&lt;/a&gt; as training sets for patent analytics. The datasets will be used in the walkthroughs. The datasets will grow over time but we will briefly introduce them and explain how to access them.&lt;/p&gt;
&lt;p&gt;The datasets are housed at the project &lt;a href=&#34;https://github.com/wipo-analytics/opensource-patent-analytics/tree/master/2_datasets&#34;&gt;GitHub repository&lt;/a&gt;. To download individual files click on the link and then select raw to download the file.&lt;/p&gt;
&lt;div id=&#34;the-datasets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The datasets&lt;/h2&gt;
&lt;p&gt;The datasets are intended to illustrate the range of possibilities for patent data including some of the challenges that may be encountered in cleaning and analysing patent data. They are also drawn from different sources.&lt;/p&gt;
&lt;div id=&#34;pizza-patent-datasets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pizza patent datasets&lt;/h3&gt;
&lt;p&gt;Almost everyone likes pizza and it is easy to search a patent database for the term “pizza”. It is also an area of patent activity that encompasses a wide range of technologies such as pizza ovens, pizza boxes, pizza cutters and pizza toppings etc. It is therefore useful for demonstrating ways of interrogating patent data for particular topics.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;pizza_small&lt;/code&gt; is a very small 26 row dataset created by downloading the first page of results from the &lt;a href=&#34;http://worldwide.espacenet.com/?locale=en_EP&#34;&gt;European Patent Office espacenet database&lt;/a&gt; for a smart search on “pizza”. It’s a quick and easy test dataset.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pizza_medium&lt;/code&gt; was created from a sample of data from a search of the &lt;a href=&#34;http://www.wipo.int/patentscope/en/&#34;&gt;WIPO Patentscope database&lt;/a&gt; for the term “pizza” and contains 9,996 rows of data. It is intended to illustrate the data format from Patentscope and to allow work on a medium sized dataset. Note that the format varies from the espacenet format and presents different challenges.
An important feature of Patentscope data from a statistical standpoint is that the field marked &lt;code&gt;publication_number&lt;/code&gt; in the original data lacks a two letter kind code and is therefore an &lt;code&gt;application_number&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;pizza_medium_clean&lt;/code&gt; dataset is a precleaned version of the &lt;code&gt;pizza_medium&lt;/code&gt; dataset. Specifically, the applicants and inventors field have already been cleaned along with corrupted characters and other common cleaning tasks. This makes it easier to work with the data and this dataset is the core dataset in the Manual. As above, note that the Patentscope publication_number field more properly refers to an application number in the absence of a kind code.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pizza_sliced&lt;/code&gt; is a set of five .csv files for a search of pizza on &lt;a href=&#34;http://worldwide.espacenet.com/searchResults?ST=singleline&amp;amp;locale=en_EP&amp;amp;submitted=true&amp;amp;DB=worldwide.espacenet.com&amp;amp;query=pizza&amp;amp;Submit=Search&#34;&gt;espacenet&lt;/a&gt;. It is designed to illustrate issues involved in loading multiple files into R. It also illustrates problems with character corruption and the importance of pre-cleaning data before analysis.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pizza_lens_1000&lt;/code&gt; is a raw dataset of 1000 records including the term pizza downloaded from &lt;a href=&#34;https://www.lens.org/lens/&#34;&gt;The Lens&lt;/a&gt; database. The dataset has not been cleaned.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;patent-landscape-reports-datasets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Patent Landscape Reports datasets&lt;/h3&gt;
&lt;p&gt;Three datasets are drawn from the &lt;a href=&#34;http://www.wipo.int/patentscope/en/programs/patent_landscapes/&#34;&gt;WIPO Patent Landscape Reports&lt;/a&gt;. The datasets address different topics, present a variety of fields and formats and are different sizes. Each dataset is linked to a detailed patent landscape report that provides an insight into approaches to patent analytics.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;ewaste&lt;/code&gt; presents the results of research for a &lt;a href=&#34;http://www.wipo.int/patentscope/en/programs/patent_landscapes/reports/ewaste.html&#34;&gt;report&lt;/a&gt; on patent activity for electronic waste recycling and its implications for developing countries.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;solar_cooking&lt;/code&gt; presents the data supporting a &lt;a href=&#34;http://www.wipo.int/patentscope/en/programs/patent_landscapes/reports/solar_cooking.html&#34;&gt;report&lt;/a&gt; on technologies that use solar energy as the source for cooking and pasteurizing food.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ritonavir&lt;/code&gt; presents the data for a patent &lt;a href=&#34;http://www.wipo.int/patentscope/en/programs/patent_landscapes/reports/ritonavir.html&#34;&gt;report&lt;/a&gt; on patent activity for the HIV antiretroviral drug Ritonavir in the field of pharmaceuticals. The dataset illustrates specific activity around issues such as dosage and also the problem of ‘evergreening’ in patent activity.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;other-datasets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other datasets&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;wipo&lt;/code&gt; is a single Excel sheet of data on trends in patent applications and growth rates from the &lt;a href=&#34;http://www.wipo.int/ipstats/en/wipi/&#34;&gt;WIPO World Intellectual Property Indicators - 2014 Edition&lt;/a&gt;. The data is used for simple graphing in tools such as R and illustrates the need to skip rows when reading data into analytics tools.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;WIPO_sequence_data&lt;/code&gt;. This dataset contains a small sample of the sequence data from the year 2000 available free of charge from the &lt;a href=&#34;https://patentscope.wipo.int/search/en/sequences.jsf&#34;&gt;WIPO Patentscope database&lt;/a&gt;. This dataset can be used to explore analysis of patent sequence data.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/tree/master/2_datasets/synbio_patents&#34;&gt;Synthetic biology&lt;/a&gt;. This is a sample of data from Thomson Innovation developed by Paul Oldham for research on patent activity involving synthetic biology. The data has been extensively cleaned in VantagePoint from Search Technology Inc. and is intended to illustrate the use of data from a commercial patent database.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Round Up&lt;/h3&gt;
&lt;p&gt;The datasets section of the project provides a series of useful training sets from a variety of sources and displaying a variety of features. These are open access datasets that can be used to test different approaches but please credit their sources. More datasets may be added to the online version of the Manual in due course. We are particularly interested in sample data from STN, QuestelOrbit, PATSTAT or other data providers that can be used as training sets.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An Introduction to Plotly for Patent Analytics</title>
      <link>/an-introduction-to-plotly-for-patent-analytics/</link>
      <pubDate>Sat, 01 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>/an-introduction-to-plotly-for-patent-analytics/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/plotly-binding/plotly.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/typedarray/typedarray.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/crosstalk/css/crosstalk.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/crosstalk/js/crosstalk.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/plotlyjs/plotly-htmlwidgets.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/plotlyjs/plotly-latest.min.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In this article we provide a quick introduction to the online graphing service &lt;a href=&#34;http://plotly.com&#34;&gt;Plotly&lt;/a&gt; to create graphics for use in patent analysis.&lt;/p&gt;
&lt;p&gt;Plotly is an online graphing service that allows you to import excel, text and other files for visualisation. It also has API services for R, Python, MATLAB and a Plotly Javascript Library. This means that you can send data directly to Plotly for visualisation. Plots can also be shared with team mates or publicly.&lt;/p&gt;
&lt;p&gt;Plotly’s great strength is that it produces attractive interactive graphics that can easily be shared with colleagues or made public. It also has a wide variety of graph types including contour and heatmaps. For examples of graphs created with Plotly see the &lt;a href=&#34;https://plot.ly/feed/&#34;&gt;public gallery&lt;/a&gt;. However, we experienced significant difficulty in loading patent data into Plotly and we struggled to draw graphs that would take a couple of minutes if we used Tableau Public as our benchmark. This suggests a need for investment of more time in understanding the expectations of the tool to make best use of the service. It may also suggest that while Plotly can produce excellent graphics, the service may need to invest more time in explaining how ordinary human beings might format their data to meet its expectations. In short, as is often the case with open source tools, the basic documentation could be better. Do not let this put you off. Plotly is potentially a great tool for creating and sharing graphics. In the rest of this article we will help you get started as a basis for exploring Plotly on your own.&lt;/p&gt;
&lt;div id=&#34;getting-started-with-plotly&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Getting Started with Plotly&lt;/h3&gt;
&lt;p&gt;We need to start out by creating an account using the Create Account Button.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig1_plotly.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next we will see an invitation to take a tour (which is worth doing) and Plotly helpfully points out that we can load files from Google Drive or Dropbox. We then select the Workspace option to begin work.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;importing-files&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Importing Files&lt;/h3&gt;
&lt;p&gt;When you first arrive you will see a Workspace with a Grid (Plotly’s term for a table or worksheet).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig2_workspace.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the workspace you will see an Import Icon that provides a range of options for importing data. Don’t import anything yet! You can also copy data from a file and paste it into the Grid.&lt;/p&gt;
&lt;p&gt;The reason not to use these options at the moment is that while the data may import fine first time, in other cases it will not. Using the options on this page you will receive no information if an import fails. We also encountered problems with saving data that had been pasted into the worksheet (even where it appeared to work). To avoid potential frustration head over to &lt;code&gt;Organize&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig3_import.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the Organize page select the New button and then Upload. Now select your local file. When you upload the file a status message will display and if all goes well then you will see a completed message. If not a red message will display informing you that there has been a problem (how you fix these problems is unclear)&lt;/p&gt;
&lt;p&gt;For this experiment we used two datasets from the Open Source Patent Analytics Manual &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/tree/master/2_datasets&#34;&gt;data repository&lt;/a&gt;. When using the Github repository click on the file of interest until you see a &lt;code&gt;View Raw&lt;/code&gt; message. Then right click to download the data file from there. You can download them for your own use directly from the following links.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/wipo/wipotrends_cleaned.csv?raw=true&#34;&gt;WIPO trends&lt;/a&gt; application trends by year and with % change&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/raw/master/2_datasets/pizza_medium_clean/pcy.csv&#34;&gt;Pizza patents by country and year&lt;/a&gt;. This is a simple dataset containing counts of patent documents containing the word pizza from &lt;a href=&#34;https://patentscope.wipo.int/search/en/search.jsf&#34;&gt;WIPO Patentscope&lt;/a&gt; broken down by country and year.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One important point to note is that Plotly is not a data processing tool. While there are some data tools, your data will generally need to be in a form that is suitable for plotting at the time of input. In part this reflects the use of APIs which allow for users of Python, R and Matlab to send their data to Plotly directly for sharing with others. This is one of the great strengths of Plotly and we will cover this below. However, we also experienced problems in loading and graphing datasets that were easy to work with in Tableau (as a benchmark). This suggests a need to invest time in understanding the formats that Plotly understands.&lt;/p&gt;
&lt;p&gt;We experienced a different type of problem with the simple WIPO trends data where Plotly concatenated the first row (containing labels) and the first data row into one heading row. However, in most cases import seemed to be fine. To turn a row into a heading row try right clicking the row with the headings and right clicking on &lt;code&gt;use row as col headers&lt;/code&gt;. Then right click again to remove the original row.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-a-graphic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating a graphic&lt;/h2&gt;
&lt;p&gt;We will start with the simple WIPO trends data by opening up that Grid&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig5_grid.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note here that in the Grid we have options to select the x or y axis for plotting. There is also an Options menu that we will come back to.&lt;/p&gt;
&lt;p&gt;The Type of plot can be changed by selecting the drop down menu as we can see below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig6_gtype.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Sticking with a line graph, when we create the plot we can add a title and then change the theme (in this case to Catherine).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig7_line.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We could also add a fit line by selecting the &lt;code&gt;FIT DATA&lt;/code&gt; menu icon. This will ask you to create a fit and then you have a range of preset functions or you can add your own. Here we have simply chosen the default Linear fit.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig8_fit.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can then save the plot and use the export button to save the plot in a variety of formats and sizes. It is also very easy to add annotations using the Notes icon. Confusingly, the large blue Share button only seems to save the file and despite saving the plot we were not able to locate it again. While Plotly certainly looks nice, and appears to have attractive functions it is not intuitive and the difficulties involved in importing and sharing can be frustrating and time consuming. In short, time is needed to invest in and explore the potential of this tool.&lt;/p&gt;
&lt;div id=&#34;adding-a-second-axis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding a Second Axis&lt;/h3&gt;
&lt;p&gt;If we go back to our original WIPO trends data we have a percentage score for the year on year change in patent applications. We might want to show this on a plot with a second axis for the percentage.&lt;/p&gt;
&lt;p&gt;To do that select the percentage as a second item for the y axis.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig9_secondy.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When we choose Line plot we will now see the two sets of data with the percentage trailing on the bottom. We now need to create a second y axis on the right and assignee the percentage data to that.&lt;/p&gt;
&lt;p&gt;To do this select the Traces icon and a menu will pop up showing the data traces. Select Growth Rate % from the Traces menu. Then where you see Lines/Markers select the dot. This will prevent the percentage scores displaying as a line.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig11_addaxis.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, select &lt;code&gt;New Axis/Subplot&lt;/code&gt; and a new screen will pop up. We have some choices here but will simply choose to create a new axis on the right.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig11_addyaxis.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The result will look something like this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig12_result1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our issue now is equalising the axes and changing the size of the points for the percentage scores. Finally we can add a title.&lt;/p&gt;
&lt;p&gt;Before we go any further let’s note that we have a significant minus axis value of -3.6% in 2009 when patent applications declined. There is also a minus value in 2002.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig13_dip.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we wanted to retain these values we would probably want to turn off the second set of grid lines. We would also want to resize the points.&lt;/p&gt;
&lt;p&gt;To turn of the grid lines on the second y axis Click on the Axes icon and then from the &lt;code&gt;All Axes&lt;/code&gt; drop down select Y Axis 2, then Lines and Grid lines OFF. Also turn the Zero line to off unless you want to retain it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig13_turnoffgrid.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To resize the points we need to go back to traces and select Growth Rate from the list of Traces. Then choose the Style tab and change the marker size to something larger such as 8.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig13_resizepoint.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can simply type in the Axis labels and a title into the text boxes provided. By choosing the Legend icon we could turn the legend on or off. Note that while this graph could be seen as self explanatory it may not be for the reader. We can also simply drag the axes labels to a different position.&lt;/p&gt;
&lt;p&gt;It is possible that we would want to remove the negative values from the plot (in that case the values would need to be explained in the accompanying text). To do that select &lt;code&gt;Axes&lt;/code&gt;, then &lt;code&gt;Y Axis2&lt;/code&gt; then in &lt;code&gt;Autorange&lt;/code&gt; choose &lt;code&gt;Non-negative&lt;/code&gt;to show only values over zero on the plot.&lt;/p&gt;
&lt;p&gt;If we wished we could also apply a fit line by choosing the FIT DATA icon. We will choose Linear.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig13_final.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, to finish off the plot we might want to add annotations using the NOTES icon. Simply click on the plus sign in the pop up menu for a new annotation and then select the arrow and text and move it into the position you want.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig13_final_annotated.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case we have added a couple of markers that may help to understand trends in activity. First, we have a dip in patent applications between 2001 and 2002. One possible explanation here is that this is a knock on effect of the collapse of the dot.com bubble where share prices reached a peak in 2000, declined rapidly and recovered before declining again into 2001. Patent data typically displays lag effects and it is reasonable to think that the decline in application activity from 2001 reflects these wider economic adjustments. Similarly, there is a significant dip in applications between 2008 and 2009 that it appears reasonable to assume reflects the knock on effects of the global economic crisis of 2007-2008. Note here that these are grosso modo way markers. We could choose to add other timeline style events or layer graphics to help understand the potential or actual relationships between wider economic activity and trends in patent applications worldwide.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;saving-and-sharing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Saving and Sharing&lt;/h3&gt;
&lt;p&gt;To save the plot we simply click Save. However, it is here that one of Plotly’s major strengths becomes apparent. As soon as we save the plot we can also invite others by email, we can create a public or private shareable link. For the collaborators, they must have a Plotly account already for this to work.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig13_email.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The next option is to share a link. Note here that the default is to share a private link. To change that select the lock icon. The private link is particularly well suited for patent professionals. You can visit the graph &lt;a href=&#34;https://plot.ly/~poldham/309/patent-applications-worldwide/&#34; class=&#34;uri&#34;&gt;https://plot.ly/~poldham/309/patent-applications-worldwide/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig13_privatelink.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You could also grab an embed code to embed the plot in a web page&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig13_embed.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Alternatively, surprise your friends and relatives by posting the plot on facebook or share with a wider audience on Twitter.&lt;/p&gt;
&lt;p&gt;In this example we have focused on developing a very simple plot using plotly. In practice there are a wide range of possible plotting options with a range of tutorials provided &lt;a href=&#34;http://help.plot.ly/&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;working-with-plotly-in-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Working with Plotly in R&lt;/h2&gt;
&lt;p&gt;We are following the instructions for setting up Plotly in R &lt;a href=&#34;https://plot.ly/r/getting-started/&#34;&gt;here&lt;/a&gt;. We will be using &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;RStudio&lt;/a&gt; for this experiment. Download RStudio for your operating system &lt;a href=&#34;https://www.rstudio.com/products/RStudio/&#34;&gt;here&lt;/a&gt;. For Python try these &lt;a href=&#34;https://plot.ly/python/getting-started/&#34;&gt;installation instructions&lt;/a&gt; to get started.&lt;/p&gt;
&lt;p&gt;In RStudio first we need to install or load the devtools package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;devtools&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then load the library.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(devtools)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The we install the plotly package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;ropensci/plotly&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we load the library it will load other required packages, note that you may need to install some of these packages if you don’t have them already. Use &lt;code&gt;install.packages(&amp;quot;ggplot2&amp;quot;)&lt;/code&gt; and so on in the Console if this happens and then load the libraries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plotly)  ## loads Plotly and the additional packages it needs.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Loading required package: RCurl
Loading required package: bitops
Loading required package: RJSONIO
Loading required package: ggplot2&lt;/p&gt;
&lt;p&gt;We now need to set our credentials for the API. Follow this &lt;a href=&#34;https://plot.ly/settings/api&#34;&gt;link&lt;/a&gt; to obtain your API key (when logged in to Plotly). Note also that you can obtain a streaming API token on the same page. Streaming will update a graphic from inside RStudio.&lt;/p&gt;
&lt;p&gt;Next save the username and key in your R profile as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Sys.setenv(plotly_username = &amp;quot;your_plotly_username&amp;quot;)
Sys.setenv(plotly_api_key = &amp;quot;your_api_key&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;!---To start plotting we will create an empty plotly object. We will call it pizza plot but you can call it what you like. 


```r
pizzaplot &lt;- ggplotly()
```

If we were to use class(pizzaplot) we would see that we have an object of &#34;PlotlyClass&#34;. ---&gt;
&lt;p&gt;Next we will use a quick plot or &lt;code&gt;qplot&lt;/code&gt; from &lt;code&gt;ggplot2&lt;/code&gt;. First we will load the pizza patents by country and year dataset from the Github repository using &lt;code&gt;readr&lt;/code&gt;. First you may need to install the &lt;code&gt;readr&lt;/code&gt; package and/or lead the library. If you have the tidyverse installed then you will simply need to load the library.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;readr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Load the library.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now read in the dataset from the Github respository (if you have it downloaded already you could load by inserting the path to your local file inside the quotes).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pcy &amp;lt;- read_csv(&amp;quot;https://github.com/poldham/opensource-patent-analytics/raw/master/2_datasets/pizza_medium_clean/pcy.csv&amp;quot;)
pcy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 325 x 4
##    pubcountry pubcode pubyear     n
##    &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;
##  1 Canada     CA         1968     1
##  2 Canada     CA         1971     2
##  3 Canada     CA         1972     4
##  4 Canada     CA         1974     1
##  5 Canada     CA         1975     1
##  6 Canada     CA         1976     1
##  7 Canada     CA         1977     1
##  8 Canada     CA         1978     4
##  9 Canada     CA         1979     8
## 10 Canada     CA         1980    11
## # ... with 315 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will now make a quick plot using &lt;code&gt;qplot&lt;/code&gt; from &lt;code&gt;ggplot2&lt;/code&gt;. The data is for trends in patent documents mentioning pizza from WIPO Patentscope. We have set a limit to the data for 1970 to 2012 to edit out sparse data and remove the data cliff for recent years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
pizza &amp;lt;- qplot(pubyear, n, data = pcy, geom = &amp;quot;line&amp;quot;, colour = pubcountry, xlim = c(1970, 
    2012))
pizza&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2015-08-01-an-introduction-to-plotly-for-patent-analytics_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we want to convert the ggplot into plotly we can use &lt;code&gt;ggplotly&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplotly(pizza)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;1473114e87109&#34; style=&#34;width:800px;height:480px;&#34; class=&#34;plotly html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;1473114e87109&#34;&gt;{&#34;x&#34;:{&#34;data&#34;:[{&#34;x&#34;:[1971,1972,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,null,null,null],&#34;y&#34;:[2,4,1,1,1,1,4,8,11,14,10,12,14,7,9,14,7,10,19,49,66,59,50,39,36,45,46,47,54,53,66,53,59,69,77,68,62,47,57,49,16,1,9,30],&#34;text&#34;:[&#34;~pubyear: 1971&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1972&lt;br /&gt;~n:   4&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1974&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1975&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1976&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1977&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1978&lt;br /&gt;~n:   4&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1979&lt;br /&gt;~n:   8&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1980&lt;br /&gt;~n:  11&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1981&lt;br /&gt;~n:  14&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1982&lt;br /&gt;~n:  10&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1983&lt;br /&gt;~n:  12&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1984&lt;br /&gt;~n:  14&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1985&lt;br /&gt;~n:   7&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1986&lt;br /&gt;~n:   9&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1987&lt;br /&gt;~n:  14&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1988&lt;br /&gt;~n:   7&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1989&lt;br /&gt;~n:  10&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1990&lt;br /&gt;~n:  19&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1991&lt;br /&gt;~n:  49&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1992&lt;br /&gt;~n:  66&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1993&lt;br /&gt;~n:  59&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1994&lt;br /&gt;~n:  50&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1995&lt;br /&gt;~n:  39&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1996&lt;br /&gt;~n:  36&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1997&lt;br /&gt;~n:  45&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1998&lt;br /&gt;~n:  46&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1999&lt;br /&gt;~n:  47&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 2000&lt;br /&gt;~n:  54&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 2001&lt;br /&gt;~n:  53&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 2002&lt;br /&gt;~n:  66&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 2003&lt;br /&gt;~n:  53&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 2004&lt;br /&gt;~n:  59&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 2005&lt;br /&gt;~n:  69&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 2006&lt;br /&gt;~n:  77&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 2007&lt;br /&gt;~n:  68&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 2008&lt;br /&gt;~n:  62&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 2009&lt;br /&gt;~n:  47&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 2010&lt;br /&gt;~n:  57&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 2011&lt;br /&gt;~n:  49&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 2012&lt;br /&gt;~n:  16&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 1968&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear: 2013&lt;br /&gt;~n:   9&lt;br /&gt;~pubcountry: Canada&#34;,&#34;~pubyear:   NA&lt;br /&gt;~n:  30&lt;br /&gt;~pubcountry: Canada&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(248,118,109,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;Canada&#34;,&#34;legendgroup&#34;:&#34;Canada&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[1996,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,null,null],&#34;y&#34;:[1,2,1,2,4,1,5,3,5,5,6,6,3,7,9,4,8],&#34;text&#34;:[&#34;~pubyear: 1996&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: China&#34;,&#34;~pubyear: 1999&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: China&#34;,&#34;~pubyear: 2000&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: China&#34;,&#34;~pubyear: 2001&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: China&#34;,&#34;~pubyear: 2002&lt;br /&gt;~n:   4&lt;br /&gt;~pubcountry: China&#34;,&#34;~pubyear: 2003&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: China&#34;,&#34;~pubyear: 2004&lt;br /&gt;~n:   5&lt;br /&gt;~pubcountry: China&#34;,&#34;~pubyear: 2005&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: China&#34;,&#34;~pubyear: 2006&lt;br /&gt;~n:   5&lt;br /&gt;~pubcountry: China&#34;,&#34;~pubyear: 2007&lt;br /&gt;~n:   5&lt;br /&gt;~pubcountry: China&#34;,&#34;~pubyear: 2008&lt;br /&gt;~n:   6&lt;br /&gt;~pubcountry: China&#34;,&#34;~pubyear: 2009&lt;br /&gt;~n:   6&lt;br /&gt;~pubcountry: China&#34;,&#34;~pubyear: 2010&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: China&#34;,&#34;~pubyear: 2011&lt;br /&gt;~n:   7&lt;br /&gt;~pubcountry: China&#34;,&#34;~pubyear: 2012&lt;br /&gt;~n:   9&lt;br /&gt;~pubcountry: China&#34;,&#34;~pubyear: 2013&lt;br /&gt;~n:   4&lt;br /&gt;~pubcountry: China&#34;,&#34;~pubyear: 2014&lt;br /&gt;~n:   8&lt;br /&gt;~pubcountry: China&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(230,134,19,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;China&#34;,&#34;legendgroup&#34;:&#34;China&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[1999,2006,2007],&#34;y&#34;:[2,1,1],&#34;text&#34;:[&#34;~pubyear: 1999&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Eurasian Patent Organization&#34;,&#34;~pubyear: 2006&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Eurasian Patent Organization&#34;,&#34;~pubyear: 2007&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Eurasian Patent Organization&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(205,150,0,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;Eurasian Patent Organization&#34;,&#34;legendgroup&#34;:&#34;Eurasian Patent Organization&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,null,null,null],&#34;y&#34;:[2,2,4,4,7,9,5,10,13,27,27,22,29,36,29,26,29,27,34,36,60,48,56,54,55,75,58,62,47,67,62,56,58,48,45,46,2],&#34;text&#34;:[&#34;~pubyear: 1979&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 1980&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 1981&lt;br /&gt;~n:   4&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 1982&lt;br /&gt;~n:   4&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 1983&lt;br /&gt;~n:   7&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 1984&lt;br /&gt;~n:   9&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 1985&lt;br /&gt;~n:   5&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 1986&lt;br /&gt;~n:  10&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 1987&lt;br /&gt;~n:  13&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 1988&lt;br /&gt;~n:  27&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 1989&lt;br /&gt;~n:  27&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 1990&lt;br /&gt;~n:  22&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 1991&lt;br /&gt;~n:  29&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 1992&lt;br /&gt;~n:  36&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 1993&lt;br /&gt;~n:  29&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 1994&lt;br /&gt;~n:  26&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 1995&lt;br /&gt;~n:  29&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 1996&lt;br /&gt;~n:  27&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 1997&lt;br /&gt;~n:  34&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 1998&lt;br /&gt;~n:  36&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 1999&lt;br /&gt;~n:  60&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 2000&lt;br /&gt;~n:  48&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 2001&lt;br /&gt;~n:  56&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 2002&lt;br /&gt;~n:  54&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 2003&lt;br /&gt;~n:  55&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 2004&lt;br /&gt;~n:  75&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 2005&lt;br /&gt;~n:  58&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 2006&lt;br /&gt;~n:  62&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 2007&lt;br /&gt;~n:  47&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 2008&lt;br /&gt;~n:  67&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 2009&lt;br /&gt;~n:  62&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 2010&lt;br /&gt;~n:  56&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 2011&lt;br /&gt;~n:  58&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 2012&lt;br /&gt;~n:  48&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 2013&lt;br /&gt;~n:  45&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 2014&lt;br /&gt;~n:  46&lt;br /&gt;~pubcountry: European Patent Office&#34;,&#34;~pubyear: 2015&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: European Patent Office&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(171,163,0,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;European Patent Office&#34;,&#34;legendgroup&#34;:&#34;European Patent Office&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[1971,1983,1984,1985,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012],&#34;y&#34;:[1,1,2,1,1,2,2,2,2,5,2,1,1,1,4,3,6,3,7,6,6,9,3,3,5,1,2,7],&#34;text&#34;:[&#34;~pubyear: 1971&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 1983&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 1984&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 1985&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 1989&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 1990&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 1991&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 1992&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 1993&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 1994&lt;br /&gt;~n:   5&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 1995&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 1996&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 1997&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 1998&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 1999&lt;br /&gt;~n:   4&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 2000&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 2001&lt;br /&gt;~n:   6&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 2002&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 2003&lt;br /&gt;~n:   7&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 2004&lt;br /&gt;~n:   6&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 2005&lt;br /&gt;~n:   6&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 2006&lt;br /&gt;~n:   9&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 2007&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 2008&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 2009&lt;br /&gt;~n:   5&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 2010&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 2011&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Germany&#34;,&#34;~pubyear: 2012&lt;br /&gt;~n:   7&lt;br /&gt;~pubcountry: Germany&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(124,174,0,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;Germany&#34;,&#34;legendgroup&#34;:&#34;Germany&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[1977,1980,1984,1985,1986,1989,1992,1995,1996,2000,2001,2002,2003,2004,2006,2007,2008,2009,2010,2011,2012],&#34;y&#34;:[1,1,1,1,1,1,1,1,1,1,3,3,1,6,2,2,1,1,2,3,1],&#34;text&#34;:[&#34;~pubyear: 1977&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Israel&#34;,&#34;~pubyear: 1980&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Israel&#34;,&#34;~pubyear: 1984&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Israel&#34;,&#34;~pubyear: 1985&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Israel&#34;,&#34;~pubyear: 1986&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Israel&#34;,&#34;~pubyear: 1989&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Israel&#34;,&#34;~pubyear: 1992&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Israel&#34;,&#34;~pubyear: 1995&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Israel&#34;,&#34;~pubyear: 1996&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Israel&#34;,&#34;~pubyear: 2000&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Israel&#34;,&#34;~pubyear: 2001&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: Israel&#34;,&#34;~pubyear: 2002&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: Israel&#34;,&#34;~pubyear: 2003&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Israel&#34;,&#34;~pubyear: 2004&lt;br /&gt;~n:   6&lt;br /&gt;~pubcountry: Israel&#34;,&#34;~pubyear: 2006&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Israel&#34;,&#34;~pubyear: 2007&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Israel&#34;,&#34;~pubyear: 2008&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Israel&#34;,&#34;~pubyear: 2009&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Israel&#34;,&#34;~pubyear: 2010&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Israel&#34;,&#34;~pubyear: 2011&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: Israel&#34;,&#34;~pubyear: 2012&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Israel&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(12,183,2,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;Israel&#34;,&#34;legendgroup&#34;:&#34;Israel&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011],&#34;y&#34;:[17,26,27,15,16,15,13,14,24,9,14,5,6,4],&#34;text&#34;:[&#34;~pubyear: 1998&lt;br /&gt;~n:  17&lt;br /&gt;~pubcountry: Japan&#34;,&#34;~pubyear: 1999&lt;br /&gt;~n:  26&lt;br /&gt;~pubcountry: Japan&#34;,&#34;~pubyear: 2000&lt;br /&gt;~n:  27&lt;br /&gt;~pubcountry: Japan&#34;,&#34;~pubyear: 2001&lt;br /&gt;~n:  15&lt;br /&gt;~pubcountry: Japan&#34;,&#34;~pubyear: 2002&lt;br /&gt;~n:  16&lt;br /&gt;~pubcountry: Japan&#34;,&#34;~pubyear: 2003&lt;br /&gt;~n:  15&lt;br /&gt;~pubcountry: Japan&#34;,&#34;~pubyear: 2004&lt;br /&gt;~n:  13&lt;br /&gt;~pubcountry: Japan&#34;,&#34;~pubyear: 2005&lt;br /&gt;~n:  14&lt;br /&gt;~pubcountry: Japan&#34;,&#34;~pubyear: 2006&lt;br /&gt;~n:  24&lt;br /&gt;~pubcountry: Japan&#34;,&#34;~pubyear: 2007&lt;br /&gt;~n:   9&lt;br /&gt;~pubcountry: Japan&#34;,&#34;~pubyear: 2008&lt;br /&gt;~n:  14&lt;br /&gt;~pubcountry: Japan&#34;,&#34;~pubyear: 2009&lt;br /&gt;~n:   5&lt;br /&gt;~pubcountry: Japan&#34;,&#34;~pubyear: 2010&lt;br /&gt;~n:   6&lt;br /&gt;~pubcountry: Japan&#34;,&#34;~pubyear: 2011&lt;br /&gt;~n:   4&lt;br /&gt;~pubcountry: Japan&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(0,190,103,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;Japan&#34;,&#34;legendgroup&#34;:&#34;Japan&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[1989,1993,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,null,null],&#34;y&#34;:[1,1,1,1,10,2,17,21,18,16,21,43,31,29,17,17,14,18,30,29,8],&#34;text&#34;:[&#34;~pubyear: 1989&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Korea, Republic of&#34;,&#34;~pubyear: 1993&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Korea, Republic of&#34;,&#34;~pubyear: 1996&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Korea, Republic of&#34;,&#34;~pubyear: 1997&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Korea, Republic of&#34;,&#34;~pubyear: 1998&lt;br /&gt;~n:  10&lt;br /&gt;~pubcountry: Korea, Republic of&#34;,&#34;~pubyear: 1999&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Korea, Republic of&#34;,&#34;~pubyear: 2000&lt;br /&gt;~n:  17&lt;br /&gt;~pubcountry: Korea, Republic of&#34;,&#34;~pubyear: 2001&lt;br /&gt;~n:  21&lt;br /&gt;~pubcountry: Korea, Republic of&#34;,&#34;~pubyear: 2002&lt;br /&gt;~n:  18&lt;br /&gt;~pubcountry: Korea, Republic of&#34;,&#34;~pubyear: 2003&lt;br /&gt;~n:  16&lt;br /&gt;~pubcountry: Korea, Republic of&#34;,&#34;~pubyear: 2004&lt;br /&gt;~n:  21&lt;br /&gt;~pubcountry: Korea, Republic of&#34;,&#34;~pubyear: 2005&lt;br /&gt;~n:  43&lt;br /&gt;~pubcountry: Korea, Republic of&#34;,&#34;~pubyear: 2006&lt;br /&gt;~n:  31&lt;br /&gt;~pubcountry: Korea, Republic of&#34;,&#34;~pubyear: 2007&lt;br /&gt;~n:  29&lt;br /&gt;~pubcountry: Korea, Republic of&#34;,&#34;~pubyear: 2008&lt;br /&gt;~n:  17&lt;br /&gt;~pubcountry: Korea, Republic of&#34;,&#34;~pubyear: 2009&lt;br /&gt;~n:  17&lt;br /&gt;~pubcountry: Korea, Republic of&#34;,&#34;~pubyear: 2010&lt;br /&gt;~n:  14&lt;br /&gt;~pubcountry: Korea, Republic of&#34;,&#34;~pubyear: 2011&lt;br /&gt;~n:  18&lt;br /&gt;~pubcountry: Korea, Republic of&#34;,&#34;~pubyear: 2012&lt;br /&gt;~n:  30&lt;br /&gt;~pubcountry: Korea, Republic of&#34;,&#34;~pubyear: 2013&lt;br /&gt;~n:  29&lt;br /&gt;~pubcountry: Korea, Republic of&#34;,&#34;~pubyear: 2014&lt;br /&gt;~n:   8&lt;br /&gt;~pubcountry: Korea, Republic of&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(0,193,154,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;Korea, Republic of&#34;,&#34;legendgroup&#34;:&#34;Korea, Republic of&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[1999,2000,2002,2003,2006,2007,2008,2009,2010],&#34;y&#34;:[2,2,2,2,3,2,4,4,2],&#34;text&#34;:[&#34;~pubyear: 1999&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Mexico&#34;,&#34;~pubyear: 2000&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Mexico&#34;,&#34;~pubyear: 2002&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Mexico&#34;,&#34;~pubyear: 2003&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Mexico&#34;,&#34;~pubyear: 2006&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: Mexico&#34;,&#34;~pubyear: 2007&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Mexico&#34;,&#34;~pubyear: 2008&lt;br /&gt;~n:   4&lt;br /&gt;~pubcountry: Mexico&#34;,&#34;~pubyear: 2009&lt;br /&gt;~n:   4&lt;br /&gt;~pubcountry: Mexico&#34;,&#34;~pubyear: 2010&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Mexico&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(0,191,196,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;Mexico&#34;,&#34;legendgroup&#34;:&#34;Mexico&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[1980,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,null,null,null],&#34;y&#34;:[2,1,2,1,1,2,6,9,8,13,31,16,20,22,23,26,34,46,63,85,77,91,73,96,102,103,121,65,79,92,80,97,89,13],&#34;text&#34;:[&#34;~pubyear: 1980&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 1983&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 1984&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 1985&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 1986&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 1987&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 1988&lt;br /&gt;~n:   6&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 1989&lt;br /&gt;~n:   9&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 1990&lt;br /&gt;~n:   8&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 1991&lt;br /&gt;~n:  13&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 1992&lt;br /&gt;~n:  31&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 1993&lt;br /&gt;~n:  16&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 1994&lt;br /&gt;~n:  20&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 1995&lt;br /&gt;~n:  22&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 1996&lt;br /&gt;~n:  23&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 1997&lt;br /&gt;~n:  26&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 1998&lt;br /&gt;~n:  34&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 1999&lt;br /&gt;~n:  46&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 2000&lt;br /&gt;~n:  63&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 2001&lt;br /&gt;~n:  85&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 2002&lt;br /&gt;~n:  77&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 2003&lt;br /&gt;~n:  91&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 2004&lt;br /&gt;~n:  73&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 2005&lt;br /&gt;~n:  96&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 2006&lt;br /&gt;~n: 102&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 2007&lt;br /&gt;~n: 103&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 2008&lt;br /&gt;~n: 121&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 2009&lt;br /&gt;~n:  65&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 2010&lt;br /&gt;~n:  79&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 2011&lt;br /&gt;~n:  92&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 2012&lt;br /&gt;~n:  80&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 2013&lt;br /&gt;~n:  97&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 2014&lt;br /&gt;~n:  89&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;,&#34;~pubyear: 2015&lt;br /&gt;~n:  13&lt;br /&gt;~pubcountry: Patent Co-operation Treaty&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(0,184,231,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;Patent Co-operation Treaty&#34;,&#34;legendgroup&#34;:&#34;Patent Co-operation Treaty&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[1979,2003,2005,2007,2008,2009,2012,null],&#34;y&#34;:[1,3,1,2,1,1,1,1],&#34;text&#34;:[&#34;~pubyear: 1979&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Portugal&#34;,&#34;~pubyear: 2003&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: Portugal&#34;,&#34;~pubyear: 2005&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Portugal&#34;,&#34;~pubyear: 2007&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Portugal&#34;,&#34;~pubyear: 2008&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Portugal&#34;,&#34;~pubyear: 2009&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Portugal&#34;,&#34;~pubyear: 2012&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Portugal&#34;,&#34;~pubyear: 2014&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Portugal&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(0,169,255,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;Portugal&#34;,&#34;legendgroup&#34;:&#34;Portugal&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[1997,2005,2007,2009,2010,2012,null],&#34;y&#34;:[5,1,1,1,1,1,1],&#34;text&#34;:[&#34;~pubyear: 1997&lt;br /&gt;~n:   5&lt;br /&gt;~pubcountry: Russian Federation&#34;,&#34;~pubyear: 2005&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Russian Federation&#34;,&#34;~pubyear: 2007&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Russian Federation&#34;,&#34;~pubyear: 2009&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Russian Federation&#34;,&#34;~pubyear: 2010&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Russian Federation&#34;,&#34;~pubyear: 2012&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Russian Federation&#34;,&#34;~pubyear: 2014&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Russian Federation&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(132,148,255,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;Russian Federation&#34;,&#34;legendgroup&#34;:&#34;Russian Federation&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[1998,2003],&#34;y&#34;:[1,1],&#34;text&#34;:[&#34;~pubyear: 1998&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Singapore&#34;,&#34;~pubyear: 2003&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Singapore&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(199,124,255,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;Singapore&#34;,&#34;legendgroup&#34;:&#34;Singapore&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[1984,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2002,2003,2004,2005,2006,2007,2008],&#34;y&#34;:[1,1,4,2,3,3,3,3,1,9,7,3,4,3,6,3,2,4,3,3,4],&#34;text&#34;:[&#34;~pubyear: 1984&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: South Africa&#34;,&#34;~pubyear: 1988&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: South Africa&#34;,&#34;~pubyear: 1989&lt;br /&gt;~n:   4&lt;br /&gt;~pubcountry: South Africa&#34;,&#34;~pubyear: 1990&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: South Africa&#34;,&#34;~pubyear: 1991&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: South Africa&#34;,&#34;~pubyear: 1992&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: South Africa&#34;,&#34;~pubyear: 1993&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: South Africa&#34;,&#34;~pubyear: 1994&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: South Africa&#34;,&#34;~pubyear: 1995&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: South Africa&#34;,&#34;~pubyear: 1996&lt;br /&gt;~n:   9&lt;br /&gt;~pubcountry: South Africa&#34;,&#34;~pubyear: 1997&lt;br /&gt;~n:   7&lt;br /&gt;~pubcountry: South Africa&#34;,&#34;~pubyear: 1998&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: South Africa&#34;,&#34;~pubyear: 1999&lt;br /&gt;~n:   4&lt;br /&gt;~pubcountry: South Africa&#34;,&#34;~pubyear: 2000&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: South Africa&#34;,&#34;~pubyear: 2002&lt;br /&gt;~n:   6&lt;br /&gt;~pubcountry: South Africa&#34;,&#34;~pubyear: 2003&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: South Africa&#34;,&#34;~pubyear: 2004&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: South Africa&#34;,&#34;~pubyear: 2005&lt;br /&gt;~n:   4&lt;br /&gt;~pubcountry: South Africa&#34;,&#34;~pubyear: 2006&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: South Africa&#34;,&#34;~pubyear: 2007&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: South Africa&#34;,&#34;~pubyear: 2008&lt;br /&gt;~n:   4&lt;br /&gt;~pubcountry: South Africa&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(237,104,237,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;South Africa&#34;,&#34;legendgroup&#34;:&#34;South Africa&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[2002,2007],&#34;y&#34;:[2,1],&#34;text&#34;:[&#34;~pubyear: 2002&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: Spain&#34;,&#34;~pubyear: 2007&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: Spain&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(255,97,204,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;Spain&#34;,&#34;legendgroup&#34;:&#34;Spain&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,null,null,null,null,null,null,null,null,null,null,null,null,null,null],&#34;y&#34;:[2,3,10,9,6,10,26,22,26,15,27,32,18,32,18,32,29,29,33,53,86,58,73,91,58,79,81,67,64,95,118,156,206,256,242,241,250,248,295,244,286,253,288,1,1,1,1,1,1,2,1,1,7,1,284,292,41],&#34;text&#34;:[&#34;~pubyear: 1970&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1971&lt;br /&gt;~n:   3&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1972&lt;br /&gt;~n:  10&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1973&lt;br /&gt;~n:   9&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1974&lt;br /&gt;~n:   6&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1975&lt;br /&gt;~n:  10&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1976&lt;br /&gt;~n:  26&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1977&lt;br /&gt;~n:  22&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1978&lt;br /&gt;~n:  26&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1979&lt;br /&gt;~n:  15&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1980&lt;br /&gt;~n:  27&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1981&lt;br /&gt;~n:  32&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1982&lt;br /&gt;~n:  18&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1983&lt;br /&gt;~n:  32&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1984&lt;br /&gt;~n:  18&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1985&lt;br /&gt;~n:  32&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1986&lt;br /&gt;~n:  29&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1987&lt;br /&gt;~n:  29&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1988&lt;br /&gt;~n:  33&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1989&lt;br /&gt;~n:  53&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1990&lt;br /&gt;~n:  86&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1991&lt;br /&gt;~n:  58&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1992&lt;br /&gt;~n:  73&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1993&lt;br /&gt;~n:  91&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1994&lt;br /&gt;~n:  58&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1995&lt;br /&gt;~n:  79&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1996&lt;br /&gt;~n:  81&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1997&lt;br /&gt;~n:  67&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1998&lt;br /&gt;~n:  64&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1999&lt;br /&gt;~n:  95&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 2000&lt;br /&gt;~n: 118&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 2001&lt;br /&gt;~n: 156&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 2002&lt;br /&gt;~n: 206&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 2003&lt;br /&gt;~n: 256&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 2004&lt;br /&gt;~n: 242&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 2005&lt;br /&gt;~n: 241&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 2006&lt;br /&gt;~n: 250&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 2007&lt;br /&gt;~n: 248&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 2008&lt;br /&gt;~n: 295&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 2009&lt;br /&gt;~n: 244&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 2010&lt;br /&gt;~n: 286&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 2011&lt;br /&gt;~n: 253&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 2012&lt;br /&gt;~n: 288&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1940&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1954&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1956&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1957&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1959&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1962&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1964&lt;br /&gt;~n:   2&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1966&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1967&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1968&lt;br /&gt;~n:   7&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 1969&lt;br /&gt;~n:   1&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 2013&lt;br /&gt;~n: 284&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 2014&lt;br /&gt;~n: 292&lt;br /&gt;~pubcountry: United States of America&#34;,&#34;~pubyear: 2015&lt;br /&gt;~n:  41&lt;br /&gt;~pubcountry: United States of America&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(255,104,161,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;United States of America&#34;,&#34;legendgroup&#34;:&#34;United States of America&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null}],&#34;layout&#34;:{&#34;margin&#34;:{&#34;t&#34;:26.2283105022831,&#34;r&#34;:7.30593607305936,&#34;b&#34;:40.1826484018265,&#34;l&#34;:43.1050228310502},&#34;plot_bgcolor&#34;:&#34;rgba(235,235,235,1)&#34;,&#34;paper_bgcolor&#34;:&#34;rgba(255,255,255,1)&#34;,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:14.6118721461187},&#34;xaxis&#34;:{&#34;domain&#34;:[0,1],&#34;type&#34;:&#34;linear&#34;,&#34;autorange&#34;:false,&#34;range&#34;:[1967.9,2014.1],&#34;tickmode&#34;:&#34;array&#34;,&#34;ticktext&#34;:[&#34;1970&#34;,&#34;1980&#34;,&#34;1990&#34;,&#34;2000&#34;,&#34;2010&#34;],&#34;tickvals&#34;:[1970,1980,1990,2000,2010],&#34;categoryorder&#34;:&#34;array&#34;,&#34;categoryarray&#34;:[&#34;1970&#34;,&#34;1980&#34;,&#34;1990&#34;,&#34;2000&#34;,&#34;2010&#34;],&#34;nticks&#34;:null,&#34;ticks&#34;:&#34;outside&#34;,&#34;tickcolor&#34;:&#34;rgba(51,51,51,1)&#34;,&#34;ticklen&#34;:3.65296803652968,&#34;tickwidth&#34;:0.66417600664176,&#34;showticklabels&#34;:true,&#34;tickfont&#34;:{&#34;color&#34;:&#34;rgba(77,77,77,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:11.689497716895},&#34;tickangle&#34;:-0,&#34;showline&#34;:false,&#34;linecolor&#34;:null,&#34;linewidth&#34;:0,&#34;showgrid&#34;:true,&#34;gridcolor&#34;:null,&#34;gridwidth&#34;:0,&#34;zeroline&#34;:false,&#34;anchor&#34;:&#34;y&#34;,&#34;title&#34;:&#34;pubyear&#34;,&#34;titlefont&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:14.6118721461187},&#34;hoverformat&#34;:&#34;.2f&#34;},&#34;yaxis&#34;:{&#34;domain&#34;:[0,1],&#34;type&#34;:&#34;linear&#34;,&#34;autorange&#34;:false,&#34;range&#34;:[-13.7,309.7],&#34;tickmode&#34;:&#34;array&#34;,&#34;ticktext&#34;:[&#34;0&#34;,&#34;100&#34;,&#34;200&#34;,&#34;300&#34;],&#34;tickvals&#34;:[-1.77635683940025e-15,100,200,300],&#34;categoryorder&#34;:&#34;array&#34;,&#34;categoryarray&#34;:[&#34;0&#34;,&#34;100&#34;,&#34;200&#34;,&#34;300&#34;],&#34;nticks&#34;:null,&#34;ticks&#34;:&#34;outside&#34;,&#34;tickcolor&#34;:&#34;rgba(51,51,51,1)&#34;,&#34;ticklen&#34;:3.65296803652968,&#34;tickwidth&#34;:0.66417600664176,&#34;showticklabels&#34;:true,&#34;tickfont&#34;:{&#34;color&#34;:&#34;rgba(77,77,77,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:11.689497716895},&#34;tickangle&#34;:-0,&#34;showline&#34;:false,&#34;linecolor&#34;:null,&#34;linewidth&#34;:0,&#34;showgrid&#34;:true,&#34;gridcolor&#34;:null,&#34;gridwidth&#34;:0,&#34;zeroline&#34;:false,&#34;anchor&#34;:&#34;x&#34;,&#34;title&#34;:&#34;n&#34;,&#34;titlefont&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:14.6118721461187},&#34;hoverformat&#34;:&#34;.2f&#34;},&#34;shapes&#34;:[{&#34;type&#34;:&#34;rect&#34;,&#34;fillcolor&#34;:null,&#34;line&#34;:{&#34;color&#34;:null,&#34;width&#34;:0,&#34;linetype&#34;:[]},&#34;yref&#34;:&#34;paper&#34;,&#34;xref&#34;:&#34;paper&#34;,&#34;x0&#34;:0,&#34;x1&#34;:1,&#34;y0&#34;:0,&#34;y1&#34;:1}],&#34;showlegend&#34;:true,&#34;legend&#34;:{&#34;bgcolor&#34;:&#34;rgba(255,255,255,1)&#34;,&#34;bordercolor&#34;:&#34;transparent&#34;,&#34;borderwidth&#34;:1.88976377952756,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:11.689497716895},&#34;y&#34;:0.913385826771654},&#34;annotations&#34;:[{&#34;text&#34;:&#34;pubcountry&#34;,&#34;x&#34;:1.02,&#34;y&#34;:1,&#34;showarrow&#34;:false,&#34;ax&#34;:0,&#34;ay&#34;:0,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:14.6118721461187},&#34;xref&#34;:&#34;paper&#34;,&#34;yref&#34;:&#34;paper&#34;,&#34;textangle&#34;:-0,&#34;xanchor&#34;:&#34;left&#34;,&#34;yanchor&#34;:&#34;bottom&#34;,&#34;legendTitle&#34;:true}],&#34;hovermode&#34;:&#34;closest&#34;,&#34;barmode&#34;:&#34;relative&#34;},&#34;config&#34;:{&#34;doubleClick&#34;:&#34;reset&#34;,&#34;modeBarButtonsToAdd&#34;:[{&#34;name&#34;:&#34;Collaborate&#34;,&#34;icon&#34;:{&#34;width&#34;:1000,&#34;ascent&#34;:500,&#34;descent&#34;:-50,&#34;path&#34;:&#34;M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z&#34;},&#34;click&#34;:&#34;function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == &#39;?viewer_pane=1&#39;) {\n          alert(&#39;To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html&#39;);\n        } else {\n          window.open(&#39;https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html&#39;, &#39;_blank&#39;);\n        }\n      }&#34;}],&#34;cloud&#34;:false},&#34;source&#34;:&#34;A&#34;,&#34;attrs&#34;:{&#34;147315b8953fa&#34;:{&#34;x&#34;:{},&#34;y&#34;:{},&#34;colour&#34;:{},&#34;type&#34;:&#34;scatter&#34;}},&#34;cur_data&#34;:&#34;147315b8953fa&#34;,&#34;visdat&#34;:{&#34;147315b8953fa&#34;:[&#34;function (y) &#34;,&#34;x&#34;]},&#34;highlight&#34;:{&#34;on&#34;:&#34;plotly_click&#34;,&#34;persistent&#34;:false,&#34;dynamic&#34;:false,&#34;selectize&#34;:false,&#34;opacityDim&#34;:0.2,&#34;selected&#34;:{&#34;opacity&#34;:1}},&#34;base_url&#34;:&#34;https://plot.ly&#34;},&#34;evals&#34;:[&#34;config.modeBarButtonsToAdd.0.click&#34;],&#34;jsHooks&#34;:{&#34;render&#34;:[{&#34;code&#34;:&#34;function(el, x) { var ctConfig = crosstalk.var(&#39;plotlyCrosstalkOpts&#39;).set({\&#34;on\&#34;:\&#34;plotly_click\&#34;,\&#34;persistent\&#34;:false,\&#34;dynamic\&#34;:false,\&#34;selectize\&#34;:false,\&#34;opacityDim\&#34;:0.2,\&#34;selected\&#34;:{\&#34;opacity\&#34;:1}}); }&#34;,&#34;data&#34;:null}]}}&lt;/script&gt;
&lt;p&gt;We can now hover over the graph to display the data points and also we can use the scroll to see the range of countries. Clearly in this case activity in the US is squashing the other countries down to the bottom so we would in reality want to split this up into separate graphs. For the moment however lets upload the graph to our account.&lt;/p&gt;
&lt;p&gt;For this we just need to use &lt;code&gt;api_create&lt;/code&gt;. It will be a good idea to check the &lt;code&gt;api_create&lt;/code&gt; help and try out the examples to get a better feel for this. Note that you can also upload data frames with the same function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;api_create(pizza)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will trigger your browser window.&lt;/p&gt;
&lt;p&gt;To visit the plot just created try this link &lt;a href=&#34;https://plot.ly/~poldham/613/&#34; class=&#34;uri&#34;&gt;https://plot.ly/~poldham/613/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You will now see an online plot that should look like this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig14_new_R_online.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we hover over the data points we will also see the data appear by country (to add a legend, edit graph and then Legends). Note here that the data table is also provided under the Data tab.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/plotlyfigs/fig15_new_R_interactivepng.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also share the graph via social media, download the data, or edit the graph. Note that the default setting for a graph sent via the API appears is public. To change that use the Edit button and then select the Share. Note that you will need a paid account to share data privately.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round Up&lt;/h2&gt;
&lt;p&gt;In this article we have provided a brief introduction to Plotly to help you get started with using this tool for patent analytics. Plotly provides visually appealing and interactive graphics that can readily be shared with colleagues, pasted into websites and shared publicly. The availability of APIs is also a key feature of Plotly for those working in Python, R or other programmatic environments.&lt;/p&gt;
&lt;p&gt;However, Plotly can also be confusing. For example, we found it hard to understand why particular datasets would not upload correctly (when they can easily be read in Tableau). We also found it hard to understand the format that the data needed to be in to plot correctly. If we examine the data table above it is clear that Plotly has converted each country in the underlying data (which is in long format) into individual x and y axes. We experienced significant problems with making datasets that work fine in Tableau work in Plotly. So, Plotly can be somewhat frustrating although it has very considerable potential for sharing appealing graphics. As is so often the case, this will also involve significant investments in time to understand the way Plotly works and in particular the format for the data that works best with Plotly.&lt;/p&gt;
&lt;p&gt;In this article we have only touched on the potential of Plotly. Other kinds of plots that are well worth exploring include Bubble maps, contour maps and heat maps. To experiment for yourself try the Plotly &lt;a href=&#34;http://help.plot.ly/tutorials/&#34;&gt;tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Accessing Patent Data with the Lens</title>
      <link>/lens/</link>
      <pubDate>Sun, 26 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>/lens/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this article we provide a brief introduction to &lt;a href=&#34;https://www.lens.org/lens/&#34;&gt;The Lens&lt;/a&gt; patent database as a free source of data for patent analytics.&lt;/p&gt;
&lt;p&gt;The Lens is a patent database based in Australia that describes itself as “an open global cyberinfrastructure to make the innovation system more efficient and fair, more transparent and inclusive.” The main way it seeks to do this is by providing access to patent information with a particular focus on sequence information as well as analysis of issues such as DNA related patent activity. An important feature of The Lens for those working on biotechnology related subjects is &lt;a href=&#34;https://www.lens.org/lens/bio&#34;&gt;PatSeq&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This article is now a chapter in the &lt;a href=&#34;https://wipo-analytics.github.io/&#34;&gt;WIPO Manual on Open Source Patent Analytics&lt;/a&gt;. You can read the chapter in electronic book format &lt;a href=&#34;https://wipo-analytics.github.io/the-lens-1.html&#34;&gt;here&lt;/a&gt; and find all the materials including presentations at the &lt;a href=&#34;https://github.com/wipo-analytics&#34;&gt;WIPO Analytics Github homepage&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-started&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;To get the most out of the Lens the first step is to sign up for an account from the front page.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/lens/fig1_front.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is possible to begin searching directly from the front page. However, selecting the small button next to the search box takes you to the search controls.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/lens/fig2_controls.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see we can use boolean queries, for searching a range of fields including the full text, title, abstract or claims (a major plus). We can also select one or multiple jurisdictions. In addition the results can be refined to patent applications or grants, and there are options for full text or one doc per family (which greatly reduces the number of results).&lt;/p&gt;
&lt;p&gt;We used our standard query “pizza”, all jurisdictions, and one document per family. We turned stemming off.&lt;/p&gt;
&lt;p&gt;Our search for pizza returned 13,714 families from a total of 29,617 publications containing the term in the full text. This approach assists with refining searches by reducing duplication.&lt;/p&gt;
&lt;p&gt;The Lens allows users to create collections of up to 10,000 results from a search. To create a collection use the &lt;code&gt;Create Collection&lt;/code&gt; button and name the collection. How you add records to a collection is not obvious and involves 2 steps.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Check the arrow next to Document as in the image below. When the mouse hovers over the arrow a menu will pop up. Choose &lt;code&gt;Top 10k Results&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;In the box displaying the name of the collection above the results press the + arrow to add the 10,000 documents to the Collection.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/lens/fig3_addtocollection.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Once you understand this process it is easy to add documents to collections. One very nice feature of the Lens is that when a collection has been created we can share it with others using the &lt;code&gt;Share&lt;/code&gt; button. Users have the option of maintaining a private collection or publicly sharing. The URL for the collection we just generated is &lt;a href=&#34;https://www.lens.org/lens/collection/9606&#34; class=&#34;uri&#34;&gt;https://www.lens.org/lens/collection/9606&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We could imagine that for more restricted searches, and taking confidentiality issues into account, this could be a useful way of sharing patent data with colleagues. One useful addition would be the ability to share with groups based on email addresses or something similar (although that may be possible by choosing a private link and sharing it).&lt;/p&gt;
&lt;p&gt;Using the small icons above &lt;code&gt;Document&lt;/code&gt; on the left we can save our query for later use, limit the data to simple families or expand to publications, and download the data.&lt;/p&gt;
&lt;p&gt;There are two main options for downloading data. The first is to download 1000 records by selecting the export button above &lt;code&gt;Document&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When we select the export button we will be presented with a choice on the number of records to export and whether to export in JSON (for programmatic use), RIS for bibliographic software or .csv for use in tools such as Excel or other programmes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/lens/fig4_exportoptions.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The outputs of the export are clean and clear about what they represent when compared with some patent databases. A &lt;code&gt;url&lt;/code&gt; link to the relevant file on the Lens is also provided which can assist in reviewing documents.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/lens/fig5_export.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The JSON output (in the lower right of the image above) is also nice and clean.&lt;/p&gt;
&lt;p&gt;The second route to exporting data is to download up to 10,000 results using the collections. When we select the &lt;code&gt;Work Area&lt;/code&gt; icon at the top of the screen and select &lt;code&gt;Collections&lt;/code&gt; we will see a new screen with a range of icons next to an individual collection.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/lens/fig5a_export_collection.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When we select the download icon we can now download the 10,000 records in the collection in either .csv, ris or JSON formats. This is very easy to use once you understand how to navigate the interface.&lt;/p&gt;
&lt;p&gt;We also have an option to upload documents into a collection using the upload button and then enter comma separated identifiers. However, at the time of writing we were not able to make this very useful function work.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;additional-features&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Additional features&lt;/h2&gt;
&lt;p&gt;In addition to these features, it is also important to note that data exports include a cited count that counts the number of patent/non-patent records &lt;code&gt;cited&lt;/code&gt; by the applicant.&lt;/p&gt;
&lt;p&gt;The online data also shows the citing documents. For example &lt;a href=&#34;https://www.lens.org/lens/patent/US_3982033_A/citations#c/out&#34;&gt;US 3982033 A Process for Coating Pizza Shells With Sauce&lt;/a&gt; cites three patent documents but has &lt;a href=&#34;https://www.lens.org/lens/patent/US_3982033_A/citations#c/in&#34;&gt;11 forward citations by later applicants&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;While the citing documents are not included with the downloaded data it is possible to visit a record of interest online and then create a new set with the citing documents. Where a number of documents of interest have been identified this could be the basis for creating a new collection of cited or citing literature on a topic of interest linked to a core query.&lt;/p&gt;
&lt;p&gt;As such, one possible workflow using the Lens would involve initial exploratory queries and refinement, downloading the results of a refined query for closer inspection and then selecting documents of interest to explore the backward (cited) and forward (citing) citations and generate a new dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualisation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualisation&lt;/h2&gt;
&lt;p&gt;The Lens makes good use of online visualisation options using &lt;a href=&#34;http://www.highcharts.com&#34;&gt;Highcharts&lt;/a&gt; and HTML5. To access the visualisations choose the small icon on the right above the &lt;code&gt;Sort by&lt;/code&gt; pull down menu.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/lens/fig6_visual.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We now see a set of charts for our results. Using the up icon in the top right of each image we can get an expanded view and work with the charts. The Lens uses the Highcharts Javascript library and a very nice feature of this approach is that it the visuals are interactive and can be used to refine search results. In the image below we have opened the applicants image. As an aside, note that each image can be copied as an iframe to embed in your own web page.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/lens/fig7_applicants.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This suggests that Google is the top user of the word pizza in the patent system with &lt;a href=&#34;https://www.lens.org/lens/collection/9608&#34;&gt;880 documents in 353 families&lt;/a&gt;. We can then select the top result and the charts will regenerate focusing on our selection (in this case Google). To view the results we need to select the results button (the first on the right above the charts) to see the following.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/lens/fig8_google.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What is very useful about is that it is easy to create a &lt;a href=&#34;https://www.lens.org/lens/collection/9608&#34;&gt;new collection&lt;/a&gt; for an applicant of interest, to download the results or select areas of a portfolio based on a jurisdiction or technology area or to explore highly cited patents. In short, we can easily dig into the data.&lt;/p&gt;
&lt;p&gt;Other interesting features of the chart area are references to authors, DOIs, and PubMed Ids for exploration of data extracted from the documents. This reflects the interest at the Lens in researching the relationship between basic scientific research and innovation. Accessing the literature related information requires opening a chart (for example authors) and selecting the top result and the moving into the results view. We then select one of the results such as &lt;a href=&#34;https://www.lens.org/lens/patent/US_8200847_B2/citations#c/publications&#34;&gt;Voice Actions On Computing Devices&lt;/a&gt; and the Citations tab. This reveals a publication from a workshop on Wireless Geographical Information Systems from 2003 as we can see below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/lens/fig9_crossref.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An impressive feature of this approach is the effort that has been made to link the citation data to the publication using &lt;a href=&#34;http://www.crossref.org&#34;&gt;crossref&lt;/a&gt;. According to the documentation around 15 million non-patent literature citations have been linked so far. Note that one additional feature of the Lens download data is that it includes a non-patent literature citation field. For example, downloading the &lt;a href=&#34;https://www.lens.org/lens/collection/9608&#34;&gt;google pizza portfolio&lt;/a&gt; and a search for the citation above will reveal the citation but without the added value of the DOI. As such, the download provided the raw NPL data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;working-with-texts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Working with texts&lt;/h2&gt;
&lt;p&gt;In common with other free databases, the Lens is not designed to allow downloads of multiple full texts. However, you can access the full text of documents, including .pdf files, and you can make notes that will be stored with a collection in your account. The image below provides an example of our ongoing efforts to understand why Google is so dominant in the results of searches for pizza in patent documents.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/lens/fig10_notes.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;patseq&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PatSeq&lt;/h2&gt;
&lt;p&gt;One important focus of the development of the Lens has been DNA sequence data including an &lt;a href=&#34;https://www.lens.org/about/&#34;&gt;ongoing series of articles&lt;/a&gt; on the interpretation and significance of sequence data in patent activity.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/lens/fig11_patseq.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Patseq includes a number of tools.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;PatSeq data permits access to patent documents disclosing sequences available for bulk download from a growing number of countries. This is a very useful site for obtaining sequence data. Note that you will need to request access to download sequence data in your account area.&lt;/li&gt;
&lt;li&gt;Species finder and keyword search focuses on searching documents that contain a sequence for a species name or key term.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/lens/fig12_patseq_species.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A series of patent portfolios have been generated for some major plant and animal species, e.g. rice, maize, humans, chickens etc. That can be downloaded as collections.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The PatSeq Explorer allows the exploration of sequence data for four genomes (at present), notably the human and mouse genome for animals and the soybean, maize and rice genome for plants.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/lens/fig13_patseq_explorer.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is an area where researchers with &lt;a href=&#34;http://www.cambia.org/daisy/cambia/home.html&#34;&gt;Cambia&lt;/a&gt;, the non-profit organisation behind the Lens, have invested considerable effort and it is well worth reading the research articles listed on the Cambia and Lens websites on this topic. PatSeq Analyzer is closely related to the Explorer and presently provides details on the genomes mentioned above with a detailed summary of sequences by document including the region, sequence, transcript, single nucleotide polymorphisms (SNPs) and grants with sequences in the patent claims.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;PatSeq Finder&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The PatSeq Finder allows a user to enter a DNA or amino acid sequence into the search box and find applications and grants with identical or similar sequences. We selected a sequence at random from the WIPO Patentscope sequence listings browser &lt;a href=&#34;http://www.wipo.int/patentscope/search/en/detail.jsf?LANGUAGE=ENG&amp;amp;KEY=16/026850&amp;amp;ELEMENT_SET=F&#34;&gt;W016/026850&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/lens/fig14_seq_explorer.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;After processing we will see a list of results that can be downloaded in a variety of formats. The results indicate that our random sequence does not appear in the claims of a granted patent or a patent application but does appear in a number of applications and grants. Further details are provided by hovering over the individual entries and additional controls are available for similarity and other scores to refine the results.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/lens/fig15_seq_results.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As far as we could tell, while the data can be downloaded, it is not presently possible to generate a collection of documents from the results of the PatSeq Finder.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round Up&lt;/h2&gt;
&lt;p&gt;The Lens is a very useful patent database that, when you have worked out the meaning of icons, is easy to use. The ease with which collections can be shared and up to 10,000 records downloaded is a real plus for the Lens. In addition, the use of HTML5 and Highcharts makes this a highly interactive experience. The ability to use charts to drill down into the data is very welcome. The link to the &lt;code&gt;crossref&lt;/code&gt; service for non-patent literature is very useful but it would be good to see this data included in some way as a field in the data downloads.&lt;/p&gt;
&lt;p&gt;With the addition of data downloads (in 2015) the Lens is becoming a very useful platform for searching, refining, visualizing and downloading patent data. What would perhaps be useful would be a set of demonstration walkthroughs or use cases that explain the way in which the Lens can be used in common work flows. For example, developing and refining a search, testing results, then retrieving backward and forward citations for refinement and visualization are quite common tasks in patent landscape analysis. Use cases would help users make the most of what the Lens has to offer.&lt;/p&gt;
&lt;p&gt;The Lens also stands out for its distinctive long term work on sequence data in patents and this will be of particular interest to researchers working on biotech particularly in exploring the analytical tools.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An Overview of Patent Analytics Tools</title>
      <link>/overview-open-tools/</link>
      <pubDate>Sun, 26 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>/overview-open-tools/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This article provides an overview of the open source and free software tools that are available for patent analytics. The aim of the chapter is to serve as a quick reference guide for some of the main tools in the tool kit.&lt;/p&gt;
&lt;p&gt;This article is now a chapter in the &lt;a href=&#34;https://wipo-analytics.github.io/&#34;&gt;WIPO Manual on Open Source Patent Analytics&lt;/a&gt;. You can read the chapter in electronic book format &lt;a href=&#34;https://wipo-analytics.github.io/an-overview-of-tools.html&#34;&gt;here&lt;/a&gt; and find all the materials including presentations at the &lt;a href=&#34;https://github.com/wipo-analytics&#34;&gt;WIPO Analytics Github homepage&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We will go into some of the tools covered in his aticle in more depth elsewhere in the WIPO Manual on Open Source Patent Analytics and leave you to explore the rest of the tools for yourself.&lt;/p&gt;
&lt;p&gt;Before we start it is important to note that we cover only a fraction of the available tools that are out there. We have simply tried to identify some of the most accessible and useful tools. Data mining and visualization are growing rapidly to the point that it is easy to be overwhelmed by the range of choices. The good news is that there are some very high quality free and open source tools out there. The difficulty lies in identifying those that will best serve your specific needs relative to your background and the time available to acquire some programming skills. That decision will be up to you. However, to avoid frustration it will be important to recognise that the different tools take time to master. In some cases, such as R and Python, there are lots of free resources out there to help you take the first steps into programming. In making a decision about a tool to use, think carefully about the level of support that is already out there. Try to use a tool with an active and preferably large community of users. That way, when you get stuck, there will be someone out there who has run into similar issues who will be able to help. Sites such as &lt;a href=&#34;http://stackoverflow.com&#34;&gt;Stack Overflow&lt;/a&gt; are excellent for finding solutions to problems.&lt;/p&gt;
&lt;p&gt;This chapter is divided into 8 sections:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;General Tools&lt;/li&gt;
&lt;li&gt;Cleaning Tools&lt;/li&gt;
&lt;li&gt;Data Mining&lt;/li&gt;
&lt;li&gt;Data visualization&lt;/li&gt;
&lt;li&gt;Network visualization&lt;/li&gt;
&lt;li&gt;Infographics&lt;/li&gt;
&lt;li&gt;Geographic Mapping&lt;/li&gt;
&lt;li&gt;Text Mining&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In some cases tools are multifunctional and so may appear in one section where they could also appear in another. Rather than repeating information we will let you figure that out.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;general-tools&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;General Tools&lt;/h2&gt;
&lt;p&gt;Quite a number of free tools are available for multi-purpose tasks such as basic cleaning of patent data and visualization. We highlight three free tools here.&lt;/p&gt;
&lt;div id=&#34;open-office&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.openoffice.org&#34;&gt;Open Office&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Many patent analysts will use Excel as a default programme including basic cleaning of smaller datasets. However, it is well worth considering Apache Open Office as a free alternative. While patent analysis will typically use the Spreadsheet (Open Office Calc) there is also a very useful Database option as an alternative to Microsoft Access.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download and Install &lt;a href=&#34;https://www.openoffice.org&#34;&gt;Apache Open Office&lt;/a&gt; for your system.&lt;/li&gt;
&lt;li&gt;Tip: When saving spreadsheet files, choose save as &lt;strong&gt;.csv&lt;/strong&gt; to avoid situations where a programme can’t read the default &lt;strong&gt;.odt&lt;/strong&gt; files.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;google-sheets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.google.co.uk/sheets/about/&#34;&gt;Google Sheets&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Google Sheets require a free Google account and those who are comfortable with Excel may wonder why it is worth switching. However, Google Sheets can be shared online with others and there are a large number of free add ons that could be used to assist with cleaning data such as Split Names or Remove duplicates as shown below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/google_sheets_addon.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;google-fusion-tables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://support.google.com/fusiontables/answer/2571232?hl=en&#34;&gt;Google Fusion Tables&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Fusion Tables are similar to Google Sheets but can work with millions of records. However, it is worth trying with smaller datasets to see if Fusion Tables suit your needs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/Fusion-Tables.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Fusion Tables appear very much like a spreadsheet. However, the Table also contains a &lt;code&gt;cards&lt;/code&gt; feature which allows each record to be seen as a whole and easily filtered. The cards can be much easier to work with than the standard row format where information in a record can be difficult to take in. Fusion Tables also attempts to use geocoded data to draw a Google Map as we can see in the second image below for the publication country from a sample patent dataset.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/Fusion-Tables-Cards.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/Fusion-Tables-Map.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;cleaning-tools&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cleaning Tools&lt;/h2&gt;
&lt;div id=&#34;open-refine-formerly-google-refine&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://openrefine.org&#34;&gt;Open Refine&lt;/a&gt; (formerly Google Refine)&lt;/h3&gt;
&lt;p&gt;A fundamental rule of data analysis and visualization is: &lt;code&gt;rubbish in = rubbish out&lt;/code&gt;. If your data has not been cleaned in the first place, do not be surprised if the results of analysis or visualization are rubbish.&lt;/p&gt;
&lt;p&gt;An in depth chapter is available &lt;a href=&#34;http://poldham.github.io/openrefine-patent-cleaning/&#34;&gt;here&lt;/a&gt; on the use of &lt;a href=&#34;http://openrefine.org&#34;&gt;Open Refine&lt;/a&gt;, formerly Google Refine, for cleaning patent data. For patent analytics Open Refine is an important free tool for cleaning applicant and inventor names.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/OpenRefine-download.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A number of platforms provide data cleaning facilities and it is possible to do quite a lot of basic cleaning in either Open Office or Excel. Open Refine is the most accessible tool for timely cleaning of patent name fields. In particular, it is very useful for splitting and cleaning thousands of patent applicant and inventor names.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-mining&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Mining&lt;/h2&gt;
&lt;p&gt;There are an ever growing number of data mining tools out there. Here are a few of those that have caught our attention with additional tools listed below.&lt;/p&gt;
&lt;div id=&#34;rstudio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.rstudio.com&#34;&gt;RStudio&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A very powerful tool for working with data and visualising data using R and then writing about it (this chapter, and the wider Manual, is entirely written in Rmarkdown with RStudio). While the learning curve with R can be intimidating a great deal of effort goes in to making R accessible through &lt;a href=&#34;http://www.rstudio.com/resources/training/online-learning/&#34;&gt;tutorials&lt;/a&gt; such as those on &lt;a href=&#34;https://www.datacamp.com&#34;&gt;DataCamp&lt;/a&gt;, &lt;a href=&#34;http://www.rstudio.com/resources/webinars/&#34;&gt;webinars&lt;/a&gt;, &lt;a href=&#34;http://www.r-bloggers.com&#34;&gt;R-Bloggers&lt;/a&gt; and &lt;a href=&#34;http://stackoverflow.com/questions/tagged/r&#34;&gt;Stack Overflow&lt;/a&gt; and free university courses such as the well known John Hopkins University R Programming Course on &lt;a href=&#34;https://www.coursera.org/course/rprog&#34;&gt;Coursera&lt;/a&gt;. Indeed, as with Python, there is so much support for users at different levels that it is hard ever to feel alone when using R and RStudio.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/RStudio.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To get started with R download RStudio for your platform by &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/&#34;&gt;following these instructions&lt;/a&gt; and making sure to install R from the link provided.&lt;/p&gt;
&lt;p&gt;If you are completely new to R then &lt;a href=&#34;https://www.datacamp.com/&#34;&gt;DataCamp&lt;/a&gt; is a good place to start. The free John Hopkins University &lt;a href=&#34;https://www.coursera.org/course/rprog&#34;&gt;R Programming Course on Coursera&lt;/a&gt; is also very good. The John Hopkins University course is accompanied by the Swirl tutorial package that can be installed using `install.packages(“swirl”) when you have installed R. This is a real asset when getting started.&lt;/p&gt;
&lt;p&gt;In developing this Manual we mainly focused on developing resources with R. However, we would emphasise that Python may also be important for your needs. For a recent discussion on the strengths and weaknesses of R and Python see this &lt;a href=&#34;http://www.r-bloggers.com/choosing-r-or-python-for-data-analysis-an-infographic/&#34;&gt;Datacamp article on the &lt;code&gt;Data Science Wars&lt;/code&gt; and accompanying excellent infographic&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rapidminer-studio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://rapidminer.com/products/studio/&#34;&gt;RapidMiner Studio&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Comes with a free service and a variety of tiered paid plans. RapidMiner focuses on machine learning, data mining, text mining and analytics.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/RapidMiner.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;knime&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.knime.org&#34;&gt;KNIME&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;An open platform for data mining.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/KNIME.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Other data mining tools (such as &lt;a href=&#34;http://www.cs.waikato.ac.nz/ml/weka/&#34;&gt;WEKA&lt;/a&gt; and &lt;a href=&#34;http://www.nltk.org&#34;&gt;NLTK&lt;/a&gt; in Python are covered below). If you would like to explore other data mining software try this &lt;a href=&#34;http://www.predictiveanalyticstoday.com/top-free-data-mining-software/&#34;&gt;article&lt;/a&gt; for some ideas.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-visualization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data visualization&lt;/h2&gt;
&lt;p&gt;If you are new to data visualization we suggest that you might be interested in the work of Edward Tufte at Yale University and his famous book &lt;a href=&#34;http://en.wikipedia.org/wiki/Edward_Tufte&#34;&gt;The Visual Display of Quantitative Information&lt;/a&gt;. His &lt;a href=&#34;http://users.ha.uth.gr/tgd/pt0501/09/Tufte.pdf&#34;&gt;critique of the uses and abuses of Powerpoint&lt;/a&gt; is also entertaining and insightful. The work of Stephen Few, such as &lt;a href=&#34;http://www.analyticspress.com/show.html&#34;&gt;Show Me the Numbers: Designing Tables and Graps to Enlighten&lt;/a&gt; is also popular.&lt;/p&gt;
&lt;p&gt;Remember that data visualization is first and foremost about communication with an audience. That involves choices about how to communicate and finding ways to communicate clearly. In very many cases the outcome of patent analysis and visualization will be a report and a presentation. Tufte’s critique of &lt;a href=&#34;http://users.ha.uth.gr/tgd/pt0501/09/Tufte.pdf&#34;&gt;powerpoint presentations&lt;/a&gt; should be required reading for presenters. You may also like to take a look at Nancy Duarte’s &lt;a href=&#34;http://resonate.duarte.com/#!page0&#34;&gt;Resonate&lt;/a&gt; for ideas on polishing up presentations and storytelling. The style may not suit everyone but &lt;a href=&#34;http://resonate.duarte.com/#!page0&#34;&gt;Resonate&lt;/a&gt; contains very useful messages and insights. In an offline environment, consider Katy Borner’s &lt;a href=&#34;https://mitpress.mit.edu/index.php?q=books/atlas-science&#34;&gt;Atlas of Science: Visualising What We Know&lt;/a&gt; as an excellent guide to the history of visualizations of scientific activity including pioneering visualizations of patent activity. Bear in mind that effective visualization takes practice and is a quite well trodden path.&lt;/p&gt;
&lt;p&gt;There are a lot of choices out there for data visualization tools and the number of tools is growing rapidly. For business analytics Gartner provides a useful (but subscription based) &lt;a href=&#34;http://www.informationweek.com/big-data/big-data-analytics/gartner-bi-magic-quadrant-2015-spots-market-turmoil/d/d-id/1319214&#34;&gt;Magic Quadrant for Business Intelligence and Analytics&lt;/a&gt; report that seeks to map out the leaders in the field. These types of reports can be useful for spotting up and coming companies and checking if there is a free version of the software (other than a short free trial).&lt;/p&gt;
&lt;p&gt;We would suggest thinking carefully about your needs and the learning curve involved. For example, if you have limited programming knowledge (or no time or desire to learn) choose a tool that will largely do the job for you. If you already have experience with javascript, Java, R or Python, or similar, then choose a tool that you feel most comfortable with. In particular, keep an eye out for tools with an API (application programming interface) in a variety of language flavours (such as Python or R) that are likely to meet your needs.&lt;/p&gt;
&lt;p&gt;If you are completely new to data visualization &lt;a href=&#34;https://public.tableau.com/s/gallery&#34;&gt;Tableau Public&lt;/a&gt; and &lt;a href=&#34;http://poldham.github.io/tableau-patents/&#34;&gt;our walk through chapter&lt;/a&gt; are a good place to learn without knowing anything about programming. Some other tools in this list are similar to Tableau Public (in part because Tableau is the market leader). We will also provide some pointers to visualization overview sites at the end of this section where you can find out about what is new and interesting in data visualization.&lt;/p&gt;
&lt;div id=&#34;google-charts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://developers.google.com/chart/interactive/docs/gallery&#34;&gt;Google Charts&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Create a Google Account to Access Google Spreadsheets and other Google programmes&lt;/li&gt;
&lt;li&gt;Take a look at the &lt;a href=&#34;https://developers.google.com/chart/interactive/docs/gallery&#34;&gt;Google Charts Gallery&lt;/a&gt; and &lt;a href=&#34;https://developers.google.com/chart/interactive/docs/reference&#34;&gt;API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;For an overview of using Google Charts in R then see the &lt;code&gt;GoogleVis&lt;/code&gt; package and its examples &lt;a href=&#34;http://cran.r-project.org/web/packages/googleVis/vignettes/googleVis.pdf&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;For an overview using Google Charts with Python see the &lt;a href=&#34;https://code.google.com/p/google-chartwrapper/&#34;&gt;google-chartwrapper&lt;/a&gt; or &lt;a href=&#34;http://python-google-charts.readthedocs.org/en/latest/#&#34;&gt;Python Google Charts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/google_sheets_addon.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tableau-public&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://public.tableau.com/s/&#34;&gt;Tableau Public&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;An in depth chapter on getting started with patent analysis and visualization using Tableau Public is available &lt;a href=&#34;http://poldham.github.io/tableau-patents/&#34;&gt;here&lt;/a&gt;. When your patent data has been cleaned, Tableau Public is a powerful way of developing interactive dashboards and maps with your data and combining it with other data sources. Bear in mind that Tableau Public data is, by definition, public and it should not be used with sensitive data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/dashboard_completed.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The workbook can be viewed online &lt;a href=&#34;https://public.tableau.com/profile/wipo.open.source.patent.analytics.manual#!/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-and-rstudio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R and RStudio&lt;/h3&gt;
&lt;p&gt;R is a statistical programming language for working with all kinds of different types of data. It also has powerful visualization tools including &lt;code&gt;packages&lt;/code&gt; that provide an interface with Google Charts, &lt;a href=&#34;https://plot.ly&#34;&gt;Plotly&lt;/a&gt; and others. If you are interested in using R then we suggest using RStudio which can be downloaded &lt;a href=&#34;http://www.rstudio.com/&#34;&gt;here&lt;/a&gt;. The entire WIPO Open Source Patent Analytics Manual was written in RStudio using Rmarkdown to output the articles for the web, .pdf and presentations. As this suggests, it is not simply about data visualization. To get started with R and RStudio try the free tutorials at &lt;a href=&#34;https://www.datacamp.com/&#34;&gt;DataCamp&lt;/a&gt;. We will cover R in more detail in other chapters and online articles.&lt;/p&gt;
&lt;p&gt;As part of an approach described as &lt;code&gt;The Grammar of Graphics&lt;/code&gt;, inspired by &lt;a href=&#34;https://en.wikipedia.org/wiki/Leland_Wilkinson&#34;&gt;Leland Wilkinson’s work&lt;/a&gt;, developers at RStudio and others have created packages that provide very useful ways to visualise and map data. The links below will take you to the documentation for some of the most popular data visualization packages.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;http://cran.r-project.org/web/packages/ggplot2/index.html&#34;&gt;ggplot2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cran.r-project.org/web/packages/ggvis/index.html&#34;&gt;ggvis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cran.r-project.org/web/packages/ggmap/index.html&#34;&gt;ggmap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cran.r-project.org/web/packages/googleVis/index.html&#34;&gt;googleVis&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will cover &lt;code&gt;ggplot2&lt;/code&gt; and &lt;code&gt;ggvis&lt;/code&gt; in greater depth in future chapters. Until then, to get started see the chapters on &lt;code&gt;ggplot2&lt;/code&gt; on &lt;a href=&#34;http://www.r-bloggers.com/search/ggplot2&#34;&gt;R-Bloggers&lt;/a&gt; and here for &lt;a href=&#34;http://www.r-bloggers.com/?s=ggvis&#34;&gt;ggvis&lt;/a&gt;. Datacamp offers a free tutorial on the use of &lt;code&gt;ggvis&lt;/code&gt; that can be accessed &lt;a href=&#34;http://www.r-bloggers.com/ggvis-tutorial-become-a-data-visualization-expert-with-rstudio/&#34;&gt;here&lt;/a&gt;. For a wider overview of some of the top R packages see Qin Wenfeng’s recent &lt;a href=&#34;https://github.com/qinwf/awesome-R&#34;&gt;awesome R list&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;shiny&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Shiny&lt;/h3&gt;
&lt;p&gt;Shiny from &lt;a href=&#34;(http://www.rstudio.com/)&#34;&gt;RStudio&lt;/a&gt; is a web application framework for R. What that means is that you can output tables and visual data from R such as those from the tools mentioned above to the web.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/Shiny.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://shiny.rstudio.com&#34;&gt;Shiny apps&lt;/a&gt; for R users allows for the creation of online interactive apps (upto 5 for free). See the &lt;a href=&#34;http://shiny.rstudio.com/gallery/&#34;&gt;Gallery&lt;/a&gt; for examples. See &lt;a href=&#34;http://www.r-bloggers.com/search/shiny&#34;&gt;RBloggers&lt;/a&gt; for more examples and tutorials.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://vnijs.github.io/radiant/index.html&#34;&gt;Radiant&lt;/a&gt; is a browser based platform for business analytics in R. It is based on Shiny (above) but is specifically business focused.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/Radiant.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For a series of starter videos on Radiant see &lt;a href=&#34;http://vnijs.github.io/radiant/tutorials.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ibm-many-eyes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www-01.ibm.com/software/analytics/many-eyes/&#34;&gt;IBM Many Eyes&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;You need to Register for a free account to really understand what this is about, &lt;a href=&#34;http://www-969.ibm.com/software/analytics/manyeyes/&#34;&gt;try this page&lt;/a&gt; and select register in the top right.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/manyeyes.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-visualization-tools&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other visualization Tools&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://tulip.labri.fr/TulipDrupal/&#34;&gt;Tulip&lt;/a&gt;: data visualization framework in C++&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sigmajs.org/&#34;&gt;SigmaJS&lt;/a&gt;: JavaScript library dedicated to graph drawing. It allows the creation of interactive static and dynamic graphs&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.telerik.com/download/kendo-ui-web-open-source&#34;&gt;Kendo UI&lt;/a&gt;: Create widgets for responsive visualizations.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://timeline.knightlab.com/&#34;&gt;Timeline&lt;/a&gt;: A KnightLab (northwestern university) is a tool allowing for the creation of interactive timelines and is available in 40 languages.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://misoproject.com/&#34;&gt;Miso Project&lt;/a&gt;: open source toolkit facilitating the creation of interactive storytelling and data visualization&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sci2.cns.iu.edu/user/index.php&#34;&gt;Sci2&lt;/a&gt;: A toolset for studying science.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.simile-widgets.org&#34;&gt;Simile Widgets&lt;/a&gt; Web widgets for storytelling as a spin off from the SIMILE Project at MIT.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.jqplot.com&#34;&gt;jqPlot&lt;/a&gt;. An open source jQuery based Plotting Plugin.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.dipity.com&#34;&gt;dipity&lt;/a&gt; for Timelines (free and premium services)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For additional visualization tools and ideas see &lt;a href=&#34;http://www.visualizing.org/&#34;&gt;visualizing.org&lt;/a&gt; and &lt;a href=&#34;http://opendata-tools.org/en/visualization/&#34;&gt;Open Data Tools&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;network-visualization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Network Visualization&lt;/h2&gt;
&lt;p&gt;Network visualization software is an important tool for visualising actors in a field of science and technology and, in particular, the relationships between them. For patent analysis it can be used for a range of purposes including:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Visualising networks of applicants and inventors in a particular field or scientific researchers. An example of this type of work for synthetic biology is &lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0034368&#34;&gt;here&lt;/a&gt; for a network of approximately 2,000 authors of articles on synthetic biology.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/synbio.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Visualising areas of technology and their relationships using the International Patent Classification and the Cooperative Patent Classification (CPC). Previous work at WIPO pioneered the use of large scale patent network analysis to identify the &lt;a href=&#34;http://www.wipo.int/patentscope/en/programs/patent_landscapes/reports/animal_gr.html&#34;&gt;patent landscape for animal genetic resources&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The image below displays a network map of Cooperative Patent Classification Codes and International Patent Classification codes for 10s of thousands of patent documents that contain references to a range of farm animals (cows, pigs, sheep etc.). The dots are CPC/IPC codes describing areas of technology. The clusters show tightly linked documents that share the same codes that can then be described as ‘modules’ or clusters. The authors of the landscape report on animal genetic resources used this network as an exploratory tool to extract and examine the documents in the cluster for relevance. Distant clusters, such as Cooking equipment and Animal Husbandry (housing of animals etc.), were discarded. The authors later used network mapping to explore and classify the individual clusters.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/animals.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Visualising networks of key terms in patent documents and their relationships with other terms as part of the exploration and refinement of analysis. In this case the authors have clustered similar terms onto each other using word stemming to understand the contents of the new breeds of animals cluster above in relation to animal breeding.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/breeds_cluster.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As such, network visualization can be seen as both an exploratory tool for defining the object of interest and as the end result (e.g. a defined network of actors in a specific area).&lt;/p&gt;
&lt;div id=&#34;gephi&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://gephi.github.io&#34;&gt;Gephi&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Gephi is Java based open source network generating software. It can cope with large datasets (depending on your computer) to produce powerful visualizations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/Gephi.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One issue that may be encountered, particularly by Mac users, is problems with installing the right Java version. This problem appears to have been solved with the latest release Version 0.9.&lt;/p&gt;
&lt;p&gt;To create &lt;code&gt;.gexf&lt;/code&gt; network files in R try the &lt;a href=&#34;http://cran.r-project.org/web/packages/rgexf/index.html&#34;&gt;gexf&lt;/a&gt; package and example code and source code &lt;a href=&#34;https://bitbucket.org/gvegayon/rgexf/wiki/Home&#34;&gt;here&lt;/a&gt;. In Python try the &lt;a href=&#34;https://github.com/paulgirard/pygexf&#34;&gt;pygexf&lt;/a&gt; library and for anything else such as Java, Javascript C++ and Perl see &lt;a href=&#34;http://gexf.net/format/&#34;&gt;gexf.net&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;nodexl&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://nodexl.codeplex.com&#34;&gt;NodeXL&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;For die hard Excel users, &lt;a href=&#34;http://nodexl.codeplex.com&#34;&gt;NodeXL&lt;/a&gt; is a plug in that can be used to visualise networks. It works well.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/NodeXL.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cytoscape&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.cytoscape.org/what_is_cytoscape.html&#34;&gt;Cytoscape&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://www.cytoscape.org/what_is_cytoscape.html&#34;&gt;Cytoscape&lt;/a&gt; is another network visualization programme. It was originally designed for the visualization of biological networks and interactions but, as with so many other bioinformatics tools, can be applied to a wider range of visualization tasks.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/cytoscape.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We mainly have experience with using Gephi (above) but Cytoscape is well worth exploring. Cytoscape works with Windows, Mac and Linux.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pajek&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://mrvar.fdv.uni-lj.si/pajek/&#34;&gt;Pajek&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This is one of the oldest and most established of the free network tools and is Windows only (or run via a Virtual Machine). It is widely used in bibliometrics and can handle large datasets. It is a matter of personal preference but tools such as Gephi may be superseding Pajek because they are more flexible. However, Pajek may possibly have an edge in precision, ease of reproducibility and the important ability to easily save work that Gephi can lack as a Beta programme.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/Pajek.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Data can also be exported from Pajek to Gephi for those who prefer the look and feel of Gephi.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;vos-viewer&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.vosviewer.com/Home&#34;&gt;VOS Viewer&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;VOS Viewer from Leiden University is similar to Gephi and Cytoscape but also presents different types of landscape (as opposed to pure network node and edge visuals). The latest version can also speak to both Gephi and Cytoscape. It is worth testing for different visual display options and its ability to handle Web of Science and Scopus bibliographic data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/VOSviewer.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hive-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.hiveplot.net&#34;&gt;Hive Plots&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We are not entirely sure what to make of Hive Plots. However, we have a lot of sympathy with the aims. The aim of network visualization should be to clarify the complex… not “wow, look, I made something that looks like spaghetti” (although that is normally part of the process). So, we find Hive Plots developed by Martin Krzywinski at the Genome Sciences Center at the BC Cancer Agency interesting.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/HivePlots.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Designed for large networks there are packages for Hive plots in Python through &lt;a href=&#34;https://pypi.python.org/pypi/pyveplot/&#34;&gt;pyveplot&lt;/a&gt; and &lt;a href=&#34;https://github.com/ericmjl/hiveplot&#34;&gt;hiveplot&lt;/a&gt;. For R there is &lt;a href=&#34;http://academic.depauw.edu/~hanson/HiveR/HiveR.html&#34;&gt;HiveR&lt;/a&gt; with documentation available on CRAN &lt;a href=&#34;http://cran.r-project.org/web/packages/HiveR/index.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In closing this discussion of network mapping tools it is also important to note that network visualizations need to be exported as images. This means that there are additional requirements for image handling software. Open source tools such as &lt;a href=&#34;http://www.gimp.org&#34;&gt;The GNU Image Manipulation Program or GIMP&lt;/a&gt; are perfectly adequate and easy to use for image handling. Where using labels particular attention should be paid to outlining the text to ensure consistency of display across different computers. These kinds of tasks can be performed in tools such as GIMP.&lt;/p&gt;
&lt;p&gt;For other sources of network visualization see &lt;a href=&#34;http://opendata-tools.org/en/visualization/&#34;&gt;FlowingData&lt;/a&gt;. Also try &lt;a href=&#34;http://www.visualcomplexity.com/vc/&#34;&gt;Visual Complexity&lt;/a&gt; and &lt;a href=&#34;http://www.visualisingdata.com/index.php/resources/&#34;&gt;visualising data&lt;/a&gt; for sources of inspiration.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;infographics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Infographics&lt;/h2&gt;
&lt;p&gt;Infographics are increasingly part of the communication toolkit. They are particularly useful for communicating the results of research in an easily digestible yet informative form. The WIPO Patent Landscape Project has developed a range of infographics with the latest being for the &lt;a href=&#34;http://www.wipo.int/export/sites/www/patentscope/en/programs/patent_landscapes/reports/documents/animal_genetics_infographic.pdf&#34;&gt;Animal Genetic Resoutces Patent Landscape Report&lt;/a&gt; and &lt;a href=&#34;http://www.wipo.int/patentscope/en/programs/patent_landscapes/reports/assistive_devices.html&#34;&gt;Assistive Devices and Technologies&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The growing popularity of infographics has witnessed the rise of a range of online services including free services. In more cases these will have limitations such as the number of icons etc. that can be used in a graphic. However, as a growing sector that may change. Here are a few services with free options that may be worth exploring.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;http://piktochart.com&#34;&gt;Piktochart.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.canva.com/create/infographics/&#34;&gt;Canva.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://infogr.am/pricing&#34;&gt;Infogr.am&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.visme.co&#34;&gt;Visme&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.easel.ly/create/#&#34;&gt;Easel.ly&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Websites such as &lt;a href=&#34;http://www.coolinfographics.com&#34;&gt;Cool Infographics&lt;/a&gt; can be useful for finding additional sources, exploring what is hot in the infographics world and tutorials. Tools such as Apple Keynote, Open Office Presentation or Powerpoint can be very useful for wire framing (sketching out) infographics to see what works.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;geographical-mapping&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Geographical Mapping&lt;/h2&gt;
&lt;p&gt;In addition to the ubiquitous &lt;a href=&#34;https://www.google.com/maps/&#34;&gt;Google Maps&lt;/a&gt; or well known &lt;a href=&#34;https://www.google.co.uk/intl/en_uk/earth/&#34;&gt;Google Earth&lt;/a&gt; we think it is well worth taking a close look at other services.&lt;/p&gt;
&lt;div id=&#34;openstreetmap&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.openstreetmap.org/#map=5/51.500/-0.100&#34;&gt;OpenStreetMap&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Rightly popular.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/OpenStreetMap.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;leaflet&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://leafletjs.com&#34;&gt;Leaflet&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A very popular Open Source JavaScript library for interactive maps&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/Leaflet.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Accessible through an API. R users could use the &lt;code&gt;leafletr&lt;/code&gt; package with tutorials and walk throughs available at &lt;a href=&#34;http://www.r-bloggers.com/?s=leaflet&#34;&gt;R-bloggers&lt;/a&gt;. For Python users try &lt;code&gt;folium&lt;/code&gt; &lt;a href=&#34;https://github.com/python-visualization/folium&#34;&gt;here&lt;/a&gt; or &lt;a href=&#34;https://pypi.python.org/pypi/folium&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tableau-public-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://public.tableau.com/s/&#34;&gt;Tableau Public&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Already mentioned above. Tableau Public uses Open Street Map to create a powerful combination of interactive graphs that can be linked to maps geocoded at various levels of detail. See an example &lt;a href=&#34;https://public.tableau.com/profile/poldham#!/vizhome/SyntheticBiologyScientificLandscape/SyntheticBiologyTrends&#34;&gt;here&lt;/a&gt; for the scientific literature on synthetic biology.&lt;/p&gt;
&lt;p&gt;Tableau Public is probably the easiest way to get started with creating your own maps with patent data. The map below was produced using custom geocoding and connecting the data to publication country and the titles of scientific publications.
&lt;img src=&#34;/images/overview/tableaumap.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To view the interactive version try this &lt;a href=&#34;http://public.tableau.com/profile/poldham#!/vizhome/SyntheticBiologyScientificLandscape/SyntheticBiologyTrends&#34;&gt;page&lt;/a&gt;. It is possible to easily create simple yet effective maps in Tableau Public.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;qgis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.qgis.org/en/site/&#34;&gt;QGIS&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A very popular and sophisticated software package running on all major platforms.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/QGIS.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Using QGIS Oldham and Hall et al mapped the worldwide geographical locations of marine scientific research and patent documents making reference to deep sea locations such as hydrothermal vents (see &lt;a href=&#34;https://www.researchgate.net/publication/273139809_Valuing_the_Deep_Marine_Genetic_Resources_in_Areas_Beyond_National_Jurisdiction&#34;&gt;Valuing the Deep&lt;/a&gt;). This is a low resolution QGIS map of scientific research locations in the oceans based on text mining of the scientific literature.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/valuing1.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;geonames.org.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.geonames.org&#34;&gt;Geonames.org&lt;/a&gt;.&lt;/h3&gt;
&lt;p&gt;Not a mapping programme, instead geonames is an incredibly useful database of georeferenced place names from around the world along with a RESTful &lt;a href=&#34;http://www.geonames.org/export/web-services.html&#34;&gt;web service&lt;/a&gt;. If you need to obtain the georeferenced data for a large number of places then this should be your first stop. geonames can be accessed in R using the &lt;a href=&#34;https://cran.r-project.org/web/packages/geonames/geonames.pdf&#34;&gt;&lt;code&gt;geonames&lt;/code&gt;&lt;/a&gt; along with client libraries for Python, Ruby, PHP and others.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/GeoNames.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;icharts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://icharts.net/product/web-data-visualization&#34;&gt;iCharts&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A Free and Premium data visualization service:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/icharts.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;openlayers3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://openlayers.org&#34;&gt;OpenLayers3&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;OpenLayers3 allows you to add your own layers to OpenStreetMap and other data sources and may come in very useful if you are seeking to create your own layers. It also has an API and tutorials.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/OpenLayer.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cartodb&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://cartodb.com/gallery/&#34;&gt;CartoDB&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Free and paid accounts with a nice looking gallery of examples&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/CartoDB.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;d3.js&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://d3js.org&#34;&gt;d3.js&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A javascript library for manipulating data and documents. This is the library behind some of the other frequently mentioned visualization tools on the web.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/D3.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;highcharts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.highcharts.com&#34;&gt;Highcharts&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Free for non-commercial use with a variety of pricing plans.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/highcharts.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;datawrapper&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://datawrapper.de&#34;&gt;Datawrapper&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;An entirely open source service for creating charts and maps with your data. Widely used by big newspapers and so the graphics will seem familiar. Either create an account or fork the source from Github &lt;a href=&#34;https://github.com/datawrapper/datawrapper&#34;&gt;here&lt;/a&gt;. There is a free option and a set of pricing plans.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/Datawrapper.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plotly&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://plot.ly/feed/&#34;&gt;Plotly&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Free and with an API with clients for R, Python and Matlab, Plotly is an increasingly popular free service that uses the D3.js library mentioned above with the enterprise version used by companies such as Google. Plotly is increasingly popular and has a range of API clients for Python, Matlab, R, Node.js, and Excel. Plotly’s ease of use and access from a range of environments are big reasons for its growing success.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/Plotly.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;text-mining&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Text Mining&lt;/h2&gt;
&lt;p&gt;There are a lot of text mining tools out there and many of them are free or open source. Here are some that we have come across.&lt;/p&gt;
&lt;div id=&#34;jigsaw-visual-analytics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.cc.gatech.edu/gvu/ii/jigsaw/&#34;&gt;Jigsaw Visual Analytics&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;For exploring and understanding document collections.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/Jigsaw.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weka&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.cs.waikato.ac.nz/ml/weka/&#34;&gt;Weka&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Java based text mining software.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/Weka.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;word-trees&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Word Trees&lt;/h3&gt;
&lt;p&gt;Word trees can be used for detailed investigation of texts such as claims trees. The first two examples are taken from the &lt;a href=&#34;http://www.wipo.int/edocs/pubdocs/en/wipo_pub_946.pdf&#34;&gt;WIPO Guidelines for Preparing Patent Landscape Reports&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;google-word-trees-on-the-google-developers-site-provides-instructions-for-generating-word-trees-using-javascript.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://developers.google.com/chart/interactive/docs/gallery/wordtree&#34;&gt;Google Word Trees&lt;/a&gt; on the Google Developers site provides instructions for generating word trees using Javascript.&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/Word-Trees.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;and &lt;a href=&#34;https://www.jasondavies.com/wordtree/&#34;&gt;Jason Davies&lt;/a&gt; tree creator.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/Word-Tree-Davies.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kh-coder-free-software-allowing-quantitative-content-analysistext-mining.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://sourceforge.net/projects/khc/?source=directory&#34;&gt;KH Coder&lt;/a&gt;: free software allowing quantitative content analysis/text mining.&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/KHCoder.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-and-the-tm-package&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R and the &lt;code&gt;tm&lt;/code&gt; package&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;tm&lt;/code&gt; package in R (e.g. using RStudio) provides access to a range of text mining tools. For an introduction from the package developers see &lt;a href=&#34;http://cran.r-project.org/web/packages/tm/vignettes/tm.pdf&#34;&gt;here&lt;/a&gt;. A number of very useful tutorials are also available for text mining on &lt;a href=&#34;http://www.r-bloggers.com/?s=text+mining&#34;&gt;R-bloggers&lt;/a&gt;. For a step by step approach see &lt;a href=&#34;http://onepager.togaware.com/TextMiningO.pdf&#34;&gt;Graham Williams (2014) Hands-On Data Science with R Text Mining&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For a recent overview of text mining tools in R see &lt;a href=&#34;http://cran.r-project.org/web/views/NaturalLanguageProcessing.html&#34;&gt;Fridolin Wild’s (2014) CRAN Task View: Natural Language Processing&lt;/a&gt; listing the various packages and their uses.&lt;/p&gt;
&lt;p&gt;Note that many text mining packages in general focus on generating words. For non-academic purposes this is not very useful. Patent analysis will typically focus on extracting and analysing &lt;code&gt;phrases&lt;/code&gt; (ngrams). Therefore look for tools that will extract phrases and allow them to be interrogated in depth.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;python-and-text-mining&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Python and Text Mining&lt;/h3&gt;
&lt;p&gt;There are quite a few resources available on text mining using Python. Note that Python may well be ahead of R in terms of text mining resources (until we are proven wrong). However, note that Python and R are increasingly used together to exploit their different strengths. Here are a few resources to help you get started.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-natural-language-toolkitntlk&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.nltk.org&#34;&gt;The Natural Language Toolkit(NTLK)&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;NTLK appears to be the leading package and covers almost all major needs. The accompanying book &lt;a href=&#34;http://shop.oreilly.com/product/9780596516499.do&#34;&gt;Natural Language Processing with Python&lt;/a&gt; may also be worth considering. The &lt;a href=&#34;http://www.christianpeccei.com/textmining/&#34;&gt;Python Textmining Package&lt;/a&gt; is simpler than the giant NTLK package but may suit your needs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/overview/ntlk.png&#34; width=&#34;600px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This &lt;a href=&#34;http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk&#34;&gt;detailed tutorial&lt;/a&gt; may be helpful for those wanting to get started with the NTLK package in Python.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-text-mining-resources&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other text mining resources&lt;/h3&gt;
&lt;p&gt;For a wider range of text mining options see this predictive analytics article on the &lt;a href=&#34;http://www.predictiveanalyticstoday.com/top-free-software-for-text-analysis-text-mining-text-analytics/&#34;&gt;top 20 free text mining software tools&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For other free text mining tools try some of the corpus linguistics websites such as &lt;a href=&#34;http://linguistlist.org:8888/sp/SearchWRListing-action.cfm?subclassid=7223&amp;amp;SearchType=LF&amp;amp;WRTypeID=2&#34;&gt;The Linguist List&lt;/a&gt;, this &lt;a href=&#34;http://www.uow.edu.au/~dlee/software.htm&#34;&gt;list&lt;/a&gt;, or this &lt;a href=&#34;http://ucrel.lancs.ac.uk/tools.html&#34;&gt;list&lt;/a&gt;. Bear in mind that most of these tools are designed with linguists in mind and quite a number may be showing their age. However, even simple concordancing tools, such as &lt;a href=&#34;http://www.laurenceanthony.net/software/antconc/&#34;&gt;AntConc&lt;/a&gt; can play an important role in filtering large numbers of documents to extract useful information.&lt;/p&gt;
&lt;p&gt;Some analysis tools such as &lt;a href=&#34;https://www.thevantagepoint.com&#34;&gt;VantagePoint from Search Technology Inc.&lt;/a&gt; have been especially developed and adapted for processing patent data and are available in a subsidised version for students from the &lt;a href=&#34;http://vpinstitute.org/wordpress/vp-marketplace/&#34;&gt;vpinstitute&lt;/a&gt;. There are also a number of qualitative data analysis software tools that can be applied to patent analysis such as &lt;a href=&#34;http://www.maxqda.com&#34;&gt;MAXQDA&lt;/a&gt;, &lt;a href=&#34;http://www.qsrinternational.com/products_nvivo.aspx&#34;&gt;NVivo&lt;/a&gt;, &lt;a href=&#34;http://atlasti.com&#34;&gt;Atlas TI&lt;/a&gt; and &lt;a href=&#34;http://provalisresearch.com/products/qualitative-data-analysis-software/&#34;&gt;QDA Miner&lt;/a&gt;. However, with the exception of &lt;a href=&#34;http://provalisresearch.com/products/qualitative-data-analysis-software/freeware/&#34;&gt;QDA Miner Lite&lt;/a&gt; (Windows only), while they offer free trials they do not fall into the category of free or open source software that is our focus.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round Up&lt;/h2&gt;
&lt;p&gt;In this chapter we have covered some of the major free and open source tools that are available for patent analysis. These are not patent specific tools but can be readily adapted to patent analysis. With the exception of cleaning of patent applicant and inventor names and concatenated fields, patent data is very well suited for visualization and network mapping. The availability of country level data, address fields and place names in texts also means that patent data can be readily used for geographical mapping.&lt;/p&gt;
&lt;p&gt;In practice, it is important to identify a set of tools that work best for you and the type of patent analysis tasks that work best for you.&lt;/p&gt;
&lt;p&gt;It is also important to emphasise that in practice you may use a mixture of paid for tools and free tools. For example, the recent &lt;a href=&#34;http://www.wipo.int/patentscope/en/programs/patent_landscapes/reports/animal_gr.html&#34;&gt;WIPO Patent Landscape for Animal Genetic Resources&lt;/a&gt; involved the use of GNU Parallel and Map Reduce for large scale text mining of 11 million whole text patents using pattern matching in Ruby, combined with the use of PATSTAT for statistics, Thomson Innovation and VantagePoint for validation, and Tableau and Gephi for visualization. In short, it is possible to perform almost all patent analysis tasks, using free tools, but in practice a mixed ecosystem of open source and commercial tools may produce the best workflow for the tasks you perform. As such, it is important to think about the tools that are needed and where they support and strengthen existing analysis workflows.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-checklist&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Checklist&lt;/h2&gt;
&lt;p&gt;If moving into open source software for the first time it may be useful to develop a list of basic questions to assess whether they a tool or a set of tools will meet your particular needs. The following list is not meant to be definitive or exhaustive but aims to encourage thinking about your particular requirements.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Does this tool make sense? That is, is it immediately clear what the purpose of a tool is? If the answer is yes, this is a good sign. If the answer is no, the tool may be too specialised for your particular needs or the creators may be struggling with clearly expressing what the tool is trying to do (a bad sign).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Do you understand the language the tool is written in? Is it a problem if you don’t (see below)? Is it worth training someone in this language? Are there free or affordable courses available?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is the source code open or proprietary and what are the terms and conditions of the open source licence? When using open source or free software it is important to be clear what the precise provisions of the licence mean. For example, are you required to make any modifications to the source code available to others on exactly the same terms as the original licence? If you are working with source code this is an important IP question. If you are not working at the source code level this may not be an issue, but it always always makes sense to understand the open source licence.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Who owns the data? If you upload data to a web based service, who owns the data once it is uploaded and who else may have access to it and under what conditions? These questions are particularly pertinent where the data is commercially relevant.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What does free actually mean? Free versions are often a lead in to premium services (hence the term &lt;code&gt;freemium&lt;/code&gt;). This transition is a key feature of open source business models. In some cases free may be highly restricted in terms of the amount of data that can be processed, saved or exported. In other cases, no restrictions on use of the tool are imposed. However, knowledge about the use of the tool may be the real premium or cost factor, particularly if you come to depend on that tool. Be prepared for this.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What other companies (or patent offices) are using this tool? This can be an indicator of confidence and also provides examples of concrete use cases.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is the tool well supported with documentation and tutorials? This is an indicator of maturity and ‘buy in’ by a community of developers and users.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How large is the community of users and are they active in creating forums and blogs etc. to support the wider community of users?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is this a one function tool or a multi-purpose tool? That is, will this tool cover almost all needs or is it a specific &lt;code&gt;good to have&lt;/code&gt; component in a toolkit. In some cases a tool that does one thing very well is a real asset where other tools fall down because they try to do too many things and do them badly. Of the tools listed above, R and Python (possibly in combination) come closest to tools that could be used for a complete patent analysis workflow from data acquisition right through to visualization. In practice, most patent analysis tool kits will consist of both general and specific tools.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Can I break this tool? It is always a very good idea to work out where the limitations of software lie so that you are not taken by surprise later when trying to do something that is mission critical. In particular, software may claim to perform particular tasks, such as handling thousands or millions of records, but do them very badly, if at all. By pushing a tool past its limits it is possible to determine where the limits are and how to get the best out of it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is the tool proportionate to my needs? There has recently been a great deal of excitement about &lt;code&gt;Big Data&lt;/code&gt; and the use of &lt;a href=&#34;https://hadoop.apache.org&#34;&gt;Hadoop&lt;/a&gt; for dealing with large volumes of data using distributed computing. While Hadoop is open source, so anyone can use it, its adoption would generally be disproportionate for the needs of most patent analysis except where dealing with almost the entire corpus of global patent documents, large volumes of literature and considerable quantities of scientific data. By way of illustration, as noted above, the WIPO Animal Genetic Resources Landscape report used GNU Parallel to process 11 million patent records. The decision to use GNU Parallel was partly made on the basis that Hadoop would have been complicated to implement and overkill for the particular use case. In short, it is worth carefully considering whether a tool is both appropriate and proportionate to the task at hand.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, the golden rule for adopting any tool for patent analysis can be expressed in very simple terms. Does this work for me?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you have any suggestions for free or open source tools that we should include in the manual please feel welcome to add a comment to the electronic version of this chapter.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;credits&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Credits&lt;/h2&gt;
&lt;p&gt;The development of the list of open source tools benefited from the following articles.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;http://www.creativebloq.com/design-tools/data-visualization-712402&#34;&gt;Creative Bloq 11/11/2014 The 37 best tools for data visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://thenextweb.com/dd/2015/04/21/the-14-best-data-visualization-tools/&#34;&gt;Nismith Sharma 2015 The 14 best data visualization tools. TNW News&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Graphing Patent Data with ggplot2 part2</title>
      <link>/graphing-patent-data-with-ggplot2-part2/</link>
      <pubDate>Mon, 20 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>/graphing-patent-data-with-ggplot2-part2/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-grammar-of-graphics&#34;&gt;The Grammar of Graphics&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#creating-a-pie-chart-aaaargh&#34;&gt;Creating a Pie Chart (aaaargh)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is Part 2 of an article introducing R for patent analytics that focuses on visualising patent data in R using the ggplot2 package.&lt;/p&gt;
&lt;p&gt;In Part 1 we introduced the basics of wrangling patent data in R using the &lt;code&gt;dplyr&lt;/code&gt; package to select and add data. In this article we will go into more detail on these functions. We then focused on using &lt;code&gt;qplot&lt;/code&gt; from the &lt;code&gt;ggplot2&lt;/code&gt; package to illustrate the ease with which graphics can be created and edited in R. In this article we will focus on &lt;code&gt;ggplot&lt;/code&gt; and the Grammar of Graphics. As in Part 1 we assume that you are new to R and make no assumptions about familiarity with R. However, you must have &lt;a href=&#34;https://www.rstudio.com/products/rstudio/#Desktop&#34;&gt;RStudio&lt;/a&gt; installed on your computer (see Part 1) for instructions. We will also move a little faster on some of the initial steps than in Part 1.&lt;/p&gt;
&lt;p&gt;The majority of examples in this article are based on the list of recipes for generating graphics using ggplot2 in &lt;code&gt;Winston Chang&#39;s R Graphics Cookbook&lt;/code&gt; and the accompanying &lt;a href=&#34;http://www.cookbook-r.com/Graphs/&#34;&gt;website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This article is a work in progress and will be updated as solutions are identified to some of the issues encountered in graphing using ggplot2. Please feel welcome to add comments to this post, particularly where you identify a solution to issues.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-grammar-of-graphics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Grammar of Graphics&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;ggplot2&lt;/code&gt; is an implementation of Leland Wilkinson’s Grammar of Graphics by Hadley Wickham at RStudio as described in &lt;a href=&#34;http://vita.had.co.nz/papers/layered-grammar.pdf&#34;&gt;this article&lt;/a&gt; and &lt;a href=&#34;http://www.amazon.co.uk/ggplot2-Elegant-Graphics-Data-Analysis/dp/0387981403&#34;&gt;ggplot2 book&lt;/a&gt;. Hadley Wickham’s grammar differs from the original by focusing on layered approach to building statistical graphics.&lt;/p&gt;
&lt;p&gt;The grammar of graphics is an approach to building graphics based on the idea that any statistical graphic can be built from the following components:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A data set&lt;/li&gt;
&lt;li&gt;A set of aesthetic or &lt;code&gt;aes&lt;/code&gt; attributes such as size, shape &amp;amp; colour&lt;/li&gt;
&lt;li&gt;Statistical transformations&lt;/li&gt;
&lt;li&gt;A geometric object &lt;code&gt;geom&lt;/code&gt; or set of objects (&lt;code&gt;geoms&lt;/code&gt;) for the type of plot e.g. line, bar or map&lt;/li&gt;
&lt;li&gt;scales for the above&lt;/li&gt;
&lt;li&gt;A coordinate system (e.g. a grid or map)&lt;/li&gt;
&lt;li&gt;Faceting (trellising)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In practice, this breaks down into three main elements:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;A base object. The first two of these components combine into a base object consisting of the data set and aesthetic mappings or &lt;code&gt;aes&lt;/code&gt; for the particular data we want to see. That includes the axes and any fill or line colours.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;geoms&lt;/code&gt; or geometric objects. We then add one or more &lt;code&gt;geom&lt;/code&gt; to specify the form in which we want to see the data we have selected in 1. This tends to also involve a statistical transformation (such as placing data into bins for a bar chart). Defaults deal with some of this. However, a statistical transformation or &lt;code&gt;stat&lt;/code&gt; can also be specified.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A coordinate system. We normally don’t need to think about this. The default is a standard Cartesian grid. However, this can be changed to a fixed grid or a polar grid.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;An easy way to think about it is that&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The base object defines what we want to see.&lt;/li&gt;
&lt;li&gt;The geoms define the form we want to see it in.&lt;/li&gt;
&lt;li&gt;The coordinate system defines the framework for the visualisation.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As with any grammar it can take a while to get used to its terms and peculiarities. The good news is that there are plenty of free resources out there for this very popular package.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Winston Chang’s &lt;a href=&#34;http://www.cookbook-r.com/Graphs/&#34;&gt;R Graphics Cookbook website&lt;/a&gt; is a very valuable practical guide to most things you will want to do with ggplot2. The full R Graphics Cookbook goes into a lot more detail and is an invaluable reference if you will be doing a lot of work with graphics in R. Those with budgets may also want to invest in Hadley Wickham’s book ggplot2 published by Springer.&lt;/li&gt;
&lt;li&gt;RStudio have developed a very helpful cheat sheet that you can download &lt;a href=&#34;http://www.rstudio.com/wp-content/uploads/2015/05/ggplot2-cheatsheet.pdf&#34;&gt;here&lt;/a&gt; or view &lt;a href=&#34;http://www.rstudio.com/resources/cheatsheets/&#34;&gt;here&lt;/a&gt;. We suggest downloading and printing the cheat sheet when using ggplot2.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;##Getting Started&lt;/p&gt;
&lt;p&gt;If you don’t have these packages already then install each of them below by pressing command and Enter at the end of each line. As an alternative select &lt;strong&gt;&lt;em&gt;Packages &amp;gt; Install&lt;/em&gt;&lt;/strong&gt; in the pane displaying a tab called Packages. Then enter the names of the packages one at a time without the quotation marks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;readr&amp;quot;)
install.packages(&amp;quot;dplyr&amp;quot;)
install.packages(&amp;quot;ggplot2&amp;quot;)
install.packages(&amp;quot;ggthemes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then make sure the packages have loaded to make them available. Press command and enter at the end of each line below (or, if you are feeling brave, select them all and then click the icon marked Run).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
library(dplyr)
library(ggplot2)
library(ggthemes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You are now good to go.&lt;/p&gt;
&lt;p&gt;###Loading the Data&lt;/p&gt;
&lt;p&gt;We will load the pizza dataset directly from the &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/tree/master/2_datasets/pizza_medium_clean&#34;&gt;Github datasets repository&lt;/a&gt; using &lt;code&gt;read_csv&lt;/code&gt; from the &lt;code&gt;readr&lt;/code&gt; package. If downloading from the repository note that it is the View Raw file that you want. If loading from a downloaded file include the full file path inside quotation marks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pizza &amp;lt;- read_csv(&amp;quot;https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium_clean/pizza.csv?raw=true&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As in Part 1 we will use &lt;code&gt;dplyr&lt;/code&gt; to create a count field using the publication number, rename some of the fields and then select fields we want to count.&lt;/p&gt;
&lt;p&gt;###Reviewing and Preparing the Dataset with &lt;code&gt;dplyr&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Inspecting our data is a first step. Type &lt;code&gt;View(pizza)&lt;/code&gt; in the console to see the table and &lt;code&gt;str(pizza)&lt;/code&gt; to see its structure.&lt;/p&gt;
&lt;p&gt;In practice we have a number of issues that we will want to fix.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We have a blank row at the bottom of the dataset that we will want to remove because it will produce a red error message. It does not have to be removed but this will help in interpreting any error messages.&lt;/li&gt;
&lt;li&gt;Patent data commonly doesn’t contain numeric fields. Data fields are mainly characters or dates with the exception at times of cited and citing counts. We will want to add a count column.&lt;/li&gt;
&lt;li&gt;We will not be working with all 31 columns in &lt;code&gt;pizza&lt;/code&gt; and so we will want to select just those we will be working with.&lt;/li&gt;
&lt;li&gt;To save typing we may want to rename some of the columns (and we can revert the names later if need be).&lt;/li&gt;
&lt;li&gt;Some data may be missing for particular countries. For example, for Canada some of the entries are missing a year field. That may be fine for raw totals but not for charting by year.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To handle these tasks we can use the very useful functions in &lt;code&gt;dplyr&lt;/code&gt;. For other tasks we might also want to use &lt;code&gt;tidyr&lt;/code&gt; or &lt;code&gt;plyr&lt;/code&gt; as sister packages to &lt;code&gt;dplyr&lt;/code&gt;. &lt;code&gt;dplyr&#39;s&lt;/code&gt; main functions are described &lt;a href=&#34;http://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html&#34;&gt;here&lt;/a&gt;. These are the functions we think you will find most useful with patent data.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;filter() and slice()&lt;/li&gt;
&lt;li&gt;arrange()&lt;/li&gt;
&lt;li&gt;select()&lt;/li&gt;
&lt;li&gt;distinct()&lt;/li&gt;
&lt;li&gt;mutate()&lt;/li&gt;
&lt;li&gt;summarise()&lt;/li&gt;
&lt;li&gt;group_by()&lt;/li&gt;
&lt;li&gt;count()&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;dplyr&lt;/code&gt; functions are important because they will help you to easily extract elements of the data for graphing. They can also be very useful for basic patent analysis workflows where tools such as Excel or Open Office will struggle. For more on wrangling data using &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;tidyr&lt;/code&gt; see the &lt;a href=&#34;http://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf&#34;&gt;RStudio Data Wrangling Cheatsheet&lt;/a&gt;. &lt;code&gt;dplyr&lt;/code&gt; also includes pipes, such as %&amp;gt;% (meaning then) that can be used to string together chunks of code in an efficient and easy to use way. We will illustrate the use of pipes in this article but will not use pipes throughout as we are adopting a simple step by step approach. As you become more familiar and comfortable with R we suggest that you increasingly start to work with pipes to make your life easier. We caution against leaping into pipes when learning R. While they are very easy to use and efficient, they are still relatively new. That can make reading ‘normal’ R code difficult until you are more familiar with it.&lt;/p&gt;
&lt;p&gt;We will now use two &lt;code&gt;dplyr&lt;/code&gt; functions.&lt;/p&gt;
&lt;p&gt;First we will add a column with a numeric value for each publication number in the dataset using &lt;code&gt;mutate&lt;/code&gt;. &lt;code&gt;mutate&lt;/code&gt; takes an argument applied to the values of one or more columns and adds a new column based on that argument. Here, as in Part 1 we simply add a new column called &lt;code&gt;n&lt;/code&gt; that uses &lt;code&gt;sum()&lt;/code&gt; to award each publication number the value of 1. We now have a numeric count column &lt;code&gt;n&lt;/code&gt; from our character vectors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pizza &amp;lt;- mutate(pizza, n = sum(publication_number = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we will rename the columns to make life easier using &lt;code&gt;rename()&lt;/code&gt;. The code has been indented to make it easier to read. To run this code, select the code and press Run in R or press command and enter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pizza &amp;lt;- rename(pizza, 
                pubcountry = publication_country_name,
                pubcode = publication_country_code, 
                pubyear = publication_year
                )
pizza&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9,996 x 32
##    applicants_cleaned    applicants_clean… applicants_orga… applicants_original
##    &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;              
##  1 &amp;lt;NA&amp;gt;                  People            &amp;lt;NA&amp;gt;             &amp;lt;NA&amp;gt;               
##  2 Ventimeglia Jamie Jo… People            &amp;lt;NA&amp;gt;             Ventimeglia Jamie …
##  3 Cordova Robert; Mart… People            &amp;lt;NA&amp;gt;             Cordova Robert;Mar…
##  4 Lazarillo De Tormes … Corporate         Lazarillo De To… LAZARILLO DE TORME…
##  5 &amp;lt;NA&amp;gt;                  People            &amp;lt;NA&amp;gt;             &amp;lt;NA&amp;gt;               
##  6 Depoortere, Thomas    People            &amp;lt;NA&amp;gt;             DEPOORTERE, Thomas 
##  7 Frisco Findus Ag      Corporate         Frisco Findus Ag FRISCO-FINDUS AG   
##  8 Bicycle Tools Incorp… Corporate         Bicycle Tools I… Bicycle Tools Inco…
##  9 Castiglioni, Carlo    People            &amp;lt;NA&amp;gt;             CASTIGLIONI, CARLO 
## 10 &amp;lt;NA&amp;gt;                  People            &amp;lt;NA&amp;gt;             &amp;lt;NA&amp;gt;               
## # ... with 9,986 more rows, and 28 more variables: inventors_cleaned &amp;lt;chr&amp;gt;,
## #   inventors_original &amp;lt;chr&amp;gt;, ipc_class &amp;lt;chr&amp;gt;, ipc_codes &amp;lt;chr&amp;gt;,
## #   ipc_names &amp;lt;chr&amp;gt;, ipc_original &amp;lt;chr&amp;gt;, ipc_subclass_codes &amp;lt;chr&amp;gt;,
## #   ipc_subclass_detail &amp;lt;chr&amp;gt;, ipc_subclass_names &amp;lt;chr&amp;gt;,
## #   priority_country_code &amp;lt;chr&amp;gt;, priority_country_code_names &amp;lt;chr&amp;gt;,
## #   priority_data_original &amp;lt;chr&amp;gt;, priority_date &amp;lt;chr&amp;gt;, pubcode &amp;lt;chr&amp;gt;,
## #   pubcountry &amp;lt;chr&amp;gt;, publication_date &amp;lt;chr&amp;gt;, publication_date_original &amp;lt;chr&amp;gt;,
## #   publication_day &amp;lt;int&amp;gt;, publication_month &amp;lt;int&amp;gt;, publication_number &amp;lt;chr&amp;gt;,
## #   publication_number_espacenet_links &amp;lt;chr&amp;gt;, pubyear &amp;lt;int&amp;gt;,
## #   title_cleaned &amp;lt;chr&amp;gt;, title_nlp_cleaned &amp;lt;chr&amp;gt;,
## #   title_nlp_multiword_phrases &amp;lt;chr&amp;gt;, title_nlp_raw &amp;lt;chr&amp;gt;,
## #   title_original &amp;lt;chr&amp;gt;, n &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now want to create 4 data tables:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;p1 as a reference set with four columns&lt;/li&gt;
&lt;li&gt;pc = publication country totals&lt;/li&gt;
&lt;li&gt;pt = publication totals by year&lt;/li&gt;
&lt;li&gt;pcy = publication country by year&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will create a new working table called &lt;code&gt;p1&lt;/code&gt; that contains just the data we want to work with using &lt;code&gt;dplyr&#39;s&lt;/code&gt; &lt;code&gt;select()&lt;/code&gt;. &lt;code&gt;select()&lt;/code&gt; will only include columns that we name and will drop the others from the new table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- select(pizza, pubcountry, pubcode, pubyear, n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;p1 will be our reference set and contains 4 columns, use &lt;code&gt;View(p1)&lt;/code&gt; to see it.&lt;/p&gt;
&lt;p&gt;If we were to inspect this data we would see that we have some sparse results dating back to the 1940s. In the last article we controlled for this in graphs using &lt;code&gt;xlim&lt;/code&gt; to limit the x axis to specific years. Here we will remove that data.&lt;/p&gt;
&lt;p&gt;To remove the sparse years we need to use &lt;code&gt;dplyrs&lt;/code&gt; filter function. &lt;code&gt;filter&lt;/code&gt; is basically the equivalent of &lt;code&gt;select&lt;/code&gt; for rows. Rather than naming each of the years that we want to remove we will us an operator for values equal or greater than 1970 &lt;code&gt;&amp;gt;=&lt;/code&gt;. We will also want to pull back from the data cliff in more recent years as discussed in Part 1. To do that we will add a second argument to filter for years that are equal to or below 2012 &lt;code&gt;&amp;lt;=&lt;/code&gt;. Note here that &lt;code&gt;dplyr&lt;/code&gt; functions can take more than one argument at a time. So we do not need to repeat the function for each filter operation. For a list of other operators see this quick &lt;a href=&#34;http://www.statmethods.net/management/operators.html&#34;&gt;table&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- filter(p1, pubyear &amp;gt;= 1970, pubyear &amp;lt;= 2012)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now go ahead and create a publication total table &lt;code&gt;pt&lt;/code&gt; using &lt;code&gt;count()&lt;/code&gt; in &lt;code&gt;dplyr&lt;/code&gt;. &lt;code&gt;count&lt;/code&gt; is actually a wrapper for two other &lt;code&gt;dplyr&lt;/code&gt; functions, &lt;code&gt;group_by&lt;/code&gt; and &lt;code&gt;summarise&lt;/code&gt;. We do not need to use those because &lt;code&gt;count&lt;/code&gt; does that for us. Note here that &lt;code&gt;wt&lt;/code&gt; for &lt;code&gt;weight&lt;/code&gt; will sum the value of n for us (see &lt;code&gt;?count&lt;/code&gt; for details).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pt &amp;lt;- count(p1, pubyear, wt = n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we inspect &lt;code&gt;pt&lt;/code&gt; we will see that we now have totals for each year. We might want to add a rank or percentage column to that for later use. There are a variety of ways of going about this. However, staying with &lt;code&gt;dplyr&lt;/code&gt;, behind the scenes &lt;code&gt;count&lt;/code&gt; function has grouped the data for us (see &lt;code&gt;?count&lt;/code&gt;). To understand this use &lt;code&gt;str(pt)&lt;/code&gt; in the console to view the data. This will reveal that we have a grouped data frame with attributes. To go further we will ungroup the table first. [Note that ungrouping is not normally necessary but is used here because of an unexpected problem calculating a percentage on a grouped table using &lt;code&gt;sum(n)&lt;/code&gt; in &lt;code&gt;dplyr&lt;/code&gt;].&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pt &amp;lt;- ungroup(pt)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with filter we can pass multiple arguments to &lt;code&gt;mutate&lt;/code&gt;. To demonstrate this we will add a column with a percentage score, then use &lt;code&gt;ntile&lt;/code&gt; to split the data into 3 groups and then calculate the percent rank using &lt;code&gt;percent_rank(nn)&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pt &amp;lt;- mutate(pt, group = ntile(nn, 3),
             percent = nn / sum(nn) * 100, 
             rank = percent_rank(nn))
pt&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 43 x 5
##    pubyear    nn group percent   rank
##      &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1    1970    2.     1  0.0223 0.    
##  2    1971    6.     1  0.0668 0.0238
##  3    1972   14.     1  0.156  0.119 
##  4    1973    9.     1  0.100  0.0714
##  5    1974    7.     1  0.0780 0.0476
##  6    1975   11.     1  0.123  0.0952
##  7    1976   27.     1  0.301  0.190 
##  8    1977   24.     1  0.267  0.143 
##  9    1978   30.     1  0.334  0.214 
## 10    1979   26.     1  0.290  0.167 
## # ... with 33 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This neatly demonstrates how easy it is to use mutate to add columns based on different calculations.&lt;/p&gt;
&lt;p&gt;The reason that we are focusing on adding counts to the publication total table is that when graphing later we can use these columns to split and order the graphics. This is particularly helpful because with patent data we normally have widely varying scores that produce crunched graphs. The availability of either buckets or percentages is very helpful for creating ranked bar charts or plots and faceting (trellis graphs). As we often want to see what happens with a graph before deciding how to proceed or drop data it is useful to have a ranking system. We can then filter the data using function at a later stage.&lt;/p&gt;
&lt;p&gt;###Creating a Publication Country Table&lt;/p&gt;
&lt;p&gt;We will follow the same procedure for the publication country table. However, in this case we will illustrate the use of pipes to simplify the process. We will use the most common pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt;, which means “then”. This basically says, “this” then “that”. Select and run the code.&lt;/p&gt;
&lt;!--- could probably use n() here. Note that if attempt to use n then it thinks it is the function.---&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pc &amp;lt;- count(p1, pubcountry, pubcode, wt = n) %&amp;gt;%
    ungroup() %&amp;gt;%
    mutate(group = ntile(nn, 3),
           percent = nn / sum(nn) * 100,
           rank = percent_rank(nn)) %&amp;gt;%
    arrange(desc(nn)) 
print(pc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 16 x 6
##    pubcountry                   pubcode    nn group percent   rank
##    &amp;lt;chr&amp;gt;                        &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1 United States of America     US      4267.     3 47.5    1.000 
##  2 Patent Co-operation Treaty   WO      1390.     3 15.5    0.933 
##  3 Canada                       CA      1316.     3 14.7    0.867 
##  4 European Patent Office       EP      1184.     3 13.2    0.800 
##  5 Korea, Republic of           KR       308.     3  3.43   0.733 
##  6 Japan                        JP       205.     2  2.28   0.667 
##  7 Germany                      DE        89.     2  0.991  0.600 
##  8 South Africa                 ZA        72.     2  0.802  0.533 
##  9 China                        CN        60.     2  0.668  0.467 
## 10 Israel                       IL        35.     2  0.390  0.400 
## 11 Mexico                       MX        23.     1  0.256  0.333 
## 12 Portugal                     PT        10.     1  0.111  0.200 
## 13 Russian Federation           RU        10.     1  0.111  0.200 
## 14 Eurasian Patent Organization EA         4.     1  0.0446 0.133 
## 15 Spain                        ES         3.     1  0.0334 0.0667
## 16 Singapore                    SG         2.     1  0.0223 0.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While we will not focus on using pipes in this article, they simplify the writing of code in R and make it clearer. The above code is identical to the code below. However, note that in the code below we have to keep overwriting each time we add an element. Also note that in the code above we only mention our reference table, p1, once at the beginning whereas in the version without pipes or chaining, we have to mention it as the first argument of each function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pc &amp;lt;- count(p1, pubcountry, pubcode, wt = n)
   pc &amp;lt;- ungroup(pc)
   pc &amp;lt;- mutate(pc, group = ntile(nn, 3), 
                percent = nn / sum(nn) * 100, 
                rank = percent_rank(nn))
   pc &amp;lt;- arrange(pc, desc(nn))
   pc&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 16 x 6
##    pubcountry                   pubcode    nn group percent   rank
##    &amp;lt;chr&amp;gt;                        &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1 United States of America     US      4267.     3 47.5    1.000 
##  2 Patent Co-operation Treaty   WO      1390.     3 15.5    0.933 
##  3 Canada                       CA      1316.     3 14.7    0.867 
##  4 European Patent Office       EP      1184.     3 13.2    0.800 
##  5 Korea, Republic of           KR       308.     3  3.43   0.733 
##  6 Japan                        JP       205.     2  2.28   0.667 
##  7 Germany                      DE        89.     2  0.991  0.600 
##  8 South Africa                 ZA        72.     2  0.802  0.533 
##  9 China                        CN        60.     2  0.668  0.467 
## 10 Israel                       IL        35.     2  0.390  0.400 
## 11 Mexico                       MX        23.     1  0.256  0.333 
## 12 Portugal                     PT        10.     1  0.111  0.200 
## 13 Russian Federation           RU        10.     1  0.111  0.200 
## 14 Eurasian Patent Organization EA         4.     1  0.0446 0.133 
## 15 Spain                        ES         3.     1  0.0334 0.0667
## 16 Singapore                    SG         2.     1  0.0223 0.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we have added the function &lt;code&gt;arrange&lt;/code&gt; and given it the value of &lt;code&gt;n&lt;/code&gt;. Arrange will sort a table on a column in ascending order by default and descending order using &lt;code&gt;desc&lt;/code&gt; inside the function as in this case. Use &lt;code&gt;View(pc)&lt;/code&gt; to take a look. To reverse the order try the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pc &amp;lt;- arrange(pc, nn)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will arrange the &lt;code&gt;pc&lt;/code&gt; data by the value of &lt;code&gt;n&lt;/code&gt; in the default ascending order.&lt;/p&gt;
&lt;p&gt;###Creating a Publication Country by Year Table&lt;/p&gt;
&lt;p&gt;For the publication country by year we do the same but retain all the columns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pcy &amp;lt;- count(p1, pubcountry, pubcode, pubyear, wt = n) %&amp;gt;% ungroup()
pcy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 296 x 4
##    pubcountry pubcode pubyear    nn
##    &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
##  1 Canada     CA         1971    2.
##  2 Canada     CA         1972    4.
##  3 Canada     CA         1974    1.
##  4 Canada     CA         1975    1.
##  5 Canada     CA         1976    1.
##  6 Canada     CA         1977    1.
##  7 Canada     CA         1978    4.
##  8 Canada     CA         1979    8.
##  9 Canada     CA         1980   11.
## 10 Canada     CA         1981   14.
## # ... with 286 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a table that sums up the values by country and year. However, we are now presented with a conundrum. We have lost our groups and ranks. We cannot simply repeat what we did before (e.g. ntile) because the data is now split by year and it will not assign correctly. In addition, using other functions is made difficult by the fact our columns are character columns.&lt;/p&gt;
&lt;p&gt;One solution to this conundrum is to join the &lt;code&gt;pcy&lt;/code&gt; and the &lt;code&gt;pc&lt;/code&gt; tables together using the &lt;code&gt;join&lt;/code&gt; functions in &lt;code&gt;dplyr&lt;/code&gt;. We will start by creating a temporary table that we will call &lt;code&gt;df&lt;/code&gt; and use &lt;code&gt;select&lt;/code&gt; to drop the columns we don’t want, then we will create a new &lt;code&gt;pcy&lt;/code&gt; using &lt;code&gt;left_join&lt;/code&gt; from &lt;code&gt;dplyr&lt;/code&gt; (see &lt;code&gt;join&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- select(pc, pubcode, group)
  pcy &amp;lt;- left_join(pcy, df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a publication country by year table where groups are assigned as in the previous table. Note that for a join to be made the table must possess one or more shared columns that serve as a key for the join (in this case the shared key is &lt;code&gt;pubcode&lt;/code&gt;). It can be a very good idea to retain a field to use as a shared key where you expect to be joining data at a later stage. Also note that this is one solution, it is not necessarily the best or most efficient solution.&lt;/p&gt;
&lt;p&gt;We could also break our publication countries into separate tables using to select only those rows in a group we want. Here we create three tables based on the groups assigned to them where they match the group number using ==.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;main &amp;lt;- filter(pcy, group == 3)
other &amp;lt;- filter(pcy, group == 2)
low &amp;lt;- filter(pcy, group == 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In practice, we do not always need to create separate tables, however, as we can now see the combination of &lt;code&gt;select&lt;/code&gt; for columns and &lt;code&gt;filter&lt;/code&gt; for rows in makes it easy to create subsets of our data. We have also seen that functions such as &lt;code&gt;left_join&lt;/code&gt; can come in useful where we cannot easily repeat the creation of a variable in a table divided on different variables. While there are almost always other ways of doing things in R, &lt;code&gt;dplyr&lt;/code&gt; and its sister packages &lt;code&gt;tidyr&lt;/code&gt; and &lt;code&gt;plyr&lt;/code&gt; aim to make everyday data preparation and analysis tasks easier.&lt;/p&gt;
&lt;p&gt;We now have four data tables and in the following sections we will work with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pt = the publication total data&lt;/li&gt;
&lt;li&gt;pc = the publication country data&lt;/li&gt;
&lt;li&gt;pcy = the publication country by year table&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will be working through graph creation in a step wise direction so that you can see what is happening at each step. In the process we will create multiple objects. This is not the fastest way to proceed. However, it is transparent in mapping out the steps and the consequences of small chunks of code. Faster ways include using pipes as we have seen above. We have initially chosen the slower route to explain what is going on. However, you may want to experiment with pipes on some of the code at the end of this article.&lt;/p&gt;
&lt;p&gt;This section has usefully reminded us that data preparation is a fundamental step in any patent analysis task including visualisation. Choices made at this stage will determine the ease or difficulty with which we can perform particular visualisation tasks later. However, there is nothing to stop us returning to our data wrangling functions at a later stage. For example, we may well discover that the three bins we have created using &lt;code&gt;ntile&lt;/code&gt; should be 4 or we might want to use a calculation to determine the number of bins. This need for adjustment is a normal part of data analysis. For more information on data wrangling in R using &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;tidyr&lt;/code&gt; see the &lt;a href=&#34;https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf&#34;&gt;RStudio Data Wrangling Cheatsheet&lt;/a&gt; and Garrett Grolemund’s excellent &lt;a href=&#34;https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf&#34;&gt;Data Wrangling with R&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;##Getting Going with ggplot in ggplot2&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ggplot()&lt;/code&gt; basically builds graphics from separate layers. Those layers are added, adjusted and specified in the ggplot function using small chunks of code describing different elements of the graph. Essential background reading here is &lt;a href=&#34;http://www.cookbook-r.com/Graphs/&#34;&gt;Winston Chang’s Cookbook for R Graphics website&lt;/a&gt; and book along with &lt;a href=&#34;http://www.r-bloggers.com/search/ggplot2&#34;&gt;R-bloggers posts on ggplot2&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;###Establishing the Base Object&lt;/p&gt;
&lt;p&gt;Most ggplot2 tutorials do not refer to a base object, possibly because it is obvious. However, this term was used in Hadley Wickham’s 2010 article &lt;a href=&#34;http://vita.had.co.nz/papers/layered-grammar.pdf&#34;&gt;A Layered Grammar of Graphics&lt;/a&gt; and helps us to understand the layering process. The base object consists of the data we want to graph and the aesthetic mappings or &lt;code&gt;aes&lt;/code&gt;. This is basically what we want to see on a graphic. Using the &lt;code&gt;pt&lt;/code&gt; (totals) table we can create a base object &lt;code&gt;t&lt;/code&gt; for total as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t &amp;lt;- ggplot(pt, aes(pubyear, weight = nn))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This follows the pattern of:&lt;/p&gt;
&lt;p&gt;data = pt, aesthetic mappings = (x = publication year, y = the sum of n)&lt;/p&gt;
&lt;p&gt;We now have a base object with the &lt;code&gt;aes&lt;/code&gt; mappings. Bear in mind that we will need to change this base object for particular types of graph.&lt;/p&gt;
&lt;p&gt;If we enter &lt;code&gt;t&lt;/code&gt; in the console now then we will see an error message &lt;code&gt;Error: No layers in plot&lt;/code&gt;. That is because we haven’t defined how we want to see the data using a &lt;code&gt;geom&lt;/code&gt;. So let’s add a &lt;code&gt;geom&lt;/code&gt; now.&lt;/p&gt;
&lt;p&gt;###Adding a &lt;code&gt;geom&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The geometric object is basically the form we want to see the data in plus any additional conditions we might want to set.&lt;/p&gt;
&lt;p&gt;For a bar graph (histogram) we would specify &lt;code&gt;t&lt;/code&gt; and &lt;code&gt;+&lt;/code&gt; for the &lt;code&gt;geom&lt;/code&gt; for a bar graph. Inside the &lt;code&gt;geom_bar()&lt;/code&gt; function we can add some additional details.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t + geom_histogram(colour = &amp;quot;white&amp;quot;, fill = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig1_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!--- this is not looking as I had expected!---&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig1_gplot2-1.png)---&gt;
&lt;p&gt;When we press command enter (or enter &lt;code&gt;t&lt;/code&gt; in the console) we will now see a bar chart.&lt;/p&gt;
&lt;p&gt;The geom specifies what we want to see. Note here that we have:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Created a white outline around the bars using colour = “white”&lt;/li&gt;
&lt;li&gt;Created a fill using fill = “red”. The default is black.&lt;/li&gt;
&lt;li&gt;We have specified bindwidth = 1 because the default is the range divided by 30. This
has been described as “perverse” by Hadley Wickham. It is intended to encourage thought and experimentation in binning data. In this case our base object is weighted to sum on the values of and we want to see the values in a bin for each year.&lt;/li&gt;
&lt;li&gt;The default for counts in a bar chart is a count of cases (number of observations in the bin), not a count of values. For that reason we specified in the base object for the y axis (weight can also be written as ).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;###Changing the Geom&lt;/p&gt;
&lt;p&gt;If we wanted to change the &lt;code&gt;geom&lt;/code&gt; to generate a different kind of graph, such as a line graph we would first need to make an adjustment to our base object. In this case we are specifying that the y axis is &lt;code&gt;n&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;l &amp;lt;- ggplot(pt, aes(pubyear, nn, weight = nn))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To view this we now specify the geom as line:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;l + geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig2_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig2_gplot2-1.png)---&gt;
&lt;p&gt;We now have a line graph. Let’s imagine that we wanted to change this to a point (scatter plot).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;l + geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig3_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig3_gplot2-1.png)---&gt;
&lt;p&gt;However, now we would like to have a line and a scatterplot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;l + geom_point() + 
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig4_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig4_gplot2-1.png)---&gt;
&lt;p&gt;This will then draw a line through our scatterplot points. For the sake of illustration we could add another geom to our list to convert to an area graph.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;l + geom_point() + 
  geom_line() + 
  geom_area()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig5_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig5_gplot2-1.png)---&gt;
&lt;p&gt;There is clearly not much point in using this combination because the area geom obliterates the other geoms. But, it illustrates that we can play around with adding and removing geometric objects without going back and changing our base object. We are simply adding and removing layers specifying the geometric object(s) we want to see in the plot.&lt;/p&gt;
&lt;p&gt;For readers following this article from the previous Tableau Public article note that our area graph is the same as in panel 1 of the Tableau Public version &lt;a href=&#34;https://public.tableau.com/views/pizzapatents/Overview?:embed=y&amp;amp;:display_count=yes&amp;amp;:showTabs=y&#34;&gt;here&lt;/a&gt;. To reproduce that in R we will start using themes from the ggthemes package. We will start by creating a base object with no fill specified in &lt;code&gt;l1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;l1 &amp;lt;- ggplot(pt, aes(pubyear, nn, weight = nn, fill=&amp;quot;&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now add layers including the theme. In this case we have chosen &lt;code&gt;theme_igray&lt;/code&gt; and specified the font as Helvetica, then &lt;code&gt;scale_fill_tableau&lt;/code&gt; specifying the “tableau20” palette. Because something for &lt;code&gt;fill&lt;/code&gt; has to be specified in the base object we end up with a legend we don’t need. To remove that when applying the theme we add a + &lt;code&gt;theme(legend.position=&amp;quot;none&amp;quot;)&lt;/code&gt; to drop the legend.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;l2 &amp;lt;- l1+ geom_area() +
  theme_igray(base_family = &amp;quot;Helvetica&amp;quot;) +
  scale_fill_tableau(&amp;quot;tableau20&amp;quot;) +
  theme(legend.position=&amp;quot;none&amp;quot;)
l2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig6_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig6_gplot2-1.png)---&gt;
&lt;p&gt;That is very close to the filled graph in our &lt;a href=&#34;https://public.tableau.com/views/pizzapatents/Overview?:embed=y&amp;amp;:display_count=yes&amp;amp;:showTabs=y&#34;&gt;Tableau Public workbook&lt;/a&gt;. A range of themes are available as part of ggthemes the package and are well worth exploring.&lt;/p&gt;
&lt;p&gt;For illustration, we could add information that augments our understanding of the data. First, we could add a line. Second, we could add a rug to the plot. A rug adds a line for each of the observations in the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;l2 + geom_line() +
  geom_rug()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig7_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig7_gplot2-1.png)---&gt;
&lt;p&gt;What has happened here is that the addition of geom_rug has added a line to the left and the bottom for each of the observations. Thus, on the x axis, each year gets a tick. On the y axis each observation (value) in the data gains a tick.&lt;/p&gt;
&lt;p&gt;What is clear from this is that we do not need a rug for the year. So, let’s change that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;l2 + geom_rug(sides = &amp;quot;l&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig8_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig8_gplot2-1.png)---&gt;
&lt;p&gt;In this case we have specified that the rug should appear on the left (the other options are t, r, b for top, right and bottom respectively).&lt;/p&gt;
&lt;p&gt;A rug plot is useful for spotting outliers in the data. In this case it is telling us where the observations that make up the area plot are concentrated over the years. However, again for illustration, we could also use a different dataset to add a rug layer. Let’s try this with our publication country year or &lt;code&gt;pcy&lt;/code&gt; table.&lt;/p&gt;
&lt;p&gt;Rather than seeing the distribution of the scores for the overall data, we are now seeing the rug by country. This tells us that no country has a score in any year above 300, with the bulk at under 100 per year. Clearly, this is not the best use of a rug, but it is a simple example of adding a layer from a different dataset to augment the information in another dataset. But, it does suggest that the majority of country observations making up the total are concentrated at less than 100 documents per year and two groupings between 200 to 300. The latter are from the US (see the &lt;code&gt;pcy&lt;/code&gt; table).&lt;/p&gt;
&lt;p&gt;What if we wanted to see the overall trend and the trend for each country on the same graphic? This might initially appear to be difficult because the total data is in the data table and the country year data is in the &lt;code&gt;pcy&lt;/code&gt; table. However, they share the same columns and (thanks to Didzis Efferts answer to this stackoverflow question question) we can do that in two steps.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;First we use the ggplot call for the data as before but add +geom_line() to draw the line graph for the total data.&lt;/li&gt;
&lt;li&gt;We add a second &lt;code&gt;geom_line()&lt;/code&gt; and then specify the data that we want to use and its aesthetic mappings inside the function. We will stay with the tableau theme in &lt;code&gt;ggthemes&lt;/code&gt;
and adapt this for a line graph using &lt;code&gt;colour&lt;/code&gt; rather than &lt;code&gt;fill&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will place that in an object called &lt;code&gt;co&lt;/code&gt; for combined.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;co &amp;lt;- ggplot(pt, aes(pubyear, nn, weight = nn)) +
  geom_line() +
  geom_line(data = pcy, aes(pubyear, nn, weight = nn, colour = pubcode)) +
  theme_igray() + scale_color_tableau(palette = &amp;quot;tableau20&amp;quot;)
co&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig9_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig9_gplot2-1.png)---&gt;
&lt;p&gt;To view the graph type &lt;code&gt;co&lt;/code&gt; in the console. Note that we have used pubcode (for the two letter country code), rather than the full country name because the graphic is easier to read.&lt;/p&gt;
&lt;p&gt;We could add to this graphic in at least two ways. Let’s try adding a trend line.&lt;/p&gt;
&lt;p&gt;###Adding a trend line&lt;/p&gt;
&lt;p&gt;What we would like to know here is the trend for pizza patent publications using a regression (see &lt;code&gt;?geom_smooth()&lt;/code&gt; and &lt;code&gt;?stat_smooth()&lt;/code&gt; for further details).&lt;/p&gt;
&lt;p&gt;To do that we will work with &lt;code&gt;co&lt;/code&gt; as follows by changing geom_line() on the pt data to geom_point() for a scatter plot and then adding a &lt;code&gt;geom_smooth()&lt;/code&gt;. By default &lt;code&gt;geom_smooth()&lt;/code&gt;
will draw a grey area indicating the standard error interval (se). That interval is set to 0.95 by default. We can do this quite easily by adding &lt;code&gt;geom_smooth()&lt;/code&gt; to &lt;code&gt;co&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;co + geom_smooth()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig10_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig10_gplot2-1.png)---&gt;
&lt;p&gt;We will now see a message telling us what is happening “geom_smooth: method=”auto&amp;quot; and size of largest group is &amp;lt;1000, so using loess. Use ‘method = x’ to change the smoothing method.&amp;quot;&lt;/p&gt;
&lt;p&gt;If we leave the standard error interval for the trend line on then our graph becomes hard to understand. We can turn it off we can set an argument inside as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;co + geom_smooth(se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig11_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig11_gplot2-1.png)---&gt;
&lt;p&gt;To experiment with an alternative method try &lt;code&gt;+ geom_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE)&lt;/code&gt; for a linear model and look up &lt;code&gt;?geom_smooth&lt;/code&gt; and associated &lt;code&gt;?stat_smooth&lt;/code&gt;. We would of course want to add a label for the trend line but we will come to labels below. &lt;strong&gt;&lt;em&gt;Add label&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;###Statistical Transformations&lt;/p&gt;
&lt;p&gt;As this suggests there are a range of statistical transformations available in ggplot2 and some are used in the default settings (such as a bar chart and binwidth). Other options include stat_density(), stat_contour() and stat_quantile() with details listed in ggplot2 under Packages and on the second page of the &lt;a href=&#34;https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf&#34;&gt;RStudio Data Visualization cheatsheet&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;###Adding Labels&lt;/p&gt;
&lt;p&gt;To add labels use the function &lt;code&gt;labs()&lt;/code&gt;. Note that the title= , x = and y = must be specified if you want to include labels. Otherwise R will not know what to do with the information.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;co + geom_smooth(se = FALSE) + 
  labs(title = &amp;quot;Patentscope Pizza Patents&amp;quot;,
          x = &amp;quot;Publication Year&amp;quot;,
          y = &amp;quot;Publication Count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig12_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;%7B%7B%20site.baseurl%20%7D%7D/images/fggplot2/fig12_gplot2-1.png&#34; alt=&#34;_config.yml&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;_config.yml&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In addition to the general labs argument you can also separately use&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;ggtitle(“Patentscope Pizza Patents”) for a main title.&lt;/li&gt;
&lt;li&gt;xlab(“New xlab”) 3. ylab(“New y lab”)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will go into more detail on legends below.&lt;/p&gt;
&lt;p&gt;###Publication Country Charts&lt;/p&gt;
&lt;p&gt;To illustrate different types of charts we will now work with the publication country &lt;code&gt;pc&lt;/code&gt; data frame that we created earlier.&lt;/p&gt;
&lt;p&gt;###A Bar chart&lt;/p&gt;
&lt;p&gt;￼Again first we set the data and aesthetic mappings, specifying the publication country code as the fill (fill with colour) and color as pubcode.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ct &amp;lt;- ggplot(pc, aes(pubcode, weight = nn, fill = pubcode))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we set the geom, in this case for &lt;code&gt;geom_bar()&lt;/code&gt;. Because the x axis will have country code labels we will remove the legend using &lt;code&gt;guides(fill=FALSE)&lt;/code&gt;. Note that the legend arises from &lt;code&gt;fill=pubcode&lt;/code&gt; in the base object. Alternative ways of removing a legend include &lt;code&gt;scale_fill_discrete(guide = FALSE)&lt;/code&gt; or &lt;code&gt;theme(legend.position=&amp;quot;none&amp;quot;)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ct + geom_bar() + guides(fill=FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig13_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig13_gplot2-1.png)---&gt;
&lt;p&gt;Note here that if we changed &lt;code&gt;fill = pubcode&lt;/code&gt; in &lt;code&gt;ct&lt;/code&gt; to &lt;code&gt;colour = pubcode&lt;/code&gt; then the countries would be outlined with colour. Try this, then select Enter at the end of the line to update the object.&lt;/p&gt;
&lt;p&gt;Following Edward Tufte we could also try a simpler approach. To do that we apply &lt;code&gt;theme_tufte()&lt;/code&gt; from &lt;code&gt;ggthemes&lt;/code&gt; and change the font to Helvetica, or something other than the default Serif in this theme, by specifying the &lt;code&gt;base_family&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ct1 &amp;lt;- ct + geom_bar() +
  theme_tufte(base_family = &amp;quot;Helvetica&amp;quot;) +
  guides(fill=FALSE)
ct1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig14_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig14_gplot2-1.png)---&gt;
&lt;p&gt;This is a nice simple example of removing clutter to focus in on what matters: presenting the data. We can also apply the tableau theme for consistency with our trends plots. In this case we will use &lt;code&gt;theme(legend.position=&amp;quot;none&amp;quot;)&lt;/code&gt; to turn off the legend.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ct2 &amp;lt;- ct + geom_bar() +
  theme_igray() +
  scale_color_tableau(&amp;quot;tableau20&amp;quot;) +
  theme(legend.position=&amp;quot;none&amp;quot;)
ct2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig15_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig15_gplot2-1.png)---&gt;
&lt;p&gt;A range of other options for controlling fonts and legends with practical demonstrations can be found on &lt;a href=&#34;http://www.cookbook-r.com/Graphs/&#34;&gt;Winston Chang’s R Graphics Cookbook website&lt;/a&gt;. Adjusting line weight and backgrounds can contribute to bringing data to the fore while minimising noise and what Tufte in The Visualisation of Quantitative Information called “chart junk”.&lt;/p&gt;
&lt;p&gt;###Adding labels to columns&lt;/p&gt;
&lt;p&gt;We can also add the values to our bars for our new object . We can achieve this using &lt;code&gt;geom_text&lt;/code&gt;. In this case we needed to specify the y axis again inside &lt;code&gt;geom_text&lt;/code&gt;. &lt;code&gt;vjust&lt;/code&gt; specifies the vertical justification with the horizontal being &lt;code&gt;hjust&lt;/code&gt;. In this case we have also specified the font size in &lt;code&gt;geom_text()&lt;/code&gt; as 12.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ct2 + geom_text(aes(y = nn, label = nn, size = 12), vjust= -0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig16_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig16_gplot2-1.png)---&gt;
&lt;p&gt;￼If we wanted to make this more closely resemble the country count chart from our earlier Tableau workbook (a stacked horizontal bar chart) we would need to swap around the axes. To do that we add &lt;code&gt;coord_flip()&lt;/code&gt; to flip around x and y and then we adjust the labelling to use &lt;code&gt;hjust&lt;/code&gt; for horizontal justification rather than &lt;code&gt;vjust&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To create the ranked bar we need to use the function &lt;code&gt;reorder()&lt;/code&gt; with either the x or the y axis. In this case it is the x axis. We need to specify both x and the y inside &lt;code&gt;reorder()&lt;/code&gt; for this to work. In other cases you may need to go back to your original data table to adjust the data.&lt;/p&gt;
&lt;p&gt;Depending on the font used you may have to change the &lt;code&gt;hjust&lt;/code&gt; value or adjust the font size. Note that only the first four lines are essential to this code. The remainder are for our tableau theme and to hide the legend.&lt;/p&gt;
&lt;p&gt;###A Ranked Bar Chart&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ctr &amp;lt;- ggplot(pc, aes(x = reorder(pubcode, nn), y = nn, fill = pubcode)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;) +
  coord_flip() +
  geom_text(aes(y = nn, label = nn, size = 12), hjust = -0.1) +
  theme_igray() +
  scale_color_tableau(&amp;quot;tableau20&amp;quot;) +
  theme(legend.position=&amp;quot;none&amp;quot;) +
  labs(title=&amp;quot;Patentscope Pizza Patents&amp;quot;, x = &amp;quot;Publication Country&amp;quot;, y = &amp;quot;Publication Count&amp;quot;)
ctr&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig17_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig17_gplot2-1.png)---&gt;
&lt;p&gt;There is room for improvement here, such as the colour of the country names (or replacing them with actual names) and adjusting the font size to more closely match the axis fonts. It can also be tricky to ensure that the highest value (in this case the label for the US) stays inside the plot. However, it is pretty close. Turning now to our patent country year table &lt;code&gt;pcr&lt;/code&gt; we will look at some other chart forms and coordinate systems.&lt;/p&gt;
&lt;p&gt;###A Dot Plot&lt;/p&gt;
&lt;p&gt;For a simple dot plot of the same data we can make three changes. 1. We change fill to colour in the base object. 2. We change &lt;code&gt;geom_bar&lt;/code&gt; to &lt;code&gt;geom_point&lt;/code&gt; and specify the size of the dots. 3. For the labelled values we change hjust.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ctd &amp;lt;- ggplot(pc, aes(x = reorder(pubcode, nn), y = nn, colour = pubcode)) + 
  geom_point(size = 3) +
  coord_flip() +
  geom_text(aes(y = nn, label = nn, size = 12), hjust = -0.4) +
  theme_igray() +
  scale_color_tableau(&amp;quot;tableau20&amp;quot;) +
  theme(legend.position=&amp;quot;none&amp;quot;) +
  labs(title=&amp;quot;Patentscope Pizza Patents&amp;quot;, x = &amp;quot;Publication Country&amp;quot;, y = &amp;quot;Publication Count&amp;quot;)
ctd&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig18_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig18_gplot2-1.png)---&gt;
&lt;p&gt;Note here that one limitation of this dot plot is the adjustment of &lt;code&gt;hjust&lt;/code&gt; where smaller numbers appear closer to the dots than larger numbers. This suggests a need for experimentation with &lt;code&gt;hjust&lt;/code&gt; and possibly the size and assessing whether to retain the labels. Options for recolouring labels might also be considered.&lt;/p&gt;
&lt;p&gt;###A Balloon Plot&lt;/p&gt;
&lt;p&gt;Another way or representing the data would be as a set of balloons sized on the number of records.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ctb &amp;lt;- ggplot(pc, aes(x = pubcode, y = nn, size = nn)) + 
  geom_point(shape = 21, colour = &amp;quot;black&amp;quot;, fill = &amp;quot;cornsilk&amp;quot;) +
  scale_size_area(max_size = 15) + 
  theme_igray() +
  scale_color_tableau(&amp;quot;tableau20&amp;quot;) +
  theme(legend.position=&amp;quot;none&amp;quot;) +
  labs(title=&amp;quot;Patentscope Pizza Patents&amp;quot;, x = &amp;quot;Publication Country&amp;quot;, y = &amp;quot;Publication Count&amp;quot;)
ctb&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig19_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig19_gplot2-1.png)---&gt;
&lt;p&gt;This is a very basic balloon plot and plots of this type could be readily adapted to represent a wide variety of patent data.&lt;/p&gt;
&lt;p&gt;###A Stacked Bar Chart&lt;/p&gt;
&lt;p&gt;To create a stacked bar chart using our &lt;code&gt;pcy&lt;/code&gt; data we can use the following. We will however rapidly run into some of the aesthetic problems that are common with stacked bar charts. In reading this section, consider whether a stacked bar chart is really the best way to represent the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sb &amp;lt;- ggplot(pcy, aes(pubyear, weight = nn, fill = pubcode)) +
  geom_bar(binwidth = 1)
sb&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig20_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig20_gplot2-1.png)---&gt;
&lt;p&gt;In practice a filled graph of this type could reasonably be described as a mess. This arises from the number of countries involved and the default colors. It is in fact less clear than the line plot by country created above. Stacked plots also suffer from problems with the order of the data and the legend. That is, as we saw above, simpler is generally better in conveying information.&lt;/p&gt;
&lt;p&gt;It is not a good idea to attempt to prettify a bad graph because it will still be a bad graph. Indeed we encounter multiple issues if we attempt to improve this stacked bar. We will illustrate this with the &lt;code&gt;main&lt;/code&gt; table we created earlier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sm &amp;lt;- ggplot(main, aes(pubyear, weight = nn, fill = pubcode))&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Adding a colour border.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We are presently using fill on the pubcode, but what if we specified a colour for the borders of the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sm + geom_bar(binwidth = 1, colour = &amp;quot;black&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig21_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig21_gplot2-1.png)---&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The order of the bars&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The addition of the colour outline is an improvement but the order of the stacked bars is not correct. We can attempt to adjust for this with a quick use of arrange&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;main &amp;lt;- arrange(main, pubcode)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A definite improvement but not earth shattering and we now have a filled legend. We can try changing the palette.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sm + geom_bar(binwidth=1, colour=&amp;quot;black&amp;quot;) +
  theme_igray() +
  scale_fill_tableau(&amp;quot;tableau20&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig22_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig22_gplot2-1.png)---&gt;
&lt;p&gt;We can also start to work on the line colour and the line weight in the function to try to clarify the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sm + geom_bar(binwidth = 1, colour = &amp;quot;gray&amp;quot;, size = .25) +
  theme_igray(base_family = &amp;quot;Helvetica&amp;quot;) +
  scale_fill_tableau(&amp;quot;tableau20&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig23_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig23_gplot2-1.png)---&gt;
&lt;p&gt;That is an improvement of sorts. We could then do the same for the other tables while noting that a means will be needed to ensure that different colours are used on the countries across the summary plots.&lt;/p&gt;
&lt;p&gt;As this suggests stacked bar charts present communication difficulties. That is they take quite a lot of work to get right and the ultimate result may still be difficult for a reader to interpret.&lt;/p&gt;
&lt;p&gt;We could try this as a stacked area graph as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sm &amp;lt;- ggplot(main, aes(pubyear, y = nn, fill = pubcode, order = pubcode)) +
   geom_area() +
   theme_igray(base_family = &amp;quot;Helvetica&amp;quot;) +
   scale_fill_tableau(&amp;quot;tableau20&amp;quot;)
sm&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig24_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig24_gplot2-1.png)---&gt;
&lt;p&gt;That looks promising but note that the order of the stack is not correct and can give a misleading impression. Thus the order should be descending from US, WO, CA, EP, KR. At the time of writing an easy way of adjusting this has not been identified. As such, significant energy could be expended attempting to improve what is essentially a bad graph.&lt;/p&gt;
&lt;p&gt;###Faceting&lt;/p&gt;
&lt;p&gt;One of the problems we commonly encounter in patent analysis is that data is compressed by dominant players, whether these be countries, applicants or technology areas. One important approach to addressing this problem is to break the data out into multiple individual plots by faceting that data. The term faceting in ggplot is perhaps more familiar as the creation of trellis graphs.&lt;/p&gt;
&lt;p&gt;To facet our publication by country and year data from the graph above we will start by creating an object with the information below. We will leave out labels. We will however add a control that excludes the legend because it will be redundant and get in the way of the large plot we will be creating.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f &amp;lt;- ggplot(pcy, aes(x = pubyear, y = nn, color = pubcode)) +
  geom_point() +
  theme_igray(base_family = &amp;quot;Helvetica&amp;quot;) +
  scale_fill_tableau(&amp;quot;tableau20&amp;quot;) +
  theme(legend.position=&amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The key to creating the facet plot is selecting:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;the right faceting option (either &lt;code&gt;facet_grid&lt;/code&gt; or &lt;code&gt;facet_wrap&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;the variable to facet on which is specified using tilde &lt;code&gt;~&lt;/code&gt; in the function e.g &lt;code&gt;~pubcode&lt;/code&gt; to facet only on pubcode or &lt;code&gt;pubcode~group&lt;/code&gt; for both pubcode and group.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For demonstration we will walk through some options:&lt;/p&gt;
&lt;p&gt;If we choose &lt;code&gt;facet_grid&lt;/code&gt; we will see a line of plots. The main issue that arises here is adjusting the labelling on the x axis. We can do that by adding &lt;code&gt;scale_x_continuous&lt;/code&gt; and specifying the breaks and values we want to see. We could also as needed change the labels by adding to the function like this &lt;code&gt;scale_x_continuous(breaks = c(1970, 2010), labels = c(label1, label2))&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f + facet_grid(~pubcode, shrink = TRUE) +
  scale_x_continuous(breaks = c(1970, 2010))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig25_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig25_gplot2-1.png)---&gt;
&lt;p&gt;However, the above plot suffers from being too vertical and the labels remain squashed. In future it may be possible to adjust this (using &lt;code&gt;+ theme(panel.margin.x = unit(5, &amp;quot;lines&amp;quot;)&lt;/code&gt; but at the time of writing this did not appear to be working) (see this &lt;a href=&#34;http://stackoverflow.com/questions/12252750/alter-just-horizontal-spacing-between-facets-ggplot2&#34;&gt;Stackoverflow discussion&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;An alternative option for a clearer plot might be to use the groups that we created in the data table to facet on.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f + facet_grid(pubcode~group, shrink = TRUE) +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  scale_x_continuous(breaks = c(1970, 2010))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig26_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig26_gplot2-1.png)---&gt;
&lt;p&gt;That is an improvement, as we can see the three groups, but note that the countries are ordered alphabetically rather than by group.&lt;/p&gt;
&lt;p&gt;###facet_wrap&lt;/p&gt;
&lt;p&gt;To create a more reasonable plot we can use &lt;code&gt;facet_wrap()&lt;/code&gt;. Note that the position of the group has been reversed to &lt;code&gt;group~pubcode&lt;/code&gt; to achieve this plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f + facet_wrap(group~pubcode) +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig27_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig27_gplot2-1.png)---&gt;
&lt;p&gt;What we now see is a facet plot that is ordered from group 1 to group 3. This is pretty good.&lt;/p&gt;
&lt;p&gt;While we would generally want to drop some of the smallest values, to improve this type of plot we might try freeing up the scales. In this case we focus on the y axis by specifying &lt;code&gt;free_y&lt;/code&gt; with the alternative being &lt;code&gt;free_x&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f + facet_wrap(group~pubcode, scales =&amp;quot;free_y&amp;quot;) +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig28_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig28_gplot2-1.png)---&gt;
&lt;p&gt;This removes the compression and gives each individual plot its own scale. The default for facets is to show the highest values at the bottom right (the default of argument &lt;code&gt;as.table&lt;/code&gt;). If we specify &lt;code&gt;as.table = FALSE&lt;/code&gt; the facets will show the higher values (in group 3) first. This might improve the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f + facet_wrap(group~pubcode, scales =&amp;quot;free_y&amp;quot;, as.table = FALSE) +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig29_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig29_gplot2-1.png)---&gt;
&lt;p&gt;Note that this still presents some problems as ideally the highest value (the US) would appear first and then read in descending order across from left to right. There appear to be some limitations to the controls possible in &lt;code&gt;facet_wrap&lt;/code&gt; and also our data structure.&lt;/p&gt;
&lt;p&gt;We will revert to the ascending order but keep the free y axis. We could if we wished add a regression to show the trend by country by adding &lt;code&gt;geom_smooth()&lt;/code&gt;. We will remove the standard error area by specifying &lt;code&gt;se = FALSE&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f + facet_wrap(group~pubcode, scales =&amp;quot;free_y&amp;quot;, as.table = TRUE) +
  geom_smooth(se = FALSE) +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig30_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig30_gplot2-1.png)---&gt;
&lt;p&gt;In running this code we will receive a message for each plot informing us of the smoothing method that is being used.&lt;/p&gt;
&lt;p&gt;The issue we encounter here is that a number of countries in group 1 and group 2 have very sparse results with no corresponding meaningful trend to display. In practice we would drop group 1 entirely and focus on group 2 and 3. To conclude, let’s do that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g &amp;lt;- filter(pcy, group %in% c(2,3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;%in%&lt;/code&gt; within filter basically says “extract rows falling into group 2 or 3 from within group”. Once again it is a simple &lt;code&gt;dplyr&lt;/code&gt; solution and easy to remember.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g1 &amp;lt;- ggplot(g, aes(x = pubyear, y = nn, color = pubcode)) +
  geom_point() +
  theme_igray(base_family = &amp;quot;Helvetica&amp;quot;) +
  scale_fill_tableau(&amp;quot;tableau20&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now try again with our trend line using &lt;code&gt;geom_smooth&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g1 + facet_wrap(group~pubcode, scales =&amp;quot;free_y&amp;quot;, as.table = TRUE) +
  geom_smooth(se = FALSE) +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig32_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig32_gplot2-1.png)---&gt;
&lt;p&gt;That is a considerable improvement and we will stop there. One limitation of facet wrap is that it appears that renaming the labels, to leave only the country codes, is not possible (although using labeller may provides solutions). It may also be the case that we would prefer to drop some of the other countries with low scores from group 2. We could do that by applying a filter on pubcode. We could also add some labels to the overall plot.&lt;/p&gt;
&lt;p&gt;##Pie and Coxcomb Plots&lt;/p&gt;
&lt;div id=&#34;creating-a-pie-chart-aaaargh&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating a Pie Chart (aaaargh)&lt;/h3&gt;
&lt;p&gt;A pie chart is the plan view of a stacked bar chart using polar coordinates. Pie charts of the familiar variety are widely criticised because they are hard to accurately interpret. They also tend to become laden with what Edward Tufte called “chart junk”, such as 3D rendering, that distracts from the presentation and communication of the data. Edward Tufte has this to say about pie charts in The Visual Display of Quantitative Information:&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“A table is nearly always better than a dumb pie chart; the only worse design than a pie chart is several of them, for then the viewer is asked to compare quantities located in spatial disarray both within and between pies… Given their low data-density and failure to order numbers along a visual dimension, pie charts should never be used.”(Tufte 2001: 178)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, Tufte’s criticism has done relatively little to diminish the popularity of the pie chart and it seems, almost everyone knows what a pie chart is.&lt;/p&gt;
&lt;p&gt;Creating a pie chart in ggplot takes some thought. As is often the case when working with R we can find a detailed walk through in the wider user community. In this case we are following the code developed by the &lt;a href=&#34;http://mathematicalcoffee.blogspot.co.uk/2014/06/ggpie-pie-graphs-in-ggplot2.html&#34;&gt;Mathematical Coffee blog entitled ggpie: pie graphs in ggplot2&lt;/a&gt;. For those familiar with R, Mathematical Coffee wraps this into a function called for generating pie charts. We will walk through the steps from the blog post and adapt it to our pizza data.&lt;/p&gt;
&lt;p&gt;The first step is to create an object as a stacked chart. In this case we attribute a single value to the x axis, specify the y axis as percent and then the fill as publication code. This creates a stacked bar chart.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;o1 &amp;lt;- ggplot(pc, aes(x=1, y = percent, fill = pubcode)) + geom_bar(stat = &amp;quot;identity&amp;quot;)
o1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig33_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig33_gplot2-1.png)---&gt;
&lt;p&gt;If we view this it will be a stacked bar. In the second step we create the pie chart by specifying the polar coordinate and &lt;code&gt;theta = y&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y &amp;lt;- o1 + coord_polar(theta = &amp;quot;y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we now enter y in the console we will see a pie chart. The remaining steps involve further defining the aesthetics of the chart, tidying up and adding labels.&lt;/p&gt;
&lt;p&gt;The following adds black lines to define the pie segments. Because these lines are then reflected in the legend the argument overrides this by using colour = NA.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z &amp;lt;- y + geom_bar(stat = &amp;quot;identity&amp;quot;, colour = &amp;quot;black&amp;quot;) +
  guides(fill=guide_legend(override.aes=list(colour=NA)))
z&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig34_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig34_gplot2-1.png)---&gt;
&lt;p&gt;Then remove the various tick marks&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z &amp;lt;- z +
    theme(axis.ticks=element_blank(),
          axis.title=element_blank(),
          axis.text.y=element_blank()
          )
z&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig35_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig35_gplot2-1.png)---&gt;
&lt;p&gt;We need to work out the position of the labels that we would like to see on the pie, and in particular the mid-point of each pie. This can be found as the cumulative sum &lt;code&gt;cumsum&lt;/code&gt; of the variable we used for the slices. That is percent expressed as &lt;code&gt;pc$percent&lt;/code&gt; to state the data frame and the column we want in the &lt;code&gt;percent&lt;/code&gt; table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y.breaks &amp;lt;- cumsum(pc$percent) - pc$percent/2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we print y.breaks to the console we will a list of values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;q &amp;lt;- z +
    theme(axis.text.x=element_text(color=&amp;#39;black&amp;#39;)) +
    scale_y_continuous(
        breaks=y.breaks,
        labels=pc$pubcode)
q&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig36_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig36_gplot2-1.png)---&gt;
&lt;p&gt;Thanks to Mathematical Coffee we now have a functional pie chart of the percentage share of pizza patent documents. The only issue with this plot is that some of the labels on the smaller values are crunched. That could however be addressed by filtering rows in accordance with the groups on the &lt;code&gt;pc&lt;/code&gt; table, as we did above for &lt;code&gt;pcy&lt;/code&gt;, to remove the overlap.&lt;/p&gt;
&lt;p&gt;###The Coxcomb Plot&lt;/p&gt;
&lt;p&gt;A coxcomb plot is generally associated with Florence Nightingales representation of the causes of mortality among the British Army in the Korean War that can be viewed &lt;a href=&#34;&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A coxcomb plot begins with a bar chart as follows. Note that we have chosen &lt;code&gt;n&lt;/code&gt; for the y value from the table. We could have used percent.&lt;/p&gt;
&lt;p&gt;￼￼We will use a filtered version of the table that only contains group 3 results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cx &amp;lt;- filter(pc, group %in% 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We start the plot by creating a bar chart. Note that we are using the value of &lt;code&gt;n&lt;/code&gt; here for the y axis. This could be changed to percent or another value as needed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cx1 &amp;lt;- ggplot(cx, aes(x = pubcode, y = nn, fill= pubcode)) +
  geom_bar(stat=&amp;quot;identity&amp;quot;)
cx1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig37_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig37_gplot2-1.png)---&gt;
&lt;p&gt;We them add a &lt;code&gt;coord_polar&lt;/code&gt; specifying theta in this case as x (the pubcode). The remainder of the code is thematic.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cx1 + coord_polar(theta=&amp;quot;x&amp;quot;) +
   theme_light() +
   theme(legend.position=&amp;quot;none&amp;quot;) +
   labs(x = &amp;quot;Publication Country&amp;quot;, y = &amp;quot;Publication Count&amp;quot;, 
        title  = &amp;quot;Pizza Patent Publications by Country&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-03-graphing-patent-data-with-ggplot2-part-2_files/figure-html/fig38_gplot2-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!---![_config.yml]({{ site.baseurl }}/images/fggplot2/fig38_gplot2-1.png)---&gt;
&lt;p&gt;In the R Graphics Cookbook, Winston Chang adds three observations about coxcomb plots. The first is that for y variables the smallest value is mapped to the centre of the plot, rather than a data value of 0 being mapped to a radius of 0. Second, for a continuous x (or theta) the smallest and largest values are merged. Third, theta values of polar coordinates do not wrap around. As such changes would need to be made to the limits of the plot (see Chang 2013: 200-203 for discussion).&lt;/p&gt;
&lt;p&gt;##Round Up&lt;/p&gt;
&lt;p&gt;In this article we have covered two main topics.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How to prepare data for graphing in R using the &lt;code&gt;dplyr&lt;/code&gt; package.&lt;/li&gt;
&lt;li&gt;How to draw a range of graphs and the issues encountered using ggplot2.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this article we have not explored the full potential of visualisation options in ggplot2, such as heat maps and choropleth maps. These topics are regularly covered on sites such as &lt;a href=&#34;http://www.r-bloggers.com&#34;&gt;r-bloggers.com&lt;/a&gt;. However, we have seen that it is possible to construct graphics from the bottom up using simple code and specifying the details we would like to see.&lt;/p&gt;
&lt;p&gt;We have seen that the great strength of ggplot2 is the ability to control all aspects of a graphic. However, in working with our sample data we have also seen that this can lead to complexity in terms of adjusting data such as arranging or labelling. While it will be possible to bundle the code into functions that could be reused, it is equally clear that significant time investments are involved in working towards publication quality graphics using ggplot2. Those time demands will diminish with improved familiarity but are nevertheless a significant factor.&lt;/p&gt;
&lt;p&gt;To go further with ggplot2 we provide a list of resources below. We also suggest installing R tutorials such as (install.packages(“swirl”)) and learning more about the use of pipes in &lt;code&gt;dplyr&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;##Useful Resources&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;http://www.rstudio.com/wp-content/uploads/2015/05/ggplot2-cheatsheet.pdf&#34;&gt;RStudio Cheatsheet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cookbook-r.com/Graphs/&#34;&gt;R Graphics Cookbook&lt;/a&gt; by Winston Chang&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://vita.had.co.nz/papers/layered-grammar.pdf&#34;&gt;Hadley Wickham 2010 A Layered Grammar of Graphics preprint article&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.amazon.co.uk/Hadley-Wickham/e/B002BOA9GI/ref=sr_tc_2_0?qid=1435678538&amp;amp;sr=1-2-ent&#34;&gt;Hadley Wickham ggplot2 book from Amazon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Swirl tutorials (install.packages(“swirl”)) and &lt;a href=&#34;https://github.com/swirldev/swirl_courses&#34;&gt;Github repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://docs.ggplot2.org/0.9.3/index.html#&#34;&gt;ggplot2 online help topics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.r-bloggers.com/search/ggplot2&#34;&gt;R-Bloggers on ggplot2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://stackoverflow.com/questions/tagged/ggplot2&#34;&gt;Stack Overflow questions and answers on ggplot2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/results?search_query=ggplot2&#34;&gt;YouTube ggplot 2 videos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://mathematicalcoffee.blogspot.co.uk/2014/06/ggpie-pie-graphs-in-ggplot2.html&#34;&gt;Mathematical Coffee Blog Post on Creating a Pie Chart with ggplot2 an the ggpie code&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Wrangling Pizza Patents in R</title>
      <link>/wrangling-pizza-patents-in-r/</link>
      <pubDate>Thu, 25 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>/wrangling-pizza-patents-in-r/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#getting-started-with-r&#34;&gt;Getting Started with R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#about-the-pizza-patent-dataset&#34;&gt;About the pizza patent dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reading-in-the-data&#34;&gt;Reading in the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#creating-a-numeric-field&#34;&gt;Creating a numeric field&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#renaming-columns&#34;&gt;Renaming Columns&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#selecting-columns-for-plotting&#34;&gt;Selecting Columns for plotting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#creating-counts&#34;&gt;Creating Counts&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#total-by-year&#34;&gt;Total by Year&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#round-up&#34;&gt;Round Up&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;This is the first part of a two part article on using R and the &lt;code&gt;ggplot2&lt;/code&gt; package to visualise patent data. In a previous article we looked at visualising pizza related patent activity in Tableau Public. In this article we look at how to wrangle our &lt;code&gt;pizza&lt;/code&gt; dataset using &lt;code&gt;dplyr&lt;/code&gt; package in RStudio to prepare the data for graphing. This is intended as a gentle introduction and you do not need to know anything about R to follow this article. You will however need to install &lt;a href=&#34;http://www.rstudio.com/products/rstudio/&#34;&gt;RStudio Desktop&lt;/a&gt; for your operating system (see below).&lt;/p&gt;
&lt;p&gt;Part 1 will introduce the basics of handling data in R in preparation for plotting and will then use the quick plot or &lt;code&gt;qplot&lt;/code&gt; function in &lt;code&gt;ggplot2&lt;/code&gt; to start graphing elements of the pizza patents dataset.&lt;/p&gt;
&lt;p&gt;Part 2 will go into more depth on handling data in R and the use of ggplot2.&lt;/p&gt;
&lt;p&gt;ggplot2 is an implementation of the theory behind the Grammar of Graphics. The theory was originally developed by Leland Wilkinson and reinterpreted with considerable success by Hadley Wickham at Rice University and RStudio. The basic idea behind the Grammar of Graphics is that any statistical graphic can be built using a set of simple layers consisting of:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A dataset containing the data we want to see (e.g x and y axes and data points)&lt;/li&gt;
&lt;li&gt;A geometric object (or &lt;code&gt;geom&lt;/code&gt;) that defines the form we want to see (points, lines, shapes etc.) known as a &lt;code&gt;geom&lt;/code&gt;. Multiple &lt;code&gt;geoms&lt;/code&gt; can be used to build a graphic (e.g, points and lines etc.).&lt;/li&gt;
&lt;li&gt;A coordinate system (e.g. a grid, a map etc.).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;On top of these three basic components, the grammar includes statistical transformations (or &lt;code&gt;stats&lt;/code&gt;) describing the statistics to be applied to the data to create a bar chart or trend line. The grammar also describes the use of faceting (trellising) to break a dataset down into smaller components (see Part 2).&lt;/p&gt;
&lt;p&gt;A very useful article explaining this approach is &lt;a href=&#34;http://vita.had.co.nz/papers/layered-grammar.pdf&#34;&gt;Hadley Wickham’s 2010 A Layered Grammar of Graphics&lt;/a&gt; (preprint) and is recommended reading.&lt;/p&gt;
&lt;p&gt;The power of this approach is that it allows us to build complex graphs from simple layers while being able to control each element and understand what is happening. One way to think of this is as stripping back a graph to its basic elements and allowing you to decide what each element (layer) should contain and look like. In short, you get to decide what your graphs look like.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ggplot2&lt;/code&gt; contains two main functions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;qplot (quick plot)&lt;/li&gt;
&lt;li&gt;ggplot()&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The main difference between the two is that quick plot makes assumptions for you and, as the name suggests, is used for quick plots. In contrast, with ggplot we build graphics from scratch with helpful defaults that give us full control over what we see.&lt;/p&gt;
&lt;p&gt;In this article we will start with qplot and increasingly merge into developing plots by adding layers in what could be called a ggplot kind of way. We will develop that further in the Part 2.&lt;/p&gt;
&lt;div id=&#34;getting-started-with-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting Started with R&lt;/h2&gt;
&lt;p&gt;This article assumes that you are new to using R. You do not need any knowledge of programming in R to follow this article. While you don’t need to know anything about R to follow the article, you may find it helpful to know that :&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;R is a statistical programming language. That can sound a bit intimidating. However, R can handle lots of other tasks a patent analyst might need such as cleaning and tidying data or text mining. This makes it a good choice for a patent analyst.&lt;/li&gt;
&lt;li&gt;R works using packages (libraries) for performing tasks such as importing files, manipulating files and graphics. There are around 6,819 packages and they are open source (mainly it seems under the MIT licence). If you can think of it there is probably a package that meets (or almost meets) your analysis needs.&lt;/li&gt;
&lt;li&gt;Packages contain functions that do things such as &lt;code&gt;read_csv()&lt;/code&gt; to read in a comma separated file.&lt;/li&gt;
&lt;li&gt;The functions take arguments that tell them what you want to do, such as specifying the data to graph and the x and y axis e.g. qplot(x = , y = , data = my dataset).&lt;/li&gt;
&lt;li&gt;If you want to learn more, or get stuck, there are a huge number of resources and free courses out there and RStudio lists some of the main resources on their website &lt;a href=&#34;http://www.rstudio.com/resources/training/online-learning/&#34;&gt;here&lt;/a&gt;. With R you are never alone.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;R is best learned by doing. The main trick with R is to install and load the packages that you will need and then to work with functions and their arguments. Given that most patent analysts are likely to be unfamiliar with R we will adopt the simplest approach possible to make sure it is clear what is going on at each step.&lt;/p&gt;
&lt;p&gt;The first step is to install R and RStudio desktop for your operating system by following the links and instructions &lt;a href=&#34;http://www.rstudio.com/products/rstudio/download/&#34;&gt;here&lt;/a&gt; and making sure that you follow the link to install R. Follow this very useful &lt;a href=&#34;http://www.computerworld.com/article/2497143/business-intelligence/business-intelligence-beginner-s-guide-to-r-introduction.html?page=2&#34;&gt;Computerworld article&lt;/a&gt; to become familiar with what you are seeing. You may well want to follow the rest of that article. Inside R you can learn a lot by installing the &lt;code&gt;Swirl&lt;/code&gt; package that provides interactive tutorials for learning R. Details are provided in the resources at the end of the article.&lt;/p&gt;
&lt;p&gt;The main thing you need to do to get started other than installing R and RStudio is to open RStudio and install some packages.&lt;/p&gt;
&lt;p&gt;In this article we will use four packages:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;readr&lt;/code&gt; to quickly read in the pizza patent dataset as an easy to use data table.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dplyr&lt;/code&gt; for quick addition and operations on the data to make it easier to graph.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ggplot2&lt;/code&gt; or Grammar of Graphics 2 as the tool we will use for graphing.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ggthemes&lt;/code&gt; provides very useful additional themes including Tufte range plots, the Economist and Tableau and can be accessed through &lt;a href=&#34;http://cran.r-project.org/web/packages/ggthemes/index.html&#34;&gt;CRAN&lt;/a&gt; or &lt;a href=&#34;https://github.com/jrnold/ggthemes&#34;&gt;Github&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-started&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;If you don’t have these packages already then install each of them below by pressing command and Enter at the end of each line. As an alternative select &lt;strong&gt;&lt;em&gt;Packages &amp;gt; Install&lt;/em&gt;&lt;/strong&gt; in the pane displaying a tab called Packages. Then enter the names of the packages one at a time without the quotation marks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;readr&amp;quot;)
install.packages(&amp;quot;dplyr&amp;quot;)
install.packages(&amp;quot;ggplot2&amp;quot;)
install.packages(&amp;quot;ggthemes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then make sure the packages have loaded to make them available. Press command and enter at the end of each line below (or, if you are feeling brave, select them all and then click the icon marked Run).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
library(dplyr)
library(ggplot2)
library(ggthemes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You are now good to go.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-pizza-patent-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;About the pizza patent dataset&lt;/h2&gt;
&lt;p&gt;The pizza patents dataset is a set of 9,996 patent documents from the WIPO Patentscope database that make reference somewhere in the text to the term &lt;code&gt;pizza&lt;/code&gt;. Almost everybody likes pizza and this is simply a working dataset that we can use to learn how to work with different open source tools. This will also allow us over time to refine our understanding of patent activity involving the term pizza and hone in on actual pizza related technology. In previous walkthroughs we divided the &lt;code&gt;pizza&lt;/code&gt; dataset into a set of distinct data tables to enable analysis and visualisation using Tableau Public. You can download that dataset in .csv format &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium_clean/pizza_medium_clean.zip?raw=true&#34;&gt;here&lt;/a&gt;. These data tables are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;pizza (the core set)&lt;/li&gt;
&lt;li&gt;applicants (a subdataset divided and cleaned on applicant names)&lt;/li&gt;
&lt;li&gt;inventors (a subdataset divided and cleaned on inventor names)&lt;/li&gt;
&lt;li&gt;ipc_class (a subdataset divided on ipc class names names)&lt;/li&gt;
&lt;li&gt;applicants_ipc (a child dataset of applicants listing the IPC codes)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this article we will focus on the &lt;code&gt;pizza&lt;/code&gt; table as the core set. However, you may want to experiment with other sets.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reading-in-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reading in the Data&lt;/h2&gt;
&lt;p&gt;We will use the &lt;code&gt;readr&lt;/code&gt; package to rapidly read in the pizza set to R (for other options see the in depth articles on reading in &lt;a href=&#34;&#34;&gt;.csv&lt;/a&gt; and &lt;a href=&#34;&#34;&gt;Excel&lt;/a&gt; files and the recent Getting your Data into R RStudio &lt;a href=&#34;http://www.rstudio.com/resources/webinars/&#34;&gt;webinar&lt;/a&gt;). &lt;code&gt;readr&lt;/code&gt; is nice and easy to use and creates a data table that we can easily view.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
library(dplyr)
pizza &amp;lt;- read_csv(&amp;quot;https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium_clean/pizza.csv?raw=true&amp;quot;) %&amp;gt;% 
    select(-applicants_cleaned, -applicants_cleaned_type, -applicants_original, -inventors_cleaned, 
        -inventors_original)  # drop cols with a multibyte string
head(pizza)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 26
##   applicants_organ… ipc_class ipc_codes ipc_names ipc_original ipc_subclass_co…
##   &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;           
## 1 &amp;lt;NA&amp;gt;              A21: Bak… A21D 13/… A21D 13/… A21D 13/00;… A21D; A23L      
## 2 &amp;lt;NA&amp;gt;              A21: Bak… A21B 3/13 A21B 3/1… A21B 3/13    A21B            
## 3 &amp;lt;NA&amp;gt;              A21: Bak… A21C 15/… A21C 15/… A21C 15/04   A21C            
## 4 Lazarillo De Tor… A21: Bak… A21D 13/… A21D 13/… A21D 13/00;… A21D; A23L      
## 5 &amp;lt;NA&amp;gt;              B65: Con… B65D 21/… B65D 21/… B65D 21/032… B65D            
## 6 &amp;lt;NA&amp;gt;              B65: Con… B65D 85/… B65D 85/… B65D 85/36   B65D            
## # ... with 20 more variables: ipc_subclass_detail &amp;lt;chr&amp;gt;,
## #   ipc_subclass_names &amp;lt;chr&amp;gt;, priority_country_code &amp;lt;chr&amp;gt;,
## #   priority_country_code_names &amp;lt;chr&amp;gt;, priority_data_original &amp;lt;chr&amp;gt;,
## #   priority_date &amp;lt;chr&amp;gt;, publication_country_code &amp;lt;chr&amp;gt;,
## #   publication_country_name &amp;lt;chr&amp;gt;, publication_date &amp;lt;chr&amp;gt;,
## #   publication_date_original &amp;lt;chr&amp;gt;, publication_day &amp;lt;int&amp;gt;,
## #   publication_month &amp;lt;int&amp;gt;, publication_number &amp;lt;chr&amp;gt;,
## #   publication_number_espacenet_links &amp;lt;chr&amp;gt;, publication_year &amp;lt;int&amp;gt;,
## #   title_cleaned &amp;lt;chr&amp;gt;, title_nlp_cleaned &amp;lt;chr&amp;gt;,
## #   title_nlp_multiword_phrases &amp;lt;chr&amp;gt;, title_nlp_raw &amp;lt;chr&amp;gt;,
## #   title_original &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a data table with the data. We can inspect this data in a variety of ways:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;1. View&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;See a separate table in a new tab. This is useful if you want to get a sense of the data or look for column numbers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;View(pizza)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;2. head (for the bottom use &lt;code&gt;tail&lt;/code&gt;)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;head&lt;/code&gt; allows you to see the top few rows or using &lt;code&gt;tail&lt;/code&gt; the bottom few rows.If you would like to see more rows add a number after the dataset name e.g. `head(pizza, 20).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(pizza)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 26
##   applicants_organ… ipc_class ipc_codes ipc_names ipc_original ipc_subclass_co…
##   &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;           
## 1 &amp;lt;NA&amp;gt;              A21: Bak… A21D 13/… A21D 13/… A21D 13/00;… A21D; A23L      
## 2 &amp;lt;NA&amp;gt;              A21: Bak… A21B 3/13 A21B 3/1… A21B 3/13    A21B            
## 3 &amp;lt;NA&amp;gt;              A21: Bak… A21C 15/… A21C 15/… A21C 15/04   A21C            
## 4 Lazarillo De Tor… A21: Bak… A21D 13/… A21D 13/… A21D 13/00;… A21D; A23L      
## 5 &amp;lt;NA&amp;gt;              B65: Con… B65D 21/… B65D 21/… B65D 21/032… B65D            
## 6 &amp;lt;NA&amp;gt;              B65: Con… B65D 85/… B65D 85/… B65D 85/36   B65D            
## # ... with 20 more variables: ipc_subclass_detail &amp;lt;chr&amp;gt;,
## #   ipc_subclass_names &amp;lt;chr&amp;gt;, priority_country_code &amp;lt;chr&amp;gt;,
## #   priority_country_code_names &amp;lt;chr&amp;gt;, priority_data_original &amp;lt;chr&amp;gt;,
## #   priority_date &amp;lt;chr&amp;gt;, publication_country_code &amp;lt;chr&amp;gt;,
## #   publication_country_name &amp;lt;chr&amp;gt;, publication_date &amp;lt;chr&amp;gt;,
## #   publication_date_original &amp;lt;chr&amp;gt;, publication_day &amp;lt;int&amp;gt;,
## #   publication_month &amp;lt;int&amp;gt;, publication_number &amp;lt;chr&amp;gt;,
## #   publication_number_espacenet_links &amp;lt;chr&amp;gt;, publication_year &amp;lt;int&amp;gt;,
## #   title_cleaned &amp;lt;chr&amp;gt;, title_nlp_cleaned &amp;lt;chr&amp;gt;,
## #   title_nlp_multiword_phrases &amp;lt;chr&amp;gt;, title_nlp_raw &amp;lt;chr&amp;gt;,
## #   title_original &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;3. dimensions&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This allows us to see how many rows there are (9996) and how many columns(31)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(pizza)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 9996   26&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;4. Summary&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Provides a summary of the dataset columns including quick calculations on numeric fields and the class of vector.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(pizza)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  applicants_organisations  ipc_class          ipc_codes        
##  Length:9996              Length:9996        Length:9996       
##  Class :character         Class :character   Class :character  
##  Mode  :character         Mode  :character   Mode  :character  
##                                                                
##                                                                
##                                                                
##                                                                
##   ipc_names         ipc_original       ipc_subclass_codes ipc_subclass_detail
##  Length:9996        Length:9996        Length:9996        Length:9996        
##  Class :character   Class :character   Class :character   Class :character   
##  Mode  :character   Mode  :character   Mode  :character   Mode  :character   
##                                                                              
##                                                                              
##                                                                              
##                                                                              
##  ipc_subclass_names priority_country_code priority_country_code_names
##  Length:9996        Length:9996           Length:9996                
##  Class :character   Class :character      Class :character           
##  Mode  :character   Mode  :character      Mode  :character           
##                                                                      
##                                                                      
##                                                                      
##                                                                      
##  priority_data_original priority_date      publication_country_code
##  Length:9996            Length:9996        Length:9996             
##  Class :character       Class :character   Class :character        
##  Mode  :character       Mode  :character   Mode  :character        
##                                                                    
##                                                                    
##                                                                    
##                                                                    
##  publication_country_name publication_date   publication_date_original
##  Length:9996              Length:9996        Length:9996              
##  Class :character         Class :character   Class :character         
##  Mode  :character         Mode  :character   Mode  :character         
##                                                                       
##                                                                       
##                                                                       
##                                                                       
##  publication_day publication_month publication_number
##  Min.   : 1.00   Min.   : 1.000    Length:9996       
##  1st Qu.: 8.00   1st Qu.: 4.000    Class :character  
##  Median :16.00   Median : 7.000    Mode  :character  
##  Mean   :15.68   Mean   : 6.608                      
##  3rd Qu.:23.00   3rd Qu.:10.000                      
##  Max.   :31.00   Max.   :12.000                      
##  NA&amp;#39;s   :30      NA&amp;#39;s   :30                          
##  publication_number_espacenet_links publication_year title_cleaned     
##  Length:9996                        Min.   :1940     Length:9996       
##  Class :character                   1st Qu.:1999     Class :character  
##  Mode  :character                   Median :2005     Mode  :character  
##                                     Mean   :2003                       
##                                     3rd Qu.:2009                       
##                                     Max.   :2015                       
##                                     NA&amp;#39;s   :30                         
##  title_nlp_cleaned  title_nlp_multiword_phrases title_nlp_raw     
##  Length:9996        Length:9996                 Length:9996       
##  Class :character   Class :character            Class :character  
##  Mode  :character   Mode  :character            Mode  :character  
##                                                                   
##                                                                   
##                                                                   
##                                                                   
##  title_original    
##  Length:9996       
##  Class :character  
##  Mode  :character  
##                    
##                    
##                    
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;5.The class of R object&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;class()&lt;/code&gt; is one of the most useful functions in R because it tells you what kind of object or vectors you are dealing with. R vectors are normally either character, numeric, or logical (TRUE, FALSE) but classes also include integers and factors. Most of the time patent data is of either the character type or a date.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(pizza)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;tbl_df&amp;quot;     &amp;quot;tbl&amp;quot;        &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;4. &lt;code&gt;str&lt;/code&gt; - See the structure&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As you become more familiar with R the function &lt;code&gt;str()&lt;/code&gt; becomes one of the most useful for examining the structure of your data. For example, using str we can see whether an object we are working with is a simple vector, a list of objects or a list that contains a set of data frames (e.g.) tables. If things don’t seem to be working then &lt;code&gt;class&lt;/code&gt; and &lt;code&gt;str&lt;/code&gt; will often help you to understand why not.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(pizza, max.level = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These options illustrate the range of ways that you can view the data before and during graphing. Mainly what will be needed is the column names but we also need to think about the column types.&lt;/p&gt;
&lt;p&gt;If we inspect this data using &lt;code&gt;str(pizza)&lt;/code&gt; we will see that the bulk of the fields are character fields. One feature of patent data is that it rarely includes actual numeric fields (such as counts). Most fields are character fields such as names or alphanumeric values (such as publication numbers e.g. US20151234A1). Sometimes we see counts such as citing documents or family members but most of the time our fields are character fields or dates. A second common feature of patent data is that some fields are concatenated. That is the cells in a column contain more than one value (e.g. multiple inventor or applicant names etc.).&lt;/p&gt;
&lt;p&gt;We will walk through how to deal with these common patent data issues in R in other articles. For now, we don’t need to worry about the form of data except that it is normally best to select a column (variable) that is not concatenated with multiple values to develop our counts. So as a first step we will quickly create a numeric field from the &lt;code&gt;publication_number&lt;/code&gt; field in &lt;code&gt;pizza&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-a-numeric-field&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating a numeric field&lt;/h2&gt;
&lt;p&gt;To create a numeric field for graphing we will need to do two things&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;add a column&lt;/li&gt;
&lt;li&gt;assign each cell in that column a value that we can then count.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The most obvious field to use as the basis for counting in the pizza data is the &lt;code&gt;publication_number&lt;/code&gt; field because typically this contains unique alphanumeric identifiers.&lt;/p&gt;
&lt;p&gt;To create a numeric field we will use the &lt;code&gt;dplyr&lt;/code&gt; package. &lt;code&gt;dplyr&lt;/code&gt; and its sister package &lt;code&gt;tidyr&lt;/code&gt; are some of the most useful packages available for working in R and come with a handy &lt;a href=&#34;http://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf&#34;&gt;RStudio Cheatsheet&lt;/a&gt; and &lt;a href=&#34;http://www.rstudio.com/resources/webinars/archives/#&#34;&gt;webinar&lt;/a&gt;. To see what the functions in &lt;code&gt;dplyr&lt;/code&gt; are then click on its name in the packages pane.&lt;/p&gt;
&lt;p&gt;Just for future reference the main functions are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;filter (to select rows in a data)&lt;/li&gt;
&lt;li&gt;select (to select the columns you want to work with)&lt;/li&gt;
&lt;li&gt;mutate (to add columns based on other columns)&lt;/li&gt;
&lt;li&gt;arrange (to sort)&lt;/li&gt;
&lt;li&gt;group_by( to group data)&lt;/li&gt;
&lt;li&gt;count (to easily summarise data on a value)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;dplyr&#39;s&lt;/code&gt; &lt;code&gt;mutate&lt;/code&gt; function allows us to add a new column based on the values contained in one or more of the other columns in the dataset. We will call this new variable &lt;code&gt;n&lt;/code&gt; and we could always rename it in the graphs later on. There are quite a variety of ways of creating counts in R but this is one of the easiest. The mutate function is really very useful and worth learning.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
pizza &amp;lt;- mutate(pizza, record_count = sum(publication_number = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What we have done here is to tell R that we want to use the &lt;code&gt;mutate()&lt;/code&gt; function. We have then passed it a series of arguments consisting of:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;our dataset = pizza&lt;/li&gt;
&lt;li&gt;record_count = the result of the function sum() which is the sum of publication_number giving the value 1 to each number.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pizza &amp;lt;-&lt;/code&gt; this tells R to create an object (a data frame) called &lt;code&gt;pizza&lt;/code&gt; containing the results. If you take a look in the Environment pane you will now see that pizza has 32 variables. Note that we have now modified the data we imported into R although the original data in the file remains the same.
If we now use &lt;code&gt;View(pizza)&lt;/code&gt; we will see a new column called &lt;code&gt;record_count&lt;/code&gt; with a value of 1 for each entry.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;renaming-columns&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Renaming Columns&lt;/h3&gt;
&lt;p&gt;We will be doing quite a lot of work with the &lt;code&gt;publication_country_name&lt;/code&gt; field, so let’s make our lives a bit easier by renaming it with the &lt;code&gt;dplyr&lt;/code&gt; function &lt;code&gt;rename()&lt;/code&gt;. We will also do the same for the &lt;code&gt;publication_country_code&lt;/code&gt; and publication_year. Note that it is easy to create labels for graphs with ggplot so we don’t need to worry about renaming column names too much. We can rename them again later if saving the file to a new &lt;code&gt;.csv&lt;/code&gt; file.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
pizza &amp;lt;- rename(pizza, pubcountry = publication_country_name, pubcode = publication_country_code, 
    pubyear = publication_year)
head(pizza)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 27
##   applicants_organ… ipc_class ipc_codes ipc_names ipc_original ipc_subclass_co…
##   &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;           
## 1 &amp;lt;NA&amp;gt;              A21: Bak… A21D 13/… A21D 13/… A21D 13/00;… A21D; A23L      
## 2 &amp;lt;NA&amp;gt;              A21: Bak… A21B 3/13 A21B 3/1… A21B 3/13    A21B            
## 3 &amp;lt;NA&amp;gt;              A21: Bak… A21C 15/… A21C 15/… A21C 15/04   A21C            
## 4 Lazarillo De Tor… A21: Bak… A21D 13/… A21D 13/… A21D 13/00;… A21D; A23L      
## 5 &amp;lt;NA&amp;gt;              B65: Con… B65D 21/… B65D 21/… B65D 21/032… B65D            
## 6 &amp;lt;NA&amp;gt;              B65: Con… B65D 85/… B65D 85/… B65D 85/36   B65D            
## # ... with 21 more variables: ipc_subclass_detail &amp;lt;chr&amp;gt;,
## #   ipc_subclass_names &amp;lt;chr&amp;gt;, priority_country_code &amp;lt;chr&amp;gt;,
## #   priority_country_code_names &amp;lt;chr&amp;gt;, priority_data_original &amp;lt;chr&amp;gt;,
## #   priority_date &amp;lt;chr&amp;gt;, pubcode &amp;lt;chr&amp;gt;, pubcountry &amp;lt;chr&amp;gt;,
## #   publication_date &amp;lt;chr&amp;gt;, publication_date_original &amp;lt;chr&amp;gt;,
## #   publication_day &amp;lt;int&amp;gt;, publication_month &amp;lt;int&amp;gt;, publication_number &amp;lt;chr&amp;gt;,
## #   publication_number_espacenet_links &amp;lt;chr&amp;gt;, pubyear &amp;lt;int&amp;gt;,
## #   title_cleaned &amp;lt;chr&amp;gt;, title_nlp_cleaned &amp;lt;chr&amp;gt;,
## #   title_nlp_multiword_phrases &amp;lt;chr&amp;gt;, title_nlp_raw &amp;lt;chr&amp;gt;,
## #   title_original &amp;lt;chr&amp;gt;, record_count &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;selecting-columns-for-plotting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Selecting Columns for plotting&lt;/h3&gt;
&lt;p&gt;We could now simply go ahead and work with pizza. However, for datasets with many columns or requiring different kinds of counts it can be much easier to simply select the columns we want to work with to reduce clutter. We can use the &lt;code&gt;select()&lt;/code&gt; function from &lt;code&gt;dplyr&lt;/code&gt; to do this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
p1 &amp;lt;- pizza %&amp;gt;% select(., pubcountry, pubcode, pubyear, record_count)
head(p1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##   pubcountry                 pubcode pubyear record_count
##   &amp;lt;chr&amp;gt;                      &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;        &amp;lt;dbl&amp;gt;
## 1 United States of America   US         2009           1.
## 2 United States of America   US         2014           1.
## 3 United States of America   US         2013           1.
## 4 European Patent Office     EP         2007           1.
## 5 United States of America   US         2003           1.
## 6 Patent Co-operation Treaty WO         2002           1.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that &lt;code&gt;dplyr&lt;/code&gt; will exclude columns that are not mentioned when using select. This is one of the purposes of &lt;code&gt;select&lt;/code&gt; as a function. For that reason you will probably want to rename the object (in this case as p1). If we used the name &lt;code&gt;pizza&lt;/code&gt; for the object our original table would be reduced to the 4 columns specified by &lt;code&gt;select&lt;/code&gt;. Type &lt;code&gt;?select&lt;/code&gt; in the console for further details.&lt;/p&gt;
&lt;p&gt;We now have a data frame with 9,996 rows and 4 variables (columns). Use &lt;code&gt;View(p1)&lt;/code&gt; or simply enter &lt;code&gt;p1&lt;/code&gt; into the console to take a look.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-counts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating Counts&lt;/h2&gt;
&lt;p&gt;To make life even easier for ourselves we can use function &lt;code&gt;count()&lt;/code&gt; from &lt;code&gt;dplyr&lt;/code&gt; to group the data onto counts by different variables for graphing. Note that we could defer counting until later, however, this is a good opportunity to learn more about &lt;code&gt;dplyr&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s go ahead and construct some counts using &lt;code&gt;p1&lt;/code&gt;. At the same time we will use quick plot (&lt;code&gt;qplot&lt;/code&gt;) for some exploratory plotting of the results. In the course of this R will show error warnings in red for missing values. We will be ignoring the warning because they are often R telling us things it things we need to know.&lt;/p&gt;
&lt;div id=&#34;total-by-year&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Total by Year&lt;/h3&gt;
&lt;p&gt;What if we wanted to know the overall total for our sample data by publication year. Try the following.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pt &amp;lt;- count(p1, pubyear, wt = record_count)
head(pt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   pubyear     n
##     &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
## 1    1940    1.
## 2    1954    1.
## 3    1956    1.
## 4    1957    1.
## 5    1959    1.
## 6    1962    1.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we now view &lt;code&gt;pt&lt;/code&gt; (either by using &lt;code&gt;View(pt)&lt;/code&gt;, noting the capital V, or clicking &lt;code&gt;pt&lt;/code&gt; in the Environment pane) we will see that R has dropped the country columns to present us with an overall total by year in &lt;code&gt;n&lt;/code&gt;. We now have a general overview of the data for graphing.&lt;/p&gt;
&lt;p&gt;Let’s go ahead and quickly plot that using the &lt;code&gt;qplot()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(x = pubyear, y = n, data = pt, geom = &amp;quot;line&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2015-06-25-wrangling-pizza-patents-in-R_files/figure-html/fig1_ggplot1-1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Round Up&lt;/h3&gt;
&lt;p&gt;That’s it. You may feel at the end of this post that this was a lot of work to get to a very simple graph. But, in reality, it is the data preparation that takes the time. In the next post we will focus in on creating different kinds of graph in ggplot2 and some of the challenges that we encounter along the way.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Accessing Patent Data with WIPO Patentscope</title>
      <link>/patentscope/</link>
      <pubDate>Mon, 25 May 2015 00:00:00 +0000</pubDate>
      
      <guid>/patentscope/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://patentscope.wipo.int/search/en/search.jsf&#34;&gt;Patentscope&lt;/a&gt; is the WIPO public access database. It includes coverage of the Patent Cooperation Treaty applications (administered by WIPO) and a &lt;a href=&#34;https://patentscope.wipo.int/search/en/help/data_coverage.jsf&#34;&gt;wide range of other countries&lt;/a&gt; including the European Patent Office, USPTO and Japan totalling 51 million patent documents including 2.8 million PCT applications.&lt;/p&gt;
&lt;p&gt;This article is now a chapter in the &lt;a href=&#34;https://wipo-analytics.github.io/&#34;&gt;WIPO Manual on Open Source Patent Analytics&lt;/a&gt;. You can read the chapter in electronic book format &lt;a href=&#34;https://wipo-analytics.github.io/patentscope-1.html&#34;&gt;here&lt;/a&gt; and find all the materials including presentations at the &lt;a href=&#34;https://github.com/wipo-analytics&#34;&gt;WIPO Analytics Github homepage&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this article we cover the basics of using Patentscope to search for and download up to 10,000 records. A detailed &lt;a href=&#34;http://www.wipo.int/edocs/pubdocs/en/patents/434/wipo_pub_l434_08.pdf&#34;&gt;User’s Guide&lt;/a&gt; provides more details on specific features. A set of &lt;a href=&#34;https://patentscope.wipo.int/search/en/tutorial.jsf&#34;&gt;video tutorials&lt;/a&gt; are also available. When compared with other free services Patentscope has the following main strengths.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Full text search in the description and claims of PCT applications on the day of publication and patent applications from a wide range of other countries including the United States, Japan, China and the European Patent Office among others.&lt;/li&gt;
&lt;li&gt;Download up to 10,000 records&lt;/li&gt;
&lt;li&gt;Expand search terms into multiple other languages using &lt;code&gt;Cross Lingual Expansion&lt;/code&gt; or &lt;a href=&#34;https://patentscope.wipo.int/search/en/clir/clir.jsf?new=true&#34;&gt;CLIR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Simple, Advanced and Combined Field searching&lt;/li&gt;
&lt;li&gt;Accessible in multiple languages and a &lt;a href=&#34;https://www3.wipo.int/patentscope/translate/translate.jsf?interfaceLanguage=en&#34;&gt;WIPO Translate&lt;/a&gt; text function&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://patentscope.wipo.int/search/mobile/index.jsf&#34;&gt;Mobile version&lt;/a&gt; and &lt;a href=&#34;http://www.wipo.int/patentscope/en/news/pctdb/2015/news_0002.html&#34;&gt;https:&lt;/a&gt; access&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://patentscope.wipo.int/search/en/sequences.jsf&#34;&gt;Sequence listing downloads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Green technologies through the &lt;a href=&#34;http://www.wipo.int/classifications/ipc/en/est/&#34;&gt;IPC Green Inventory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Different types of graphical analysis of results lists on the fly using the Options menu.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To get the most out of Patentscope it is a good idea to consult the two detailed guides and the video tutorials:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;http://www.wipo.int/edocs/pubdocs/en/patents/434/wipo_pub_l434_08.pdf&#34;&gt;Patentscope Search: The User’s Guide&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Patentscope CLIR for the Cross-Lingual Information Retrieval Tool &lt;a href=&#34;https://patentscope.wipo.int/search/help/en/CLIR_DOC.pdf&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://patentscope.wipo.int/search/en/tutorial.jsf&#34;&gt;Patentscope video tutorials&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you would like to download patent or sequence data you will need to register for a free account. To register for a free account go &lt;a href=&#34;https://patentscope.wipo.int/search/en/reg/registration.jsf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;collections-to-search&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Collections to Search&lt;/h2&gt;
&lt;p&gt;Perhaps the best place to start is with the collections we will be searching. Those can be accessed under the Options menu on the main menu and then the tab reading &lt;a href=&#34;https://patentscope.wipo.int/search/en/reg/registration.jsf&#34;&gt;office&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/patentscope/collections.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see here that Patentscope provides access to the Patent Cooperation Treaty collection, regional collections such as the ARIPO and the European Patent Office and national collections such as the United States, Japan and Others. The ability to search and retrieve data from the LATIPAT collection will be particularly useful for researchers in Latin America and could be linked to analysis using the espacenet version of &lt;a href=&#34;http://lp.espacenet.com&#34;&gt;LATIPAT&lt;/a&gt;. If you are only interested in particular collections, this is the place to change the settings.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simple-search&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simple Search&lt;/h2&gt;
&lt;p&gt;We can select a range of different fields for search. In this case we have selected full text from the drop down menu for a simple search on the term pizza.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/patentscope/simplesearch.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that Patentscope groups documents for the same application into a record or dossier and that we are seeing the document that is the key for the record. The other documents in the dossier for the record can be accessed by clicking the document number and selecting the Documents menu as in this &lt;a href=&#34;https://patentscope.wipo.int/search/en/detail.jsf?docId=WO2014047700&amp;amp;recNum=2&amp;amp;tab=PCTDocuments&amp;amp;maxRec=25177&amp;amp;office=&amp;amp;prevFilter=&amp;amp;sortOption=Relevance&amp;amp;queryString=ALLTXT%3A%28pizza%29&#34;&gt;example&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For more details on using simple search see the &lt;code&gt;Simple Search&lt;/code&gt; &lt;a href=&#34;https://patentscope.wipo.int/search/en/tutorial.jsf&#34;&gt;video tutorial&lt;/a&gt;. Videos are also available on the use of Field Combinations for constructing searches and Advanced Search.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;When we arrive at the results page we can see that we have 24,614 results with our query displaying as searching &lt;code&gt;AllTXT&lt;/code&gt; and all languages. We then have an RSS button to copy the feed over to an RSS feeder for updates.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/patentscope/patentscopsesimple_pizza.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is also a query tree button that displays results by language and terms in the relevant sections of the document. We can see an example of this for a more complex query below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/patentscope/query_tree.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A video tutorial is also available for the &lt;a href=&#34;https://patentscope.wipo.int/search/en/tutorial.jsf&#34;&gt;Search result list&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;downloading-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Downloading Results&lt;/h2&gt;
&lt;p&gt;The two excel icons at the end of the menu allow a user to download either the short list (first icon) or the second list as a &lt;code&gt;.xls&lt;/code&gt; file. To see these icons you must be logged in with a user account or they will not display.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/patentscope/pizzaexporting.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When we download these results we will receive an &lt;code&gt;.xls&lt;/code&gt; sheet with up to 10,000 entries with a couple of header rows that show the query. Note that each record in the Excel sheet is hyperlinked to the corresponding record in Patentscope.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/patentscope/results.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will go into the use of this data, including with Tableau Public and other tools, in some depth and there are a few things to note here. The first is that the hyperlinked publication number does not possess a kind code (A1, B1 etc.). This only matters in the sense that the number will retrieve multiple documents in other databases linked to the Patentscope number. A second point to note is that Patentscope data is &lt;code&gt;raw&lt;/code&gt; in the sense that it is data as it comes from the data providers and is not processed. That means that there can be encoding issues that we will come back to later on in the discussions on data cleaning.&lt;/p&gt;
&lt;p&gt;What is very useful about Patentscope is that we can actually obtain quite a significant volume of data on a topic of interest. While this article has simply downloaded the first 10,000 results, to obtain the full result set it would be easy enough to limit the data by year and download the data as a series of sets that can be combined later (e.g. three sets).&lt;/p&gt;
&lt;p&gt;To do this we need to visit the Field Combination Page. Here we will start by putting our query in English All to gain the total number of results. Then we will restrict the data by the publication data field using &lt;code&gt;[]&lt;/code&gt; and period between dates (as DD.MM.YYYY). An example is shown below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/wipo_settings.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This will helpfully show the total results for the query (although it can take some time) and we can run and then download results for each year limited segment.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/wipo_results.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When working with multiple downloads it is a good idea to write down the total number of results and then the results for each date limited segment to ensure that the data adds up to what you would expect. Some experimentation may also be needed with the field settings using the Boolean AND/OR operators.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-lingual-searching&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cross Lingual Searching&lt;/h2&gt;
&lt;p&gt;One challenge in patent searching is the use of different expressions in different languages for the same query. Patentscope presents a very useful solution to this through cross-lingual searching. From the pull down menu select &lt;code&gt;Cross Lingual Expansion&lt;/code&gt;, then enter the search terms and press go. The tool will now generate search terms in multiple languages.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/patentscope/pizzacrosslingual.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To go further with this tool use either the slider settings (precision vs. recall). For example, if we were to insert the search term “synthetic biology” and move recall to the top level (4), we would generate the following query.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;&amp;quot;FP:((EN_TI:(&amp;quot;synthetic biology&amp;quot; OR &amp;quot;biologic synthetic&amp;quot;) OR EN_AB:(&amp;quot;synthetic biology&amp;quot; OR &amp;quot;biologic synthetic&amp;quot;)) OR (DE_TI:(&amp;quot;synthetische Biologie&amp;quot; OR &amp;quot;synthetischen biologischen&amp;quot; OR &amp;quot;biologische synthetische&amp;quot; OR &amp;quot;Biologische synthetische&amp;quot;) OR DE_AB:(&amp;quot;synthetische Biologie&amp;quot; OR &amp;quot;synthetischen biologischen&amp;quot; OR &amp;quot;biologische synthetische&amp;quot; OR &amp;quot;Biologische synthetische&amp;quot;)) OR (ES_TI:(&amp;quot;biológicas sintéticas&amp;quot;) OR ES_AB:(&amp;quot;biológicas sintéticas&amp;quot;)) OR (FR_TI:(&amp;quot;biologie synthétique&amp;quot; OR &amp;quot;biologie synthéthique&amp;quot;) OR FR_AB:(&amp;quot;biologie synthétique&amp;quot; OR &amp;quot;biologie synthéthique&amp;quot;)) OR (JA_TI:(&amp;quot;生物合成&amp;quot; OR &amp;quot;合成生体&amp;quot; OR &amp;quot;の生物学的合成&amp;quot;) OR JA_AB:(&amp;quot;生物合成&amp;quot; OR &amp;quot;合成生体&amp;quot; OR &amp;quot;の生物学的合成&amp;quot;)) OR (ZH_TI:(&amp;quot;合成生物&amp;quot;) OR ZH_AB:(&amp;quot;合成生物&amp;quot;)))&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If supervised mode is selected from the &lt;code&gt;Expansion mode&lt;/code&gt; drop down, it becomes possible to select technology areas for the generation of terminology. While we haven’t worked through this in detail that could be very helpful for domain specific query generation. All in all, this is one of the most original and powerful tools that Patentscope has to offer. A detailed &lt;code&gt;.pdf&lt;/code&gt; guide to using CLIR is available &lt;a href=&#34;https://patentscope.wipo.int/search/help/en/CLIR_DOC.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sequence-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sequence Data&lt;/h2&gt;
&lt;p&gt;A third major feature of Patentscope is access to DNA and amino acid sequence listings filed with PCT Applications. This data can be accessed and downloaded for individual records &lt;a href=&#34;https://patentscope.wipo.int/search/en/sequences.jsf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/patentscope/sequencesearching.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A sample record from the lists can be seen below as a plain text file. Note that some issues may arise with reconciling the plain text file with the WIPO publication number (WO etc.) and this merits careful attention if using this data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/patentscope/pctseq.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Registered account holders can also use the &lt;code&gt;ftp anonymous download&lt;/code&gt; service from the same page. This provides access to the sequence data by year as can be seen below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/patentscope/sequenceftp.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If using the anonymous ftp service note that the recent data is measured in gigabytes, so do not try to download this data over a weak WIFI connection, a gated connection or to your phone(!). Nevertheless, the open accessibility of this data is important. For other sequence data sources you may be interested in the European Bioinformatics Institute resources &lt;a href=&#34;http://www.ebi.ac.uk/patentdata&#34;&gt;here&lt;/a&gt; and for the US by document number &lt;a href=&#34;http://seqdata.uspto.gov/&#34;&gt;here&lt;/a&gt; and until March 2015 at the DNA Patent Database &lt;a href=&#34;https://dnapatents.georgetown.edu/&#34;&gt;here&lt;/a&gt;. Also important is the Lens &lt;code&gt;Patseq&lt;/code&gt; tool &lt;a href=&#34;https://www.lens.org/lens/bio/sequence&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round Up&lt;/h2&gt;
&lt;p&gt;WIPO Patentscope is a powerful tool for gaining access to a significant amount of patent data on a topic of interest. The ability to download 10,000 or more records at a time cannot be beaten by other free tools. The &lt;code&gt;Cross Lingual Searching&lt;/code&gt; tool appears to be unique and valuable. Free access to bulk download of sequence data is likely to keep bioinformaticians happy for quite a long time.&lt;/p&gt;
&lt;p&gt;One way of thinking about the role of Patentscope in patent analytics is as a resource that can be combined with other data tools. For example, if we wanted to obtain the abstracts, descriptions or claims of PCT documents in Patentscope then we might use the Patenscope numbers to retrieve data from EPO Open Patent Services or Google Patents using R or Python or other tools. That is, in this case Patentscope overcomes the limitations of search results from other tools but allows for the targeted use of other tools to retrieve more information. The &lt;code&gt;Cross Lingual Searching&lt;/code&gt; tool could also be particularly useful for trying to identify and later acquire patent documents from other jurisdictions where a company or organisation may be seeking to operate or to expand patent landscape analysis into jurisdictions with non-Roman alphabets.&lt;/p&gt;
&lt;p&gt;The main difficulties that arise from using Patentscope can stem from occasional noise in the data. Patenscope does not clean the data provided from the individual collections with the exception of checking for typological errors in priority numbers and IPC codes. In addition, all text is transformed into UTF-8. However, as is common when dealing with diverse data sources, the results are not always perfect. In addition, because Patentscope data is drawn from a wide range of languages users may need to update their font libraries if large numbers of unusual characters appear in the data (such as installing the Asian language pack for Windows). In practice, as is common with most patent data sources, this can mean significant time is required to clean up the data. Having said this, no other free database tool allows us to download as much data in table form for analysis. As we will see, it is possible to do a lot with Patentscope data.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Network Visualisation with Gephi</title>
      <link>/gephi_patent_network/</link>
      <pubDate>Sun, 17 May 2015 00:00:00 +0000</pubDate>
      
      <guid>/gephi_patent_network/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;!---update dataset links---&gt;
&lt;p&gt;This article focuses on visualising patent data in networks using the open source software &lt;a href=&#34;http://gephi.github.io&#34;&gt;Gephi&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Gephi is one of a growing number of free network analysis and visualisation tools with others including &lt;a href=&#34;http://www.cytoscape.org&#34;&gt;Cytoscape&lt;/a&gt;, &lt;a href=&#34;http://tulip.labri.fr/TulipDrupal/&#34;&gt;Tulip&lt;/a&gt;, &lt;a href=&#34;http://www.graphviz.org&#34;&gt;GraphViz&lt;/a&gt;, &lt;a href=&#34;http://mrvar.fdv.uni-lj.si/pajek/&#34;&gt;Pajek&lt;/a&gt; for Windows, and &lt;a href=&#34;http://www.vosviewer.com/Home&#34;&gt;VOSviewer&lt;/a&gt; to name but a few. In addition, network visualisation packages are available for R and Python. We have chosen to focus on Gephi because it is a good all round network visualisation tool that is quite easy to use and to learn.&lt;/p&gt;
&lt;p&gt;This article is now a chapter in the WIPO Manual on Open Source Patent Analytics. You can read the chapter in electronic book format &lt;a href=&#34;https://wipo-analytics.github.io/gephi-1.html&#34;&gt;here&lt;/a&gt; and find all the materials including presentations at the &lt;a href=&#34;https://github.com/wipo-analytics&#34;&gt;WIPO Analytics Github homepage&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/gephi_front.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this article we will focus on creating a simple network visualisation of the relationship between patent applicants (assignees). However, network visualisation can be used to visualise a range of fields and relationships, such as inventors, key words, IPC and CPC codes, and citations among other options.&lt;/p&gt;
&lt;p&gt;For this chapter we will use a dataset on drones from the &lt;a href=&#34;https://www.lens.org/lens/search?q=%22drone%22+OR+%22drones%22&amp;amp;predicate=%26%26&amp;amp;l=en&#34;&gt;Lens patent database&lt;/a&gt;. The dataset consists of 5884 patent documents containing the terms “drone or drones” in the full text deduplicated to individual families from the full publication set. The dataset has been extensively cleaned in Vantage Point by separating out applicant and inventor names and then using fuzzy logic matching to clean up names. Very very similar results can be achieved using Open Refine as described in Chapter 9 of this Manual.&lt;/p&gt;
&lt;p&gt;The dataset can be downloaded from Github in a zip file to unzip &lt;a href=&#34;https://github.com/wipo-analytics/drones_data/raw/master/use_me/gephi/gephi_drones_fulltext_cleaned_5884.csv.zip&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-gephi&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Installing Gephi&lt;/h2&gt;
&lt;p&gt;You should install gephi 9.1 (the latest release) rather than an earlier version. Note that any later updates may not contain the key functionality that is needed below (as it takes a while for some of the plugins and features to catch up).&lt;/p&gt;
&lt;p&gt;To install for your operating system follow these &lt;a href=&#34;https://gephi.org/users/download/&#34;&gt;instructions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;After you have finished this chapter you may want to follow the &lt;a href=&#34;https://gephi.github.io/tutorials/gephi-tutorial-quick_start.pdf&#34;&gt;Quick start guide&lt;/a&gt; although we will cover those topics in the article. The &lt;a href=&#34;http://gephi.github.io/users/&#34;&gt;Learn section&lt;/a&gt; of the website provides additional tutorials.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;opening-gephi-and-installing-plugins&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Opening Gephi and Installing Plugins&lt;/h2&gt;
&lt;p&gt;When you have installed Gephi, open it and you should see the following welcome screen.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/welcome.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Before we do anything else, we need to install a plugin developed by &lt;a href=&#34;http://www.em-lyon.com/en/faculty-research-education/faculty-research/international-business-school-professors/Permanent-Professors/Clement-LEVALLOIS&#34;&gt;Clement Levallois&lt;/a&gt; to convert Excel and csv files into gephi network files. To install the plugin select the &lt;code&gt;Tools&lt;/code&gt; menu in the menu bar and then &lt;code&gt;Plugins&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/plugin.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You will see a pop up menu for the plugins. At this point you may want to press &lt;code&gt;Reload Catalog&lt;/code&gt; to make sure everything is loaded. Then head to &lt;code&gt;Available Plugins&lt;/code&gt;. Click on &lt;code&gt;name&lt;/code&gt; to sort them alphabetically. You now want to look for a plugin called &lt;code&gt;Convert Excel and csv files to networks&lt;/code&gt;. Select the check box, press &lt;code&gt;Install&lt;/code&gt; and follow through the menus. Just keep pressing at the prompts and then you will need to restart at the end.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/install.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You will need to restart Gephi for it to take effect but if you return to the Plugins menu and then choose the installed tab you should see this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/result.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You are good to go. While you are there you may want to check out the other plugins to get an idea of what is available. For more on the conversion plugin see this description &lt;a href=&#34;https://marketplace.gephi.org/plugin/excel-csv-converter-to-network/&#34;&gt;Excel/csv converter to network plugin&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;importing-a-file-to-gephi-with-the-converter-plugin&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Importing a file to Gephi with the converter plugin&lt;/h2&gt;
&lt;p&gt;We will concentrate on using the &lt;code&gt;drones&lt;/code&gt; patent dataset in the zipped .csv version &lt;a href=&#34;https://github.com/wipo-analytics/drones_data/raw/master/use_me/gephi/gephi_drones_fulltext_cleaned_5884.csv.zip&#34;&gt;here&lt;/a&gt; and don’t forget to unzip the file. While Gephi works with .csv files, the import plugin includes a timeline option that only works with Excel. For that reason we will use the Excel version.&lt;/p&gt;
&lt;div id=&#34;step-1.-open-gephi-and-choose-file-import&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 1. Open Gephi and Choose File &amp;gt; Import&lt;/h3&gt;
&lt;p&gt;For this to work we need to use the &lt;code&gt;Import&lt;/code&gt; function under the File menu.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/import.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You should now see a menu like that below. Make sure that you choose the co-occurrence option.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/wizard.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next you will be asked to select the file to use. We will download and then unzip the &lt;a href=&#34;https://github.com/wipo-analytics/drones_data/raw/master/use_me/gephi/gephi_drones_fulltext_cleaned_5884.csv.zip&#34;&gt;gephi_drones_fulltext_cleaned_5884.csv&lt;/a&gt; file that is located on the WIPO Analytics website on Git Hub.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/select.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When you have chosen &lt;code&gt;Data importer(co-occurrences)&lt;/code&gt; then choose &lt;code&gt;Next&lt;/code&gt;. Make sure the column headers stays selected (unless using your own data). You will then need to choose a delimiter. In this case it is a comma but in other cases it may be a semicolon or a tab.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/figures_gephi/fig5_idelimiter.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We now need to choose the agents, that is the actors or objects that we want to create a network map with. We will use &lt;code&gt;patent_assignees_cleaned&lt;/code&gt; as this is a relatively small set. We will choose the same field in the two boxes because we are interested in co-occurrence analysis.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/applicants.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the next step we need to specify the delimiter for splitting the contents of the &lt;code&gt;applicants_use_me&lt;/code&gt; column. In all the fields it is a semicolon so let’s choose that. Note that if you are doing this with raw Lens data that you have not previously cleaned the Lens delimited is a double semi-colon (which is not helpful) and will need to be replaced prior to import.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/delimiter.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will then be asked if we want a dynamic network. This presently only works with Excel files and even then it does not always work well. We will leave this blank as we are using a .csv file. Note that if we were using an Excel file the choices we would use would normally be the publication year or publication date or the priority year or date for patent data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/dynamic.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The next menu provides us with a list of options. Unfortunately, with one exception, it is not entirely clear what the consequences of these choices are so experimentation may be needed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/figures_gephi/fig8_options.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Choice1. Create links between &lt;code&gt;applicants_use_me&lt;/code&gt;.
Choice 2. Remove duplicates. We don’t need that as we know that they are unique.
Choice 3. Remove self-loops. Generally we do want this (otherwise known as removing the diagonal to prevent actors counting onto themselves - this will produce a large black hoop or handle for a self-loop in Gephi).&lt;/p&gt;
&lt;p&gt;We will choose to create the links and to remove the self loops.&lt;/p&gt;
&lt;p&gt;Next we will see a create network screen setting out our choices.
&lt;img src=&#34;/images/gephi9/create.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Press Finish&lt;/p&gt;
&lt;p&gt;Next we will see an import screen.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/warning.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is quite common to see warning messages on this screen.&lt;/p&gt;
&lt;p&gt;In this case some of the applicants cells in the worksheet are blank because no data is available. When you see warning messages it is a good idea to check the underlying file to make sure that you understand the nature of the warning.&lt;/p&gt;
&lt;p&gt;A second common warning with dynamic networks is that the year field is not correctly formatted. In that case, check that the format of the date/year field is what gephi is expecting in the underlying data. You can review the data in the data laboratory.&lt;/p&gt;
&lt;p&gt;Note that the import screen also provides options on the type of graph. Normally for networks of authors, inventors and actors leave this as an undirected (unordered) network. Undirected is the basic default for patent data and scientific literature. We will also see the number of nodes (dots) and edges (connections). It is important to keep an eye on these values. If the nodes are much lower than you expect then it is useful to go back to your data and inspect for problems such as concatenation of cells and so on.&lt;/p&gt;
&lt;p&gt;Click OK. You should now see a raw network that looks like this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/network.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that we can see the number of Nodes and Edges in the top right. If we switch to the top left, we will see three tabs, for &lt;code&gt;Overview&lt;/code&gt;, &lt;code&gt;Data Laboratory&lt;/code&gt; and &lt;code&gt;Preview&lt;/code&gt;. Choose &lt;code&gt;Data Laboratory&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In the Data laboratory we can see the ID, Label, type of field and the frequency (the count of the number of times the name appears). Note that these fields are editable by clicking inside the entry and can also be grouped (for example where a variant of the same name has been missed during the name cleaning process in a tool such as Open Refine).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/laboratory.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In some cases you may have filled any blank cells in the dataset with NA (for Not Available). If this is the case NA will show up as a node on the network. You can address this type of issue in the Data Laboratory by right clicking on the NA value and then Delete. Note also that you can always exclude or combine nodes after you have laid out the network by editing in the Data Laboratory.&lt;/p&gt;
&lt;p&gt;The second part of the Data Laboratory is the Edges under Data Table in the Data Laboratory. The edges table involves a source and a target, where the source is the source node and the target is another node where there is a link between the nodes. We can see the edges table sorted alphabetically (click the source heading to sort) where the value in weight is the number of shared records.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/edges.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again, note that it is possible to export the edges set and import a set. Also note the menus at the bottom of the screen which allow column values to be copied over. This can be useful where the label value is not populated meaning that a name will not display on the node when the graph is laid out.&lt;/p&gt;
&lt;p&gt;Most of the time we can simply proceed with laying out the network without paying much attention to the data laboratory. However, it is important to become familiar with the data laboratory to avoid unexpected problems or to spot corruption in the data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sizing-and-colouring-nodes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sizing and Colouring Nodes&lt;/h2&gt;
&lt;p&gt;When we look at the &lt;code&gt;Overview&lt;/code&gt; screen we have quite a wide range of options. We will start on the upper right with the Statistics panel.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/statistics.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Run&lt;/code&gt; buttons will calculate a range of statistics on the network. Probably the two most useful are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Modularity Class. This algorithm iterates over the connections (edges) and allocates the nodes to communities or clusters based on the strength of the connections. This algorithm is explained in detail in this article &lt;a href=&#34;http://arxiv.org/abs/0803.0476&#34;&gt;Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, Etienne Lefebvre, Fast unfolding of communities in large networks, in Journal of Statistical Mechanics: Theory and Experiment 2008 (10), P1000&lt;/a&gt;. The ability to detect communities in networks based on the strength of connections is a powerful tool in patent analytics.&lt;/li&gt;
&lt;li&gt;Network Diameter. This calculates two measures of &lt;code&gt;betweeness&lt;/code&gt;, that is &lt;code&gt;betweeness centrality&lt;/code&gt; (how often a node appears on the shortest path between nodes) and centrality(the average distance from a starting node to other nodes in the network). Network Diameter also calculates Eccentricity which is the distance between a given node and the farthest node from it in the network. For background on this see the Wikipedia entry and also &lt;a href=&#34;http://www.inf.uni-konstanz.de/algo/publications/b-fabc-01.pdf&#34;&gt;Ulrik Brandes, A Faster Algorithm for Betweenness Centrality, in Journal of Mathematical Sociology 25(2):163-177, (2001)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Whereas Modularity Class identifies communities (particularly in large networks), centrality measures examine the position of a node in the graph relative to other nodes. This can be useful for identifying key actors in networks based on the nature of their connections with other actors (rather than simply the number of records).&lt;/p&gt;
&lt;p&gt;If we run Modularity Class as in the figure a pop up message will inform us that there are 246 communities in the network. Given that there are only 362 nodes this suggests a weakly connected network made up of small individual clusters.&lt;/p&gt;
&lt;div id=&#34;filtering-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Filtering the data&lt;/h3&gt;
&lt;p&gt;We have a total of 5,265 nodes which is quite dense. After running the modularity class algorithm above, we will now move over to the Filters tab next to Statistics. Our aim here is to reduce the size of the network&lt;/p&gt;
&lt;p&gt;Move over to the left where it says Ranking and then select the red inverted triangle. Set the largest value to 200 and the smallest to 20 (it is up to you what you choose). Then apply. The network will now change.&lt;/p&gt;
&lt;p&gt;Open the Filters menu and choose Attributes. That will open a set of Folders and we would like to use Range. When the Range folder is open drag frequency into the Queries area below (marked with a red icon and Drag message when empty). Then either drag the range bar until you see a frequency of 5 as the minimum or change the number by clicking on it. Note that as we drag the results the number of Nodes and Edges in the Context above will change. We are looking for a manageable number. In the image below I have set the number to 5.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/filter.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-node-size&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting Node Size&lt;/h3&gt;
&lt;p&gt;Next we want to size the nodes. On the left, look for the Appearance tab and then with Nodes in grey choose the Ranking button. Here the minimum size is set to 20 and the maximum to 150. Note that the default setting is 10 and this is generally too small for easy visibility. Press Apply and you will see the network changes to display the size of nodes based on the frequency. You can always adjust the size of nodes later if you are unhappy with them.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/size.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;colouring-the-nodes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Colouring the Nodes&lt;/h3&gt;
&lt;p&gt;To colour the nodes choose the small palette icon next to the size icon. We now have choices on Unique (simply grey), Partition or Ranking. In this case we will choose Ranking and frequency. Note that a range of colour palettes can be accessed by clicking on the small icon to the right of the colour bar under ranking. When you have found a palette that you like then click Apply.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/colour.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An alternative way of colouring the graph in earlier versions of gephi was to partition on Modularity class. This would colour the nodes as ‘communities’ of closely linked nodes. However, at present in Gephi 9 this option does not appear to be consistently available but may return in a future update.&lt;/p&gt;
&lt;p&gt;There are a range of other options for colouring nodes including a colour plugin that will detect if a column with a colour value is present in the imported data. This can be very useful if you have colour coded categories of data prior to importing to gephi.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;laying-out-the-graph&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Laying out the Graph&lt;/h2&gt;
&lt;p&gt;In the bottom left panel called Layout in the figure above there are a range of network visualisation options with more that can be imported from the plugin menus. Among the most useful are Fruchterman-Reingold, Force Atlas, OpenOrd and Circular with specialist plugins for georeferenced layouts that are well worth experimenting with.&lt;/p&gt;
&lt;p&gt;We will illustrate network layout using Fruchterman-Reingold. The first setting is the area for the graph. The default is 10,000 but we will start with 20,000 because 10,000 tends to be too crunched. The default for the gravity setting is 10. This is intended to stop the nodes dispersing too far but is often too tight when labels are applied. Try changing the setting to 5 (which reduces the gravitational pull). The settings in the different layout options can take some getting used too and it is worthwhile creating a record of useful settings. Gephi does not save your settings so make sure you write down useful settings.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/layout_settings.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We are now good to go. But, before we start take note of two important options for later use.&lt;/p&gt;
&lt;p&gt;The first is the NoOverlap plugin we installed above. This will help us to deal with overlapping nodes after layout. The second is Expansion which will help us to increase the size of a network are to make it easier to see the labels. Note also the Contraction option which will allow us to pull a network back in if it expands too far.&lt;/p&gt;
&lt;p&gt;Now make sure that Fruchterman-Reingold is selected with the settings mentioned above and click &lt;code&gt;Run&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can leave the network to run and the nodes will start to settle. If the network disappears from view (depending on your mouse) try scrolling to zoom out. Our aim is to arrive at a situation where lines only cross through nodes where they are connected. As you become more experienced with layout you may want to assist the nodes with moving into a clear position for a tidier graph.&lt;/p&gt;
&lt;p&gt;You will now have a network that looks something like this (note that 15,000 for the Area may have been enough).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/layout.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that some of the nodes are set very close together. That will affect the ability to label the nodes in a clear way. To address this we first use the NoOverlap function and later we may want to use the Expansion function in the Layout drop down menu items.&lt;/p&gt;
&lt;p&gt;Choose nooverlap from the menu and Run.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/noverlap.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While the difference is very minor in this case we have at least moved the nodes into separate positions. At a later stage you may want to use the Expansion function. This will increase the size of the network and is useful when working with labels.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/noverlap_laidout.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;save-your-work&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Save your work&lt;/h4&gt;
&lt;p&gt;At this stage we will save our work. One feature of Gephi as a Java programme is that there is no undo option. As a result it is a good idea to save work at a point where you are fairly happy with the layout as it stands.&lt;/p&gt;
&lt;p&gt;Go to File and choose Save As and give the file name a &lt;code&gt;.gephi&lt;/code&gt; extension. Do not forget to do this or gephi will not know how to read the file. If all goes well the file will save. On some occasions Java may throw an exception and you will basically have to start again. That is one reason to save work in Gephi regularly because it is a beta programme and subject to the predilections of Java on your computer.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-labels&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding Labels&lt;/h2&gt;
&lt;p&gt;The next step is to add some labels. In the bottom menu bar there are a range of options. What we want is the small grey triangle on the right of this menu bar that will open up a new bar. Click in the triangle and you will see a set of options. Choose labels and then at the far left check the &lt;code&gt;Node&lt;/code&gt; box. We will not see any labels yet.&lt;/p&gt;
&lt;p&gt;To the right is a menu with size. This is set to scaled. To see some labels move the scale slider as far as it will go. We will see labels come into view and a first hint that we will need to do some more work on laying out the graph to make it readable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/labels.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, change size to Node Size, the screen will now fill with text. Go to the scaler and pull it back until there is something more or less readable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/labels2.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At this stage we may need to take a couple of actions.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Where it is clear that our nodes are too close together we will need to run Expansion from the layout menu. As a general rule of thumb you should only need to do this twice at most… but it may depend on your graph.&lt;/li&gt;
&lt;li&gt;If you have very long labels such as Massachusetts Institute of Technology you will probably want to head over to the Observatory and edit the Node Label to something manageable such as MIT. This can make a big difference in cleaning up the labels.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the image below we have used Expansion twice and then manually resize the labels using the slider.&lt;/p&gt;
&lt;p&gt;You will now have something that looks more or less like this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/expanded.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that you can use the slider to the right in the bottom menu to adjust the sizes and you could of course adjust the font. In some cases you may be happy with a rough and ready network rather than the detailed adjustments that are required for a final network graph.&lt;/p&gt;
&lt;p&gt;Note the small camera icon on the left of the bottom menu. Either press that to take a screenshot or hold to bring up a configure menu that will allow you to choose a size.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/screenshot.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you pursuing this option you may also want to adjust the font or the colour and to use the bottom menu to arrive at a result you are happy with for display. In some cases (as we will deal with below) manually moving the nodes will allow you to arrive at a cleaner network for a screenshot.&lt;/p&gt;
&lt;p&gt;Screenshots can be a very useful step in exploring data or sharing data internally. For publication quality graphics you will need to move over to using the Preview Options and engage in the &lt;code&gt;gephi shuffle&lt;/code&gt; to progressively clean up the network for a publication quality graphic.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-preview-options&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using the Preview Options&lt;/h2&gt;
&lt;p&gt;A more involved option for network visualisation is to move over to the Preview tab next to the Data Laboratory.&lt;/p&gt;
&lt;p&gt;The default option uses curved edges. To use this press &lt;code&gt;Refresh&lt;/code&gt;. This is fine but we can’t see any labels. In the presets now try default curved. You can play around with the different settings until you find a version that you like.&lt;/p&gt;
&lt;p&gt;The main issue that we have here is that the labels are too large and the line weigths may also be very heavy.&lt;/p&gt;
&lt;p&gt;To address the line weigth look for and check the rescale weight option under edges.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/rescale_weight.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note here the difference with the visualisation in the Overview. With Gephi what you see is not what you get.&lt;/p&gt;
&lt;p&gt;To arrive at a more readable network the first option is to adjust the size of the font in the &lt;code&gt;Node Labels&lt;/code&gt; panel of the preview settings. Note here that label size is set to be proportional to the font (uncheck that and experiment if you wish). If we stick with proportional font size then we will start smaller and move upwards. For example, if we adjust the font size to 3 then the proportional font size will be reduced. In deciding on the font size an important consideration will be how many nodes you want to be legible to the reader. In this case setting the font size to 3 and this produces a pretty legible network.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/font3.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That is quite an acceptable graph for seeing the larger nodes. However, note that the labels of some of the nodes are overlapping some of the other nodes. This can produce a very cluttered look. The larger the base font size the more cluttered the graph will look and the more adjustment it is likely to need.&lt;/p&gt;
&lt;p&gt;To make adjustments to this network we will use size 3. We will now need to move back and forwards between the Preview and the Overview adjusting the position of the nodes. For very complex graphs it can help to print out the preview to see what you need to adjust. Another sensible way to proceed is to mentally divide the graph into quarters and proceed clockwise quarter by quarter adjusting the nodes as you go. It is a very good idea to save your work at this point and as you go along.&lt;/p&gt;
&lt;p&gt;In the first and second quarter moving clockwise things look good with no overlapping labels. However, some adjustments are needed in the third quarter in the middle of the network where Campanella and Kurs are overlapping. To make the adjustment move to the Overview tab, then select the small hand in the left vertical menu for grab. Locate Campanella and move it out of the way so it is not overlapping. Be gentle. Now go back to Preview and hit Refresh. When doing this quarter by quarter it can be helpful to zoom in in the Overview and in the Preview. For each of the overlapping nodes quarter by quarter make an adjustment periodically checking back by using Refresh in Preview and saving as you go along. Note that the aim is minor adjustments rather than major adjustments to node position (it is also possible to attempt to use Label Adjust in the Layout options but in practice this can distort the network). In the process it is also worth watching out for edges that intersect with nodes where there is no actual link. In those cases adjust the node position by trying to move it to the side of the unrelated edge. Note that this is often not possible with complex graphs and you will need to explain in the text that nodes may intersect with unrelated edges. Also check that any edits to labels do not contain mistakes (such as CATECH rather than CALTECH) and adjust accordingly. Typically long labels cause problems at this point and can be edited down in the Data Laboratory.&lt;/p&gt;
&lt;p&gt;Through a series of minor adjustments in a clockwise direction you should arrive at a final network graph. Expect to spend about 20 minutes on clean up when you are familiar with Gephi depending on the number of nodes. It is worth noting that you will often need to go back to make final adjustments.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/final.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The basic principle here is that each node should have a readable label when you zoom in and that edges should not intersect with unrelated nodes (except if this is unavoidable). In this case we have taken a screen shot of the core of the network.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/zoom.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is quite common when you arrive at a publication quality graphic to suddenly discover a mistake in the network. For example, at the data cleaning stage you may have decided not to group two companies with very similar names. However, at the network visualization stage the network suggests that in practice the two companies are one and the same. In this case check the data and head over to the Data Laboratory to group the nodes to a single node. As this suggests, time spent on data cleaning and data preparation will normally reap dividends in terms of time saved later in the analysis and visualization process.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exporting-from-preview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exporting from Preview&lt;/h2&gt;
&lt;p&gt;At this stage we will want to do a final check and then export the data. Arriving at a publication quality export can in fact be the most time consuming and troublesome aspect of using Gephi. Before going any further save your work.&lt;/p&gt;
&lt;p&gt;When exporting note that what you see on the screen and what you get are not exactly the same thing. The main issues are borders around nodes and the weight of lines in the edges. To adjust for this in the Nodes panel in Preview change the border width to a lower setting or 0 (the option we are choosing here). In the edges panel, if you do not want heavy lines then adjust the thickness or opacity (or both). In this case we have reduced the opacity of the edges to 50 and left the thickness as is. If you change something remember to hit Refresh.&lt;/p&gt;
&lt;p&gt;Next select the export button in the bottom left. We will export to .pdf.&lt;/p&gt;
&lt;p&gt;When you choose Export note that there is an &lt;code&gt;Options&lt;/code&gt; menu for tighter control of the export.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/export.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The defaults are sensible and we will use those. If you are tempted to adjust them note that Gephi does not remember your settings, even when saved, so write them down. In reality the defaults work very well.&lt;/p&gt;
&lt;p&gt;If all goes well you will end up with an image that looks like this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/gephi9/drones.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Because the default size is portrait you will want to crop the image. For publication you will also want to outline the text (to fix the font across system). This can be done with the free GIMP software or paid software such as Adobe Illustrator.&lt;/p&gt;
&lt;p&gt;Congratulations, you have now created your first Gephi network graph.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;resources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Resources&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;http://gephi.github.io/&#34;&gt;Gephi website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gephi/gephi&#34;&gt;Gephi github repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Quick start &lt;a href=&#34;https://gephi.github.io/tutorials/gephi-tutorial-quick_start.pdf&#34;&gt;guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Installation instructions for &lt;a href=&#34;http://gephi.github.io/users/install/&#34;&gt;All platforms&lt;/a&gt;.
Gephi 8 suffers from a known issue for Mac users. That is, it uses Java 6 which is not installed by default on Macs. To resolve this you should follow the instructions posted &lt;a href=&#34;http://sumnous.github.io/blog/2014/07/24/gephi-on-mac/&#34;&gt;here&lt;/a&gt; and works very well in most cases. It basically involves downloading a mac version of Java containing Java 6 and then running three or four commands in the Terminal on the mac to configure Gephi. If that doesn’t work try this more &lt;a href=&#34;https://lbartkowski.wordpress.com/2014/11/28/gephi-0-8-2-on-apple-osx-yosemite/&#34;&gt;detailed account&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://marketplace.gephi.org/plugin/excel-csv-converter-to-network/&#34;&gt;Excel/csv converter to network plugin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;For ideas on patent network visualisation you might want to try this article on &lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0034368&#34;&gt;synthetic biology&lt;/a&gt;, this &lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0078737&#34;&gt;article&lt;/a&gt; on species names in patent data, and the use of exploratory network analysis using IPC/CPC co-occurrence analysis in the &lt;a href=&#34;http://www.wipo.int/patentscope/en/programs/patent_landscapes/reports/animal_gr.html&#34;&gt;WIPO Patent Landscape for Animal Genetic Resources&lt;/a&gt;. For more try this &lt;a href=&#34;https://www.google.co.uk/webhp?sourceid=chrome-instant&amp;amp;ion=1&amp;amp;espv=2&amp;amp;ie=UTF-8#q=patent%20network%20analysis&#34;&gt;Google Search&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Patent Databases</title>
      <link>/patent-databases/</link>
      <pubDate>Sun, 17 May 2015 00:00:00 +0000</pubDate>
      
      <guid>/patent-databases/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This article provides a quick overview of some of the main sources of free patent data. It is intended for quick reference and points to some free tools for accessing patent databases that you may not be familiar with.&lt;/p&gt;
&lt;p&gt;This article is now a chapter in the WIPO Manual on Open Source Patent Analytics. You can read the chapter in electronic book format &lt;a href=&#34;https://wipo-analytics.github.io/databases.html&#34;&gt;here&lt;/a&gt; and find all the materials including presentations at the &lt;a href=&#34;https://github.com/wipo-analytics&#34;&gt;WIPO Analytics Github homepage&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It goes without saying that getting access to patent data in the first place is fundamental to patent analysis. There are quite a few free services out there and we will highlight some of the important ones. Most free sources have particular strengths or weaknesses such as the number of records that can be downloaded, the data fields that can be queried, the format the data comes back in or how &lt;code&gt;clean&lt;/code&gt; data is in terms of the hours required to prepare for analysis. We won’t go into all of the details that but will provide some basic pointers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-databases&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Databases&lt;/h2&gt;
&lt;div id=&#34;the-lens&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.lens.org/lens/&#34;&gt;The Lens&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Previously known as the Patent Lens this is a well designed site with quite a few visualisation options and access to sequence data. It is possible to search the title, abstract, description and claims of patent documents and create and share data in collections. In 2015 the ability to download up to 10,000 records at a time was added. When combined with interactive charts that allow the user to drill down into results set, this has transformed the Lens into a very useful and innovative database and visualization tool.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/Lens_2015-0517_14-19-26.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;patentscope&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://patentscope.wipo.int/search/en/search.jsf&#34;&gt;Patentscope&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The WIPO Patentscope database provides access to Patent Cooperation Treaty data including downloads of a selection of fields (up to 10,000 records), a very useful &lt;a href=&#34;https://patentscope.wipo.int/search/en/clir/clir.jsf?new=true&#34;&gt;search expansion translation tool&lt;/a&gt;, and &lt;a href=&#34;https://www3.wipo.int/patentscope/translate/translate.jsf?interfaceLanguage=en&#34;&gt;translation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/simplesearchresultspizza.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Obtaining &lt;a href=&#34;https://patentscope.wipo.int/search/en/sequences.jsf&#34;&gt;sequence data from Patentscope&lt;/a&gt;. Note that this rapidly becomes gigabytes of data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/pctseq.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;espacenet&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://worldwide.espacenet.com/?locale=en_EP&#34;&gt;espacenet&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Probably the best known free patent database from the European Patent Office.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/Espacenet.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;latipat&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://lp.espacenet.com&#34;&gt;LATIPAT&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;For readers in Latin America (or Spain &amp;amp; Portugal) LATIPAT is a very useful resource.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/Espacenet_Latipat_2015-0517_15-11-21.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;epo-open-patent-services&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.epo.org/searching/free/ops.html&#34;&gt;EPO Open Patent Services&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Access patent data through the EPO Application Programming Interface (API) free of charge. Requires programming knowledge.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/OPS.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The developer portal allows you to test your API queries and is recommended.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/OPS_Developer_Portal.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;uspto-patents-view&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.patentsview.org/web/&#34;&gt;USPTO Patents View&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;http://patft.uspto.gov&#34;&gt;USPTO main database search page&lt;/a&gt; can reasonably be described as well… old. In 2016 the USPTO team initiated an &lt;a href=&#34;http://www.uspto.gov/learning-and-resources/open-data-and-mobility&#34;&gt;Open Data and Mobility initiative&lt;/a&gt; that opens up USPTO patent and trademark data. The new &lt;a href=&#34;https://developer.uspto.gov&#34;&gt;Open Date Portal&lt;/a&gt; is still in Beta but provides an insight into things to come.&lt;/p&gt;
&lt;p&gt;As part of the shift to open data the USPTO has established an external &lt;a href=&#34;http://www.patentsview.org/web/&#34;&gt;Patents View&lt;/a&gt; for free searches and &lt;a href=&#34;http://www.patentsview.org/download/&#34;&gt;bulk downloads&lt;/a&gt;. If simple searching does not meet your needs, or the bulk options are too overwhelming, then &lt;a href=&#34;http://www.patentsview.org/api/doc.html&#34;&gt;the new JSON API service&lt;/a&gt; is likely to meet your needs. The services are still in beta but this is a very exciting development for those who need greater levels of access to patent data or access to specific data fields.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;google-patents&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.google.com/patents&#34;&gt;Google Patents&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/googlepatents_2015-0517_14-09-22.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://developers.google.com/patent-search/terms&#34;&gt;Google Patent Search API&lt;/a&gt; has been deprecated. Access through the Google Custom Search API with the API flag for patents &lt;a href=&#34;http://stackoverflow.com/questions/15028166/python-module-for-searching-patent-databases-ie-uspto-or-epo&#34;&gt;reported&lt;/a&gt; to be &lt;code&gt;&amp;amp;tbm=pts&lt;/code&gt; with example code for using the API in Python.&lt;/p&gt;
&lt;p&gt;In the free version of the Google Custom Search API data retrieval is limited and the patent field headings are unclear (that is they use non-standard names). For free patent analytics, Google Custom Search is presently of very limited use.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;google-prior-art-finder&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.google.com/patents/related&#34;&gt;Google Prior Art Finder&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The Google Prior Art Finder is a relatively recent development that allows you to enter search terms or patent numbers and to view and export results.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/google_priorart1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
The results include a Top Ten and are broken down into sections including Google Scholar, Patents etc.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/google_priorart2.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The Export button will export the top ten results for each section in a .csv file.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/google_priorart3.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is possible to load more results for a section (e.g. see More Patent Results at the bottom of the results) and then export them (e.g. 20 patent documents rather than 10). In a test we managed to export 140 patent results but this could rapidly become laborious. An additional issue is that the data will need transposing. At the time of writing we had not identified an API route to Prior Art Finder.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;google-uspto-bulk-download&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.google.com/googlebooks/uspto.html&#34;&gt;Google USPTO Bulk download&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;http://patft.uspto.gov&#34;&gt;USPTO patent databases&lt;/a&gt; may be archaic but you can download the entire US collection from the &lt;a href=&#34;https://www.google.com/googlebooks/uspto-patents.html&#34;&gt;Google USPTO Bulk download service&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/USPTObulk.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is a fantastic service, and an example to patent offices everywhere on freeing up patent data. If you have a good broadband connection and the hard drive space, it is quite good fun to suddenly have access to millions of patent records. The authors used the service to text mine the collection for millions of biological species names as reported &lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0078737&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/USPTOGrant.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, one important issue to note is that the XML delimiting individual documents is not always well demarcated. This means that any code that will work for one bulk set of files may fail on another set. While it is possible to address this, be prepared to spend time working on this and/or seek assistance from a professional programmer. For an insight into these issues see this &lt;a href=&#34;http://stackoverflow.com/questions/25107557/parseing-xml-by-r-always-return-xml-declaration-error&#34;&gt;Stackoverflow discussion&lt;/a&gt; on parsing the data in R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;free-patents-online&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.freepatentsonline.com&#34;&gt;Free Patents Online&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Sign up for a free account for enhanced access and to save and download data. It has been around quite a while now and while the download options are limited we rather like it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/Freepatentsonline2015-03-26 16-26-13.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;depatisnet&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.dpma.de/english/service/e-services/depatisnet/&#34;&gt;DEPATISnet&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We are not covering national databases. However, the patent database of the German Patent and Trademark Office struck us as potentially very useful. It allows for searches in English and German and has extensive coverage of international patent data, including the China, EP, US and PCT collections. The coverage details are &lt;a href=&#34;https://depatisnet.dpma.de/DepatisNet/depatisnet?action=datenbestand&#34;&gt;here&lt;/a&gt;. Worth experimenting with.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/DEPATISnet_13-53-19.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;oecd-patent-databases&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.oecd.org/sti/inno/oecdpatentdatabases.htm&#34;&gt;OECD Patent Databases&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;One that is more for patent statisticians. The OECD has invested a lot of effort into developing patent indicators and resources including citations, the Harmonised Applicants names database &lt;a href=&#34;http://www.oecd.org/sti/inno/43846611.pdf&#34;&gt;HAN database&lt;/a&gt;, mapping through the &lt;a href=&#34;http://www.oecd.org/sti/inno/40794372.pdf&#34;&gt;REGPAT database&lt;/a&gt; among other resources that are available free of charge.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/OECD_patent_databases.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Along the same lines the US National Bureau of Economic Research &lt;a href=&#34;http://www.nber.org/patents/&#34;&gt;NBER US Patent Citations Data File&lt;/a&gt; is an important resource.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;epo-world-patent-statistical-database&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.epo.org/searching-for-patents/business/patstat.html&#34;&gt;EPO World Patent Statistical Database&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The most important database for statistical use is the EPO World Patent Statistical Database (PATSTAT) and contains around 90 million records. PATSTAT is not free and costs 1250 Euro for a year (two editions) or 630 Euro for a single edition. The main barrier to using PATSTAT is the need to run and maintain a +200 Gigabyte database. However, there is also an online version of PATSTAT that is free for the first two months if you wish to try it by signing up for the trial (knowledge of SQL required).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/patstat.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For users seeking to load PATSTAT into a MySQL database Simone Mainardi provides the following &lt;a href=&#34;https://github.com/simonemainardi/load_patstat&#34;&gt;code on Github&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-data-sources&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other data sources&lt;/h3&gt;
&lt;p&gt;A number of companies provide access to patent data, typically with tiered access depending on your needs and budget. Examples include &lt;a href=&#34;https://www.thomsoninnovation.com/login&#34;&gt;Thomson Innovation&lt;/a&gt;, &lt;a href=&#34;https://www.questel.com/index.php/en/&#34;&gt;Questel Orbit&lt;/a&gt;, &lt;a href=&#34;http://www.stn-international.de/index.php?id=123&#34;&gt;STN&lt;/a&gt;, and &lt;a href=&#34;https://www.patbase.com/login.asp&#34;&gt;PatBase&lt;/a&gt;. We will not be focusing on these services but we will look at the use of data tools to work with data from services such as Thomson Innovation.&lt;/p&gt;
&lt;p&gt;For more information on free and commercial data providers try the excellent &lt;a href=&#34;http://www.piug.org&#34;&gt;Patent Information User Group&lt;/a&gt; and its list of &lt;a href=&#34;http://wiki.piug.org/display/PIUG/Patent+Databases&#34;&gt;Patent Databases&lt;/a&gt; from Tom Wolff and Robert Austin.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/PIUG_Wiki_2015-0517_15-45-05.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Also worth mentioning is the Landon IP &lt;a href=&#34;http://www.intellogist.com/wiki/Main_Page&#34;&gt;Intellogist&lt;/a&gt; blog which maintains &lt;a href=&#34;http://www.intellogist.com/wiki/Category:Intellogist_Reports&#34;&gt;Search System Reports&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/Intellogist_2015-0517_16-03-52.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tools-for-accessing-patent-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tools for Accessing Patent Data&lt;/h2&gt;
&lt;p&gt;In closing this chapter we will highlight a couple of tools for accessing patent data, typically using APIs and Python. We will come back to this later and are working to try this approach in R.&lt;/p&gt;
&lt;div id=&#34;patent2net-in-python&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://github.com/Patent2net/Patent2Net&#34;&gt;Patent2Net&lt;/a&gt; in Python&lt;/h3&gt;
&lt;p&gt;A Python tool to access and process the data from the European Patent Office OPS service.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/Patent2Net_GitHub_2015-0517_15-49-58.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;python-epo-ops-client-by-gsong&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://github.com/55minutes/python-epo-ops-client&#34;&gt;Python EPO OPS Client&lt;/a&gt; by Gsong&lt;/h3&gt;
&lt;p&gt;A Python client for OPS access developed by Gsong and freely available on GitHub. Used in Patent2Net above.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/python-epo-ops-client-GitHub_2015-0517_15-53-34.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fung-institute-patent-server-for-uspto-data-in-json&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://github.com/funginstitute/patentserver&#34;&gt;Fung Institute Patent Server&lt;/a&gt; for USPTO data in JSON&lt;/h3&gt;
&lt;p&gt;Researchers at the Fung Institute have also been active in developing open source resources for accessing and working with patent data. We highlight &lt;code&gt;patentserver&lt;/code&gt; but it is worth checking out other resources in the repository such as &lt;a href=&#34;https://github.com/funginstitute&#34;&gt;patentprocessor&lt;/a&gt;, a set of Python scripts for processing USPTO bulk download data. Note that development of these tools no longer appears to be active.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tools/funginstitutepatentserver.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round Up&lt;/h2&gt;
&lt;p&gt;One problem confronting patent analysts is access to data in a form that is suitable for more detailed analysis. Typically this involves hundreds or many thousands of records. Recent years have increasingly opened up patent data through the ability to download 1,000 or 10,000 records at a time. However, access to downloads of titles, abstracts and claims or descriptions and full text remains limited when this is what is needed. Patent offices such as the USPTO have taken a leading role in making bulk patent data available and this is very much to be welcomed for those working on patent analytics. However, it is reasonable to say that the present situation is one of improvements in access (through Patentscope, the Lens and the EPO OPS service) but not quite in the quantitities or with the data fields patent analysts would like.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualising Data with Tableau Public</title>
      <link>/tableau-patents/</link>
      <pubDate>Sun, 17 May 2015 00:00:00 +0000</pubDate>
      
      <guid>/tableau-patents/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this article we will be analysing and visualising patent data using Tableau Public.&lt;/p&gt;
&lt;p&gt;Tableau Public is a free version of Tableau Desktop and provides a very good practical introduction to the use of patent data for analysis and visualisation. In many cases Tableau Public will represent the standard that other open source and free tools will need to meet.&lt;/p&gt;
&lt;p&gt;This is a practical demonstration of the use of Tableau in patent analytics. We have created a set of cleaned patent data tables on &lt;code&gt;pizza patents&lt;/code&gt; using a sample of 10,000 records from WIPO Patentscope that you can download as a .zip file from &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium_clean/pizza_medium_clean.zip?raw=true&#34;&gt;here&lt;/a&gt; to use during the walkthrough. Details of the cleaning process to reach this stage are provided in the codebook that can be viewed &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium_clean/pizza_medium_code_book_15052105.txt&#34;&gt;here&lt;/a&gt;. The &lt;a href=&#34;http://poldham.github.io/openrefine-patent-cleaning/&#34;&gt;Open Refine walkthrough&lt;/a&gt; can be used to generate cleaned files very similar to those used in this walkthrough using your own data. You will not need to clean any data using our training set files.&lt;/p&gt;
&lt;p&gt;This article will take you through the main features of Tableau Public and the types of analysis and visualisation that can be performed using Tableau. In the process you will be creating something very similar to this &lt;a href=&#34;https://public.tableau.com/profile/wipo.open.source.patent.analytics.manual#!/vizhome/pizzapatents/Overview&#34;&gt;workbook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This article is now a chapter in the WIPO Manual on Open Source Patent Analytics. You can read the chapter in electronic book format &lt;a href=&#34;https://wipo-analytics.github.io/tableau-public-2.html&#34;&gt;here&lt;/a&gt; and find all the materials including presentations at the &lt;a href=&#34;https://github.com/wipo-analytics&#34;&gt;WIPO Analytics Github homepage&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-tableau&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Installing Tableau&lt;/h2&gt;
&lt;p&gt;Tableau can be installed for your operating system by visiting the &lt;a href=&#34;https://public.tableau.com/s/&#34;&gt;Tableau Public website&lt;/a&gt; and entering your email address as in the image below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/providemail.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While you are waiting for the app to download it is a good idea to select &lt;code&gt;Sign In&lt;/code&gt; and then &lt;code&gt;Create one now for Free&lt;/code&gt; to sign up for a Tableau Public Account that will allow you to load up your workbooks to the web and share them. We will deal with privacy issues in making workbooks public or private below but as its name suggests Tableau Public is not for sensitive commercial information.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/signup.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This will lead you to an empty profile page.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/profile.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While you are there you might want to check out the &lt;a href=&#34;https://public.tableau.com/s/gallery&#34;&gt;Gallery&lt;/a&gt; of other Tableau Public workbooks to get some ideas on what it is possible to achieve with Tableau. You may want to view a &lt;a href=&#34;https://public.tableau.com/profile/poldham#!/&#34;&gt;Tableau Workbook&lt;/a&gt; for scientific literature that accompanied this &lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0034368&#34;&gt;PLOS ONE article on synthetic biology&lt;/a&gt;. While it is now a few years old it gives an idea of the possibilities of Tableau and the feel of an existing profile page.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/gallery.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-started&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;When you first open the application you will see a blank page. Before we load some data, note the helpful &lt;code&gt;How-to-Videos&lt;/code&gt; on the right and the link to a &lt;code&gt;visualisation of the day&lt;/code&gt;. There are also quite a lot of training videos &lt;a href=&#34;http://www.tableau.com/learn/training&#34;&gt;here&lt;/a&gt; and a very useful &lt;a href=&#34;http://community.tableau.com/community/forums&#34;&gt;community forum&lt;/a&gt;. If you get stuck, or wonder how somebody produced a cool visualisation, this is the place to go.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/open.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To avoid staring at a blank page we now need to load some data. In Tableau Public this is limited to text or Excel files. To download the data as a single &lt;code&gt;.zip&lt;/code&gt; file click &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium_clean/pizza_medium_clean.zip?raw=true&#34;&gt;here&lt;/a&gt; or visit the &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium_clean/pizza_medium_clean.zip&#34;&gt;GitHub repository&lt;/a&gt;. unzip the file and you will see a collection of &lt;code&gt;.csv&lt;/code&gt; files. The excel file and codebook should be ignored as supplementary.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/github.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see above there are a number of files in this dataset. The &lt;code&gt;core&lt;/code&gt; or reference file is &lt;code&gt;pizza.csv&lt;/code&gt;. All other files are aspects of that file, such as applicants, inventors and international patent classification codes. That is concatenated fields in pizza have been separated out and cleaned up. One file, &lt;code&gt;applicants_ipc&lt;/code&gt; is a child file of &lt;code&gt;applicants&lt;/code&gt; that will allow us to access IPC information for individual applicants. This may not make a lot of sense at the moment but don’t worry it will shortly.&lt;/p&gt;
&lt;p&gt;To get started we will select the &lt;code&gt;pizza.csv&lt;/code&gt; file:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/load_file.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will then see a new screen showing some of the data and the other files in the folder. At the bottom is a flag with &lt;code&gt;Go to Worksheet&lt;/code&gt;, so let’s do that.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/pizza1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will now see a screen that is divided in to &lt;code&gt;Dimensions&lt;/code&gt; on the left, with &lt;code&gt;Measures&lt;/code&gt; below. We can see that in the dimensions there are quite a large number of data fields. Note that Tableau will attempt to guess the type of data (for example numeric or date information is marked with &lt;code&gt;#&lt;/code&gt;, geographic data is marked with a globe, text fields are marked with &lt;code&gt;Abc&lt;/code&gt;). Note that Tableau does not always get this right and that it is possible to change a data type by selecting a field and right clicking as we can see below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/tableau_fields.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On the right hand side we can see a floating panel menu. This can be hidden as a menu bar by clicking the x. This panel displays the visualisation options that are available for the data field that we have selected. In this case two map options are available because Tableau has automatically recognised the country names as geographic information. Note that persuading Tableau to present the option that you want (for example visualising year on year data as a line graph) can involve changing the settings for the field until the option you want becomes available.&lt;/p&gt;
&lt;p&gt;At the bottom of the screen we will see a worksheet number &lt;code&gt;Sheet 1&lt;/code&gt; and then options for adding three types of sheet:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A New Worksheet&lt;/li&gt;
&lt;li&gt;A New Dashboard&lt;/li&gt;
&lt;li&gt;A New Story&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For the moment we will focus on building worksheets with the data and then move into creating Dashboards and then Stories around our pizza data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;publication-trends&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Publication Trends&lt;/h2&gt;
&lt;p&gt;One of the first things we normally want to do with patent data is to map trends, either in first filings, publications or family members. In the case of our pizza patents from Patentscope we have a single member of a dossier of files linked to a particular application. This data is fine for demonstration needs and we can easily map trends for this data.&lt;/p&gt;
&lt;p&gt;To do that we simply drag the publication year in the dimensions to the columns field and the number of records from the measures field. Note that Tableau automatically counts the number of rows in a set to create this field. If working with data where accurate counts are important it is important to make sure that the data has been deduplicated on the relevant field before starting. While it does not apply in this case, another important tip is to always have a way of checking key counts in Tableau such as using quick pivot tables in Excel or Open Office. We do not need to worry about this now, but while Tableau is clever software it is still software: it will not always perform calculations as you expect them. For that reason a cross check of counts is a sensible if not vital part of a Tableau workflow.&lt;/p&gt;
&lt;p&gt;Tableau will guess what we are after and draw a graph.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/publication_trend.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see we now have a graph that plunges off a cliff as we approach the present and contains one null. Null values are typically rows or columns containing blank cells. If there is only 1 null value then the data can probably be left as is (in this case it was a blank row at the bottom of the dataset introduced during cleaning in R). However, it pays to inspect nulls by right clicking on the file in &lt;code&gt;Data&lt;/code&gt; and selecting &lt;code&gt;View data&lt;/code&gt;. If there are large numbers of nulls then you may need to go back and inspect the data and ensure that blank cells are filled with &lt;code&gt;NA&lt;/code&gt; values. Let’s go back to our graph.&lt;/p&gt;
&lt;p&gt;What we see here is the &lt;code&gt;data cliff&lt;/code&gt; that is common with patent data. That is, the cliff does not represent a radical decline in the use of the term &lt;code&gt;pizza&lt;/code&gt;, it represents a radical decline in the availability of patent data the closer we move to the present day. The reason for this is that it generally, as a rough rule of thumb, takes about 24 months for an application to be published and can take longer for patent databases to catch up. As such, our &lt;code&gt;data cliff&lt;/code&gt; reflects a lack of available data in recent years, not a lack of activity. Typically we need to pull back about 2 to 3 years to gain an impression of the trend.&lt;/p&gt;
&lt;p&gt;Before we go any further and adjust the axis we will change the graph to something more attractive. To do that we will select filled graph in the floating panel. Behind that panel is a small colour button that will allow us to select a colour we like. The reason that we do this before adjusting the axis is that when we change the graphic type Tableau will revert any changes made to the axis.&lt;/p&gt;
&lt;p&gt;Next we right click the x (lower) axis and adjust the time frame to something more sensible such as 1980 to 2013 by selecting the &lt;code&gt;fixed&lt;/code&gt; option. As a very rough rule of thumb moving back two or three years from the present will take out the data cliff from the lack of published patent information. Note that if we were counting first filings (patent families) the decline would be earlier and much steeper. These lag effects, and ways to deal with them, have been investigated in detail by the &lt;a href=&#34;http://www.oecd.org/sti/inno/intellectual-property-statistics-and-analysis.htm&#34;&gt;OECD patent statistics team&lt;/a&gt;, see in particular work on &lt;a href=&#34;http://www.oecd.org/science/inno/39485567.pdf&#34;&gt;nowcasting patent data&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We now have a good looking graph with a sensible axis. Note here that if we were graphing multiple trends on the same graph (family and family members) we might prefer a straightforward line graph for the sake of clarity.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/publication_trend_fill.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will give this a name &lt;code&gt;Trends&lt;/code&gt; and add a new worksheet by clicking the icon next to our existing sheet.&lt;/p&gt;
&lt;p&gt;The next piece of information we would like is who the most active applicants are. This will also start to expose issues about the different actors who use the term &lt;code&gt;pizza&lt;/code&gt; in the patent system and encourage us to think about ways to drill down into the data to get more accurate information on technologies we might be interested, such as, in this case, pizza boxes and &lt;a href=&#34;http://www.google.co.uk/patents/US8720690&#34;&gt;musical pizza boxes&lt;/a&gt; in particular.&lt;/p&gt;
&lt;p&gt;It is at this point that the work we did in a previous article on separating individual applicant names into their own rows and cleaning them up using Open Refine, becomes important. In this dataset we have taken this a step further using VantagePoint to separate out individuals from organisations. This information is found in the &lt;code&gt;Applicants Organisations&lt;/code&gt; field in the dataset. Lets just drop that onto the worksheet as a row and then add the number of records as a column (tip, simply drop it onto the sheet).&lt;/p&gt;
&lt;p&gt;At first sight everything seems pretty good. But now we need to rank our applicants. To do that we select the small icon in the menu bar with a stacked bar pointing down.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/sort_applicants.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We now see, as we would in the Excel raw file, that there are a significant number of blank entries for applicants in the underlying data, followed by 85 records for Google and 77 for Microsoft. This is also a very good indicator that there may be multiple uses of the word pizza in the patent system unless these software companies have started selling pizzas online.&lt;/p&gt;
&lt;p&gt;In reality this is &lt;strong&gt;&lt;em&gt;a partial view of activity&lt;/em&gt;&lt;/strong&gt; by the applicants because elsewhere in the data the names are concatenated together. This is normally more obvious than in the present dataset through the presence of multiple names separated by &lt;code&gt;;&lt;/code&gt;(to see this scroll down to the first entry for Unilever).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/applicants_original.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To understand why this is a partial view we will now import the &lt;code&gt;applicants.csv&lt;/code&gt; file. The correct way to do this is to select the menu called &lt;code&gt;Data&lt;/code&gt; then &lt;code&gt;New Data Source&lt;/code&gt; and the file &lt;code&gt;applicants.csv&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Next, drag &lt;code&gt;Applicants Orgs All&lt;/code&gt; onto the Rows. Note that Tableau is interpreting these titles for us (the original is &lt;code&gt;applicants_orgs_all&lt;/code&gt;). Then drag &lt;code&gt;Number of Records&lt;/code&gt; from the dimensions onto the sheet or into the columns entry. Now choose the stacked bar icon as above to rank the applicants by the number of records. We will now see the following.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/applicants_organisations.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note the difference between the original applicants field (where Google scored a total of 85 records) and our separated and cleaned field where Google now scores 191 records. In short, before the separation and cleaning exercises we were only seeing 44% of activity in our dataset by Google involving the term pizza. This still does not mean that they have entered the online pizza business… . What it does tell us is that patent analysis that does not separate or split the concatenated data and clean up name variants is missing over 60% of the story when viewed in terms of applicant activity. As this makes clear, the gains from separating or splitting and cleaning data are huge even where, as in this case, the original data appeared to be quite ‘clean’. That appearance was deceptive.&lt;/p&gt;
&lt;p&gt;Now we have a clearer view of what is happening with our applicants we can make this more attractive. To do that first select the blue bar in the floating panel. The worksheet will now be presented as ranked bars. Next, drag the number of records from Measures onto the &lt;code&gt;Label&lt;/code&gt; button next to &lt;code&gt;Color&lt;/code&gt;. That looks pretty good. If we wanted to go a step further we could now turn to the dimensions panel and drag &lt;code&gt;Applicants Orgs All&lt;/code&gt;, onto the &lt;code&gt;Color&lt;/code&gt; button. The bars will now turn to different colours for each applicant. If this is too bright simply grab the green &lt;code&gt;Applicants Orgs All&lt;/code&gt; box from under the buttons menu and move it towards dimensions to remove it. Finally, if we want to adjust the right alignment of the text to the left, then first right click on the name of a company, pick &lt;code&gt;Format&lt;/code&gt; then alignment and left. While the default is right align, in practice left align creates more readable labels. To change the default do this with the first worksheet you create before creating any others.&lt;/p&gt;
&lt;p&gt;We now have an applicants data table that looks, depending on your aesthetic sensibilities, like this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/applicants_cleaned.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At this stage we might want to take a couple of actions. To make the labels more visible, drag the line between the names and the columns to the right. This will open up some space. Next, think about editing long names down to something short. For example, International Business Machines Corporation, who are also not famous for pizzas, is a little bit too long. Right click on the name and select &lt;code&gt;Edit alias&lt;/code&gt; as in the image below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/edit_alias.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now edit the name to IBM. As a tip note that where you discover you have missed a duplicate name in clean up (remember that we focus on good enough rather than perfect in data cleaning) it is also possible to highlight two rows, right click, look for a filing clip icon and group two entries onto a new name. However, the resulting named group must be used in all later analysis. It is also important to realise that data cleaning is not a Tableau strength, Tableau is about data analysis and exploration through visualisation. For data cleaning use a tool such as Open Refine.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-new-data-sources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding New Data Sources&lt;/h2&gt;
&lt;p&gt;We will follow the same procedure that we used for applicants to add the remaining files as data sources. We will add the following four files (as they appear in the folder in alphabetical order).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;applicants_ipc.csv&lt;/li&gt;
&lt;li&gt;inventors.csv&lt;/li&gt;
&lt;li&gt;ipc_class.csv&lt;/li&gt;
&lt;li&gt;ipc_subclass.detail.csv&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To add the data sources either click the &lt;code&gt;Data&lt;/code&gt; menu and &lt;code&gt;New Data Source&lt;/code&gt; or (faster) the cylinder with a plus sign. Then select &lt;code&gt;Text file&lt;/code&gt;, add each file and allow it to load.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/add_data.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If all goes well the &lt;code&gt;Data&lt;/code&gt; panel will now contain the following files.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/data_panel.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note here that the &lt;code&gt;applicants&lt;/code&gt; data displays a blue tick. This is because it was the last data source that we used and is therefore active. The fields we see in Dimensions belong to that data source. Next click in the bottom menu to create a new worksheet and then click &lt;code&gt;inventors&lt;/code&gt; in the &lt;code&gt;Data&lt;/code&gt; field. The field names will now change slightly. It is important to keep an eye on the data source that you are using because it is quite easy to drop a field from one data source onto another. In some cases this is a good thing. But, if you receive a warning message you will be attempting to drop a data source on to another data source where there is no matching field. We will come back to this on data &lt;code&gt;blending&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Next follow the same procedure for ranking applicants with inventors using the &lt;code&gt;Inventors All&lt;/code&gt;. For anyone interested in seeing the dramatic impacts of concatenated fields try dropping the &lt;code&gt;Inventors Original&lt;/code&gt; field onto the worksheet.&lt;/p&gt;
&lt;p&gt;Using &lt;code&gt;Inventors All&lt;/code&gt; you should now see the following ranked list of inventors.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/inventors_ranked.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now repeat this exercise for the remaining data sources by first creating a sheet and then selecting the data source. As you move through this select the following dimensions to add to the sheet and then drop number of&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;applicants_ipc. Drop &lt;code&gt;Ipc Subclass Detail&lt;/code&gt; onto the sheet. Then drop number of records onto the sheet where the field says Abc. Note that a number &lt;code&gt;6&lt;/code&gt; will appear in the first row. This is an artifact from the separation process. Select that cell, right click and then choose &lt;code&gt;Exclude&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Do not rank this data, but instead drag the field &lt;code&gt;Applicants Orgs All&lt;/code&gt; onto the sheet so that it is the first row (tip, it is easiest to do this by dragging the field into the row bar before the IPC field). You will now see a list of company names followed by a list of IPCs. Congratulations, we now have an idea of who is patenting in a particular area of technology using the word pizza at the level of individual applicants.&lt;/p&gt;
&lt;p&gt;Add a new sheet. Then click on &lt;code&gt;ipc_subclass_detail&lt;/code&gt;. Note that if you click on the data source first, the dimensions panel will go orange. Don’t panic. The reason is that Tableau thinks you are trying to blend data from the ipc_subclass_detail source with applicants_ipc. If you do this simply click on ipc_subclass-detail again.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;ipc_subclass-detail. Drop the &lt;code&gt;Ipc Subclass Detail&lt;/code&gt; dimension on to the sheet. Then drop the number of records onto the sheet. Then click on the first cell containing &lt;code&gt;6&lt;/code&gt; as an artifact and exclude. Repeat for &lt;code&gt;7&lt;/code&gt;. Then select the bar chart in the floating &lt;code&gt;Show Me&lt;/code&gt; panel, then drag &lt;code&gt;Number of Records&lt;/code&gt; onto the &lt;code&gt;Label&lt;/code&gt; button. Now rank the column using the descending button in the upper menu as before.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At this point, if we had not trimmed the leading white space the ranked list would display indentations and there would be duplicates of the same IPC code. For that reason it is important to trim leading white space before attempting to visualise data (and this applies to all our separated fields).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-an-overview-dashboard&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating an Overview Dashboard&lt;/h2&gt;
&lt;p&gt;You should now have five worksheets each of which displays aspects of our core &lt;code&gt;pizza&lt;/code&gt; set. We have named the sheets as follows and suggest that you might want to do the same. Note that where there is more than one sheet containing similar but distinct information it will be helpful to give them distinct names (e.g. IPC Subclass and Applicants IPC Subclasses). We might even start using less technical labels by calling the IPC something clearer like Technology Area, to aid communication with non-IP specialists&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/sheet_names.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s get a quick overview of the data so far. Next to the add worksheet button in the worksheets bar is a second icon to create a dashboard. Click on that and we will now see a sheet called &lt;code&gt;Dashboard 1&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Dashboards are perhaps Tableau’s best known feature and are rightly very popular. We can fill our dashboard by dragging the worksheets from the &lt;code&gt;Dashboard&lt;/code&gt; side menu. The order in which we do this can make life easier or more difficult to adjust later. Let’s do it in the following steps&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Drag &lt;code&gt;Trends&lt;/code&gt; onto the dashboard and it will now fill the view.&lt;/li&gt;
&lt;li&gt;Drag &lt;code&gt;Organisations&lt;/code&gt; onto the dashboard.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/dashboard1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That is rather messy, but all is not lost. Simply click in the top right corner of the organisations panel on the right to remove it (in the original worksheet click on it and select &lt;code&gt;Hide&lt;/code&gt;). We now have an &lt;code&gt;Organisations&lt;/code&gt; column that still looks crunched.&lt;/p&gt;
&lt;p&gt;Now select the top of the organisations box and a small inverted triangle will appear. Click on that and then choose &lt;code&gt;Fit &amp;gt; Fit Width&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/dashboard_fitwidth.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The bars may now disappear. Click into the box on the line where the bars start and drag them back into view. At this point long names may start to be obscured. If desired, right click on a long name such as &lt;code&gt;Graphic Packaging International&lt;/code&gt;, choose &lt;code&gt;Edit alias&lt;/code&gt; and edit it down to something sensible such as &lt;code&gt;Graphic Packaging Int&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We now have two panels on the dashboard. Let’s add two more. First drag technology areas below the line where Trends and Organisations finish. Grey shaded boxes will appear that show the placement, across the width is fine. This can take some time to get right, when the whole of the bottom area is highlighted let go of the mouse. If it goes somewhere strange either select the box and in the top right press &lt;code&gt;x&lt;/code&gt; to remove it, or try moving it (in our experience it is often easier to remove it and try again).&lt;/p&gt;
&lt;p&gt;Do not try to format this box yet. Instead, grab inventors and drag it into the space before the technology areas below.&lt;/p&gt;
&lt;p&gt;We now have four panels in the dashboard but they need some tidying up. First, in the two boxes we have just edited repeat the &lt;code&gt;Fit Width&lt;/code&gt; exercise and then drag the line for the bars around until they are in view and satisfactory. Next, we have names such as &lt;code&gt;Applicants Orgs All&lt;/code&gt; that are our internal reference names. Click on them in each of the three panels one at a time and select &lt;code&gt;Hide Field Labels for Rows&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Hmm… our Technology Areas panel is proving troublesome because even the edited version of the IPC is rather long.&lt;/p&gt;
&lt;p&gt;Before we do any editing, first experiment with the &lt;code&gt;Size&lt;/code&gt; menu in the bottom right. The default dashboard size in Tableau Public is actually quite small. Change the settings until you have something that looks cleaner even if there are still some overlaps. Options such as &lt;code&gt;Desktop&lt;/code&gt;, &lt;code&gt;Laptop&lt;/code&gt; and &lt;code&gt;Large blog&lt;/code&gt; are generally decent sizes but in part the decision depends on where you believe it will be displayed.&lt;/p&gt;
&lt;p&gt;To fix the long Technology areas labels we go back to the original sheet (tip: if you move the mouse to the top right in the panel an arrow with &lt;code&gt;Go to Sheet&lt;/code&gt; will appear, it is very useful for large workbooks). Inside the original sheet, try dragging the line separating the text and bars so that the bars now cover some of the longer text. Then switch back to the dashboard. If you feel unhappy with the result then right click in the panel in the dashboard and then choose &lt;code&gt;Edit alias&lt;/code&gt;. This is useful for simply making labels in the view more visible (it does not change the original data).&lt;/p&gt;
&lt;p&gt;If all goes well you will now have a dashboard that looks more or less like this. Note that depending on the worksheet settings you may want to make the font size consistent (right click and choose Format, then font size). Note also that if you increase the font size (the default is 8 point) then you may need to edit some of the labels again.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;images/tableau/dashboard_completed.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have now done quite a lot of work and produced an Overview dashboard. It is time to save the workbook to the server before doing anything else.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;saving-display-and-privacy-settings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Saving, Display and Privacy Settings&lt;/h2&gt;
&lt;p&gt;The only option for saving a Tableau Public workbook is to save it online. To save the file go to &lt;code&gt;File&lt;/code&gt; and Save to Tableau Public. If you want to save the workbook as a new file (after previously saving) then choose Save As.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/save_public.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You will then be asked to enter your username and password (Tableau does not remember the password) and the file will upload. Tableau will then compress the data. As of June 2015 it is possible to store 10GB of data overall and to have up to 10 million rows in a workbook (which is generally more than enough).&lt;/p&gt;
&lt;p&gt;Tableau will then open a web browser at your profile page and it will look a lot like this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/profile_dashboard.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Having read the message, click &lt;code&gt;Got it&lt;/code&gt; on the right. Do you notice anything strange. Yes, we can only see the Dashboard and not any of the other sheets. To change this and any other details click on &lt;code&gt;edit details&lt;/code&gt; near the title and some menus will open up as follows.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/profile_viewtabs3.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To make sure the worksheets are visible select the check box marked &lt;code&gt;Show workbook sheets as tabs&lt;/code&gt; and then &lt;code&gt;Save&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/profile_viewtabs3.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To access this demonstration workbook go &lt;a href=&#34;https://public.tableau.com/profile/wipo.open.source.patent.analytics.manual#!/vizhome/pizzapatents/Overview&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;privacy-and-security&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Privacy and Security&lt;/h2&gt;
&lt;p&gt;As emphasised above, Tableau Public is by definition a place for publicly sharing workbooks and visualisations. It is not for sensitive data. In the past users, such as journalists, relied on what might be called ‘security by obscurity’ but the trend towards storing data on a Tableau public profile (the only option) makes that less of an option. If this a concern there are two actions that might potentially be considered that limit the visibility of a workbook and its wider use. Logically, the answer to any concerns about Tableau Public and sensitive information is &lt;strong&gt;&lt;em&gt;not to include sensitive information in the first place&lt;/em&gt;&lt;/strong&gt;. The following are not recommendations but simply highlight the available options.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In the discussion on the settings above, there is a check box to prevent users from downloading a workbook. You might want to select that option where a workbook contains information that you do not want to be seen other than what you choose to make visible.&lt;/li&gt;
&lt;li&gt;It is possible to create a setting so that a workbook does not show up on a user’s profile. This is hard to spot, and appears by hovering over the workbook in the Profile view.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/profile_visibility.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As the message points out using this option does not prevent a workbook being found through search engines or seen by users. It just means it is not visible on the profile page.&lt;/p&gt;
&lt;p&gt;As such, Tableau public is fundamentally about sharing information with others through visualisation. That is, it is for information that you want others to see. Here it is briefly worth returning to the completed dashboard above and clicking the share button.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/tableau/profile_sharing.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see here, Tableau generates embed codes for use on websites or for emailing as a link along with twitter and facebook.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round Up&lt;/h2&gt;
&lt;p&gt;In this chapter we have introduced the visualisation of patent data using a set of nearly 10,000 patent documents from WIPO Patentscope that mention pizza. As should by now be clear Tableau Public is a very powerful free tool for data visualisation. It requires attention to detail and care in construction but is one of the best free tools that is out there for visualisation and dashboarding.&lt;/p&gt;
&lt;p&gt;To take working with Tableau on pizza patents forward on your own here are some tips.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;You already know how to use Tableau to create a map of publication countries.&lt;/li&gt;
&lt;li&gt;The pizza source file contains a set of publication numbers. Try a) creating a visualisation with the publication numbers, b) looking in the pizza source file for a set of URL and then exploring what can be done with &lt;code&gt;Worksheet &amp;gt; Action&lt;/code&gt; with that URL.&lt;/li&gt;
&lt;li&gt;In dashboards consider using one field as a filter for another field (such as applicant and title). What data source or data sources would you need to do that?&lt;/li&gt;
&lt;li&gt;What kinds of stories does the pizza data tell us and how might we visualise them using the information provided on applicants and its subset Applicants IPCs?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you get stuck, and it does take time to become familiar with Tableau’s potential, perhaps try exploring this &lt;a href=&#34;https://public.tableau.com/profile/poldham#!/&#34;&gt;workbook on synthetic biology&lt;/a&gt; and the use of Tableau images in this article &lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0034368&#34;&gt;PLOS ONE article&lt;/a&gt;. As a tip, try clicking on the bars and then the titles to understand Actions. Downloading workbooks prepared by others can be a very good way of learning the tips and tricks of tableau visualisation and dashboarding.&lt;/p&gt;
&lt;p&gt;If you would like to download the pizza workbook it is &lt;a href=&#34;https://public.tableau.com/profile/wipo.open.source.patent.analytics.manual#!/vizhome/pizzapatents/Overview&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, one of the most important issues exposed by working with Tableau is that you must ensure that fields you want to visualise are &lt;code&gt;tidy&lt;/code&gt;, that is not concatenated, and also that they are as clean as it is reasonable to make them. For researchers wishing to work up their own data we suggest the Open Refine article as a good starting point.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Patent Data Fields</title>
      <link>/patent-data-fields/</link>
      <pubDate>Thu, 07 May 2015 00:00:00 +0000</pubDate>
      
      <guid>/patent-data-fields/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This article provides a walk through of patent data fields for those who are completely new to patent analytics or want to understand the workings of patent data a little bit better. A video version of the walk through is available &lt;a href=&#34;https://youtu.be/RDPlIUB0_QE?list=PLsZOGmKUMi56dAuqSEHjVWkxEf3MbXlQG&#34;&gt;here&lt;/a&gt; and the slide deck is available for download in &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/raw/master/3_obtaining_patent_data/understanding_data_fields/patent_data_fields_OM.pdf&#34;&gt;.pdf&lt;/a&gt;, &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/raw/master/3_obtaining_patent_data/understanding_data_fields/patent_data_fields_OM.pptx&#34;&gt;powerpoint&lt;/a&gt; and &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/raw/master/3_obtaining_patent_data/understanding_data_fields/patent_data_fields_OM.key&#34;&gt;apple keynote&lt;/a&gt; from &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/tree/master/3_obtaining_patent_data/understanding_data_fields&#34;&gt;GitHub&lt;/a&gt;. This article goes into greater depth on each data field and their use in patent analysis.&lt;/p&gt;
&lt;p&gt;This article is now a chapter in the WIPO Manual on Open Source Patent Analytics. You can read the chapter in electronic book format &lt;a href=&#34;https://wipo-analytics.github.io/datafields.html&#34;&gt;here&lt;/a&gt; and find all the materials including presentations at the &lt;a href=&#34;https://github.com/wipo-analytics&#34;&gt;WIPO Analytics Github homepage&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-a-patent&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is a Patent?&lt;/h2&gt;
&lt;p&gt;A patent can be described in two main ways:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;As a form of intellectual property right.&lt;/li&gt;
&lt;li&gt;As a type of document.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Understanding the structure of patent documents and data fields is the essential foundation of patent analytics. However, for those who are new to the patent system it is worth highlighting the key features of patents as a form of intellectual property right.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;as-a-form-of-intellectual-property-right&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;As a form of intellectual property right&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A patent is a temporary grant of an exclusive right to a patentee to prevent others from making, using, offering for sale, or importing, a patented invention without their consent, in a country where a patent is in force.&lt;/li&gt;
&lt;li&gt;Patent rights are territorial rights - they are only valid in the territory of the country where granted.&lt;/li&gt;
&lt;li&gt;Patents are typically granted for a period of 20 years from the filing data of an application but may be opposed or revoked.&lt;/li&gt;
&lt;li&gt;To be eligible a claimed invention must:
&lt;ul&gt;
&lt;li&gt;Involve patentable subject matter&lt;/li&gt;
&lt;li&gt;Be new or novel&lt;/li&gt;
&lt;li&gt;Involve an inventive step&lt;/li&gt;
&lt;li&gt;Be susceptible to industrial application or useful.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;patents-as-a-type-of-document&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Patents as a type of document&lt;/h2&gt;
&lt;p&gt;For patent analytics we need to concentrate on patents as a form of document and to understand:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The structure of patent documents and their data fields.&lt;/li&gt;
&lt;li&gt;The strengths and limitations of different patent databases as a means for obtaining patent data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this chapter we deal with the basics of patent documents and their data fields.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-data-types&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basic Data Types&lt;/h2&gt;
&lt;p&gt;When performing patent analysis we are dealing with data of seven different types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dates&lt;/strong&gt; (priority, application and publication dates)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Numbers&lt;/strong&gt; (priority number, application number, publication number, family members, citations)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Names&lt;/strong&gt; (Applicants - also known as Assignees - and Inventors)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Classification codes&lt;/strong&gt; (e.g. International Patent Classification/Cooperative Patent Classification)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Text fields&lt;/strong&gt; (Title, Abstract, Description, Claims, Sequence data)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Images&lt;/strong&gt; (Diagrams)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Additional Information&lt;/strong&gt; (Legal Status, Public Registry etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will walk through each of these fields using a patent application for synthetic genomes from the J. Craig Venter Institute as an example. In the electronic version each of the titles for the images are hyperlinked to their sources to make it easy to explore the data as you go through them.&lt;/p&gt;
&lt;div id=&#34;synthetic-genomes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://www.nature.com/nature/journal/v473/n7347/full/473403a.html&#34;&gt;Synthetic Genomes&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Synthetic biology (and synthetic genomics) began to hit the international headlines with the news in 2010 that members of the J. Craig Venter Institute had successfully synthesised the genome of a Mycoides microbe and transplanted the genome into the empty cell of another Mycoides that then &lt;code&gt;booted up&lt;/code&gt;. This led to considerable excitement about the creation of artificial life and is part of the story of the growing prominence of synthetic biology. For our purposes it is an interesting example for walking through standard patent data fields.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/datafields/synthetic_genomes_nature_news.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;original-front-page&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://worldwide.espacenet.com/publicationDetails/originalDocument?CC=WO&amp;amp;NR=2008024129A2&amp;amp;KC=A2&amp;amp;FT=D&amp;amp;ND=5&amp;amp;date=20080228&amp;amp;DB=EPODOC&amp;amp;locale=en_EP&#34;&gt;Original Front Page&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;What we see below is the front page of an international &lt;a href=&#34;http://www.wipo.int/pct/en/&#34;&gt;Patent Cooperation Treaty(PCT)&lt;/a&gt; application from the &lt;a href=&#34;http://www.jcvi.org/cms/home/&#34;&gt;J. Craig Venter Institute&lt;/a&gt; on Synthetic Genomes. The PCT allows applicants to submit a single application for potential consideration in up to 148 other countries that are Parties to the PCT based on decisions made by applicants and examination decisions in individual countries and regions. The front page (or biblio) displays the data fields that are typically used in patent analysis.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/datafields/synthetic_genomes_frontpage_WO.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Starting with dates. There are three dates on this application.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The first date is the &lt;code&gt;priority date&lt;/code&gt; (6 December 2005) in the &lt;code&gt;Priority Data&lt;/code&gt; field (30). This refers to the original (first filing date) for a US application that is the priority (or parent) of all later filings of the same application anywhere else in the world (known as a patent family).&lt;/li&gt;
&lt;li&gt;The second date is the &lt;code&gt;International Filing Date&lt;/code&gt; (22) which is 12 months after the priority filing (US60742542).&lt;/li&gt;
&lt;li&gt;The third date is the &lt;code&gt;International Publication Date&lt;/code&gt; (field 43) which is just over 24 months after the international filing date and 3 years from the first filing date (priority application).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For patent analytics the most important dates are generally the &lt;strong&gt;priority date&lt;/strong&gt; (06.12.2005) and the &lt;strong&gt;publication date&lt;/strong&gt; (28.02.2008). The priority date is important for two reasons. First, in legal terms, it establishes the priority claim for this claimed invention over other claims to the same invention submitted in the same period or later under the terms of the &lt;a href=&#34;http://www.wipo.int/treaties/en/text.jsp?file_id=288514&#34;&gt;Paris Convention&lt;/a&gt;. Second, in economic analysis the priority date is the date that is closest to the investment in research and development and therefore the most important in economic analysis (see the &lt;a href=&#34;http://www.oecd.org/sti/inno/oecdpatentstatisticsmanual.htm&#34;&gt;OECD Patent Statistics Manual&lt;/a&gt; ). However, this information only becomes available when an application is published. That is typically 24 months from the original filing date. As a result this information falls off a cliff the closer we move towards the present day.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;publication date&lt;/strong&gt; is important because, like the &lt;strong&gt;publication number&lt;/strong&gt;, it is generally the most accessible in patent databases. However, in this case there is a 2 to 3 year lag between the first filing date and the publication date. For patent analysis this means that counts based on publication date are always displaying trends that are 2 to 3 years after the original activity has taken place. However, because applicants must pay at each stage of the process patent publication data can be useful as an indicator of demand for patent rights in one or more countries. In many cases the publication date will be the only date that is available to map trends.&lt;/p&gt;
&lt;p&gt;One important lesson from understanding patent date fields is that &lt;strong&gt;patent data is always historic&lt;/strong&gt;. That is, it always refers to activity in the past.&lt;/p&gt;
&lt;p&gt;We will address other data fields below. For now note the applicant and inventor information including address and other useful information for patent analysis on the front page (fields 71 and 72). Also, note the &lt;a href=&#34;http://www.wipo.int/classifications/ipc/en/&#34;&gt;International Patent Classification&lt;/a&gt; data as an indicator of technology areas expressed through alphanumeric codes (e.g. &lt;a href=&#34;http://web2.wipo.int/ipcpub/#refresh=symbol&amp;amp;notion=scheme&amp;amp;version=20150101&amp;amp;symbol=C07H0021040000&#34;&gt;C07H21/04&lt;/a&gt; which tells us that the claimed invention involves nucleic acids). We also see text fields (for text mining) in the title and abstract and finally we have an image with information on DNA cassettes forming part of the invention.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;espacenet-front-page&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://worldwide.espacenet.com/publicationDetails/biblio?CC=WO&amp;amp;NR=2008024129A2&amp;amp;KC=A2&amp;amp;FT=D&amp;amp;ND=5&amp;amp;date=20080228&amp;amp;DB=EPODOC&amp;amp;locale=en_EP&#34;&gt;espacenet Front Page&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Here we can see the same information in the front page for the record in the &lt;a href=&#34;http://worldwide.espacenet.com/?locale=en_EP&#34;&gt;espacenet&lt;/a&gt; database. &lt;a href=&#34;http://worldwide.espacenet.com/?locale=en_EP&#34;&gt;espacenet&lt;/a&gt; is easily accessible and popular. Even when using commercial tools espacenet is often the fastest way to look up or check information. For a brief overview see these &lt;a href=&#34;https://www.youtube.com/playlist?list=PLsZOGmKUMi55SmfBRlNCSq_ZmxLHz16fO&#34;&gt;videos&lt;/a&gt; and the &lt;a href=&#34;http://application.epo.org/wbt/espacenet/&#34;&gt;interactive espacenet assistant&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/datafields/synthetic_genomes_WO.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case we see data in the form that will typically come back from a patent database such as espacenet. Starting with the &lt;strong&gt;dates&lt;/strong&gt;, we can see that the priority number field contains the priority (first filing) date 20051206 (as YYYYMMDD), linked to the priority document &lt;a href=&#34;http://worldwide.espacenet.com/publicationDetails/biblio?DB=EPODOC&amp;amp;II=0&amp;amp;ND=3&amp;amp;adjacent=true&amp;amp;locale=en_EP&amp;amp;FT=D&amp;amp;date=20130930&amp;amp;CC=DK&amp;amp;NR=1968994T3&amp;amp;KC=T3&#34;&gt;US20050742542P&lt;/a&gt;, a provisional US application. This is followed by the application number &lt;a href=&#34;http://worldwide.espacenet.com/searchResults?submitted=true&amp;amp;locale=en_EP&amp;amp;DB=EPODOC&amp;amp;ST=advanced&amp;amp;TI=&amp;amp;AB=&amp;amp;PN=&amp;amp;AP=WO2006US46803&amp;amp;PR=&amp;amp;PD=&amp;amp;PA=&amp;amp;IN=&amp;amp;CPC=&amp;amp;IC=&#34;&gt;WO2006US46803&lt;/a&gt; and date and the publication number and date. The publication number, &lt;a href=&#34;http://worldwide.espacenet.com/searchResults?submitted=true&amp;amp;locale=en_EP&amp;amp;DB=EPODOC&amp;amp;ST=advanced&amp;amp;TI=&amp;amp;AB=&amp;amp;PN=WO2008024129A2&amp;amp;AP=&amp;amp;PR=&amp;amp;PD=&amp;amp;PA=&amp;amp;IN=&amp;amp;CPC=&amp;amp;IC=&#34;&gt;WO2008024129A2&lt;/a&gt;, is normally the easiest to use when searching a patent database.&lt;/p&gt;
&lt;p&gt;Other things to note are that the Applicant and Inventor fields include country code information (e.g US) using standard two letter country codes. While this information is not always available (notably for filings purely on the national level) this data is very useful in patent analytics for identifying cross country collaborations between inventors and applicants. However, be aware that for statistical use it is important to calculate the number of records that possess this country information or to use only those jurisdictions where this data is recorded.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;description&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://worldwide.espacenet.com/publicationDetails/description;jsessionid=2kCKJqOvMF0Te-kUiu5GaPA9.espacenet_levelx_prod_2?CC=WO&amp;amp;NR=2008024129A2&amp;amp;KC=A2&amp;amp;FT=D&amp;amp;ND=5&amp;amp;date=20080228&amp;amp;DB=EPODOC&amp;amp;locale=en_EP&#34;&gt;Description&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The description section (also called the specification) contains details on:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;previous patent filings and prior art such as scientific literature.&lt;/li&gt;
&lt;li&gt;in the case of the United States, applicants include information on whether the research leading to the invention was government funded including the funding agency and relevant contract number.&lt;/li&gt;
&lt;li&gt;a summary followed by detailed background to the claimed invention. This will typically include examples which may be actual worked examples or paper (prophetic) examples.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/datafields/synthetic_genomes_description.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Data provided in the description can be very useful where text mining approaches are applied. For example, the authors have previously text mined millions of documents for biological species names as in this &lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0078737&#34;&gt;article&lt;/a&gt;. In other cases, it may be desirable to investigate the description for information on the &lt;a href=&#34;https://www.cbd.int/doc/meetings/cop/cop-10/information/cop-10-inf-44-en.pdf&#34;&gt;country of origin of materials and traditional knowledge&lt;/a&gt;, or to explore the uses of particular extracts or chemical compounds.&lt;/p&gt;
&lt;p&gt;However, when working with data in the description note that it is often noisy and care is required in constructing a query. For example, a search for pigs will capture lots of data on pigs as animals but also toy pigs and devices for cleaning pipelines (pipeline pigs). In contrast, searches for a country name (such as Senegal or Niger) may produce thousands of results that have nothing to do with that country because they are part of species names (e.g. Acacia senegal or Aspergillus niger).&lt;/p&gt;
&lt;p&gt;It is therefore important to carefully consider and test search queries for text fields such as the Title, Abstract, Description and Claims to avoid being overwhelmed by irrelevant results.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;claims&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://worldwide.espacenet.com/publicationDetails/claims?CC=WO&amp;amp;NR=2008024129A2&amp;amp;KC=A2&amp;amp;FT=D&amp;amp;ND=5&amp;amp;date=20080228&amp;amp;DB=EPODOC&amp;amp;locale=en_EP&#34;&gt;Claims&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The claims section of a patent document is commonly regarded as the most important part of the document because it tells us what the applicant is actually claiming as an invention. What is claimed in a patent application must be supported by the description. For example, one could not insert a section of Jane Austen’s “Pride and Prejudice” into the description for a synthetic genomes applications and expect it to go forward. Furthermore, in countries such as the United States, patent claims are interpreted (constructed) in light of the contents of the description (see this informative &lt;a href=&#34;http://scholarship.law.upenn.edu/cgi/viewcontent.cgi?article=1202&amp;amp;context=penn_law_review&#34;&gt;2009 article by Dan Burk and Mark Lemley&lt;/a&gt; on debates in the US).&lt;/p&gt;
&lt;p&gt;Patent claims take a variety of forms and what may be permitted may vary according to the country or jurisdiction or take specialised forms (e.g. Design patents or US Plant Patents). That can make describing and interpreting patent claims difficult. For more detailed discussion see the &lt;a href=&#34;http://www.wipo.int/edocs/pubdocs/en/patents/867/wipo_pub_867.pdf&#34;&gt;WIPO Patent Drafting Manual&lt;/a&gt; with examples from the manual below, &lt;a href=&#34;http://www.wipo.int/edocs/pubdocs/en/patents/867/wipo_pub_867.pdf&#34;&gt;see pages 84-90&lt;/a&gt;:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Compositions of matter (e.g. an extract, a compound).&lt;/li&gt;
&lt;li&gt;Apparatus (e.g. a stand for a camera).&lt;/li&gt;
&lt;li&gt;Methods (e.g. methods for amplifying a nucleic acid or for making tea).&lt;/li&gt;
&lt;li&gt;Process (e.g. processes for producing a particular product such as tea, known as Product by Process Claims).&lt;/li&gt;
&lt;li&gt;Result to be Achieved/Parameters. (e.g. an ashtray that automatically extinguishes a cigarette).&lt;/li&gt;
&lt;li&gt;Design claims (e.g. a specific design for an umbrella).&lt;/li&gt;
&lt;li&gt;Plant patents (limited to certain jurisdictions, generally 1 claim for a distinct variety of a particular cultivar e.g. of &lt;strong&gt;Banisteriopsis caapi&lt;/strong&gt; named ‘Da Vine’). Restricted to the claimed cultivar and not to be confused with a utility patent as in the &lt;a href=&#34;http://www.ciel.org/Bio/ayahuascapatentcase.html&#34;&gt;Ayahuasca controversy&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Biotechnology claims tend to take the form of “An isolated polynucleotide selected from…” followed by sequence identifiers (SEQ ID).&lt;/li&gt;
&lt;li&gt;Use Claims. In some jurisdictions, an applicant may claim a new use for a known compound. For example, the use of a well known compound for the treatment of a disease (where that has not previously been described).&lt;/li&gt;
&lt;li&gt;Software Claims. Jurisdictions also vary on whether they permit software claims (and the law is also subject to revision). Examples include references to “A computer-readable medium storing instructions…” or “A memory for storing data for access by an application program…” followed by further details on the data structure and objects.&lt;/li&gt;
&lt;li&gt;Omnibus claims: such as “1. An apparatus for harvesting corn as described in the description. 2. A juice machine as shown in Figure 4.”&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;While this sounds like a lot of different types of claim in practice you do not encounter all of these all the time. In our experience (mainly working on biological issues), the claims tend to be for compositions of matter, including biotechnology above, and methods. That could vary depending on your field of interest.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/datafields/synthetic_genomes_claims.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the case of the synthetic genomes patent application we can see that we are dealing with a method claim (claim 1) relating to the assembly of nucleic acid cassettes. The first claim is the most important (and often the most useful) of the claims because everything else that follows normally depends on that claim.&lt;/p&gt;
&lt;p&gt;Claims can be divided into independent and dependent claims and they form a claims tree. In this case claims 2 - 14 depend on claim 1 and this can be identified by the reference to “The method of claim 1” at the beginning of each of these claims. The next independent claim in &lt;a href=&#34;http://worldwide.espacenet.com/publicationDetails/claims?CC=WO&amp;amp;NR=2008024129A2&amp;amp;KC=A2&amp;amp;FT=D&amp;amp;ND=5&amp;amp;date=20080228&amp;amp;DB=EPODOC&amp;amp;locale=en_EP&#34;&gt;WO2008024129A2&lt;/a&gt; appears in claim 32 for “32. A synthetic genome”. Independent claims do not depend on the other claims.&lt;/p&gt;
&lt;p&gt;One final note on patent claims for patent analysis is that claims may be cancelled or may be modified in particular jurisdictions. In some cases an examiner may determine that there is more than one invention in the application. This can lead to the application being divided into separate applications linked to the original application (although rules vary on this). As such, depending on the type of analysis required it can be important to trace through applications into different jurisdictions. This is where the patent family comes in.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;family-members&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://worldwide.espacenet.com/publicationDetails/inpadocPatentFamily?CC=WO&amp;amp;NR=2008024129A2&amp;amp;KC=A2&amp;amp;FT=D&amp;amp;ND=5&amp;amp;date=20080228&amp;amp;DB=EPODOC&amp;amp;locale=en_EP&#34;&gt;Family Members&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In the discussion above we noted that when a patent is filed for the first time anywhere in the world it becomes the priority filing or, as the authors tend to call it, first filing. The priority filing also becomes the &lt;strong&gt;parent&lt;/strong&gt; for any follow on publications in that country or another country (applications and grants, including administrative publications such as search reports or corrected documents). The priority filing is therefore the founder of a &lt;strong&gt;patent family&lt;/strong&gt; and the later documents are children who are &lt;strong&gt;family members&lt;/strong&gt;. Because this can lead to quite a lot of confusion let’s look at this example.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://worldwide.espacenet.com/publicationDetails/inpadocPatentFamily?CC=WO&amp;amp;NR=2008024129A2&amp;amp;KC=A2&amp;amp;FT=D&amp;amp;ND=5&amp;amp;date=20080228&amp;amp;DB=EPODOC&amp;amp;locale=en_EP&#34;&gt;Family Members&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/datafields/synthetic_genomes_WO_family.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see here that the family of our international application on synthetic genomes contains &lt;strong&gt;9 applications&lt;/strong&gt; (including the WO reference document). We can see that in some cases these 9 applications have led to more than one publication leading to a total of &lt;strong&gt;15 family members&lt;/strong&gt;. Patent publication numbers are generally accompanied by two letter codes at the end of the numbers called kind codes (e.g. A2, B1, T1 etc.) These codes in technical terms tell us about the publication level but also about the type of document involved.&lt;/p&gt;
&lt;p&gt;In some cases these are administrative republications. For example, for the Patent Cooperation Treaty (WO) kind code A3 means publication of the international search report. While this is a family member we would not want to include counts of these documents in patent statistics unless we were studying actions by patent offices.&lt;/p&gt;
&lt;p&gt;In contrast, AU is the country code for Australia and it has two documents with kind codes A1 and B2. Because patent offices vary in their use of these codes they can be difficult to accurately interpret, for more details &lt;a href=&#34;http://www.thomsonfilehistories.com/docs/RESOURCES_Kind%20Codes%20by%20Country.pdf&#34;&gt;see this list&lt;/a&gt;. In the case of Australia kind code A tells us that this was the publication of an application and kind code B tells us it was also published as a patent grant. When we scan down the list of family members we can see that there are a number of other countries with publications of type A and B.&lt;/p&gt;
&lt;p&gt;The interpretation of kind codes requires considerable care because patent office practices also vary over time. For example, prior to 2001 the United States Patent and Trademark Office only published patent documents when they were granted and did not publish patent applications. Also, until 2001 the USPTO either did not use kind codes or used kind code A. From 2001 onwards the USPTO published both applications and grants with applications receiving kind code A and grants receiving kind code B. Knowing this is critical to the calculation of patent trends because the pre-2001 data needs to be adjusted.&lt;/p&gt;
&lt;p&gt;Having said this, &lt;strong&gt;as a general rule of thumb&lt;/strong&gt;, and with the exception of the United States prior to 2001, kind code A can be taken to mean an application and kind code B as a patent grant. While we would emphasise that this is not entirely satisfactory it is the best proxy available for counting data across countries until patent offices adopt more uniform practices. However, where dealing with a single country it is better to explore the significance of each type of code.&lt;/p&gt;
&lt;p&gt;Patent family data thus provides us with a route to identifying all other documents that are linked to an original first filing. Through an understanding of patent families we can also make progress in distinguishing between patent applications and patent grants (although this is imperfect) for the purpose of developing statistics. While this is satisfactory for developing analysis of patent trends, for other purposes we would want to explore other information (such as whether a patent grant is being maintained… see below).&lt;/p&gt;
&lt;p&gt;When working with patent data there are a variety of patent family types. For example, the EPO Documentation Database (DOCDB) is the central source of most patent data and has a DOCDB family system. In addition, the International Patent Documentation Centre (INPADOC), now part of the EPO, established the widely used INPADOC system. &lt;a href=&#34;http://ep.espacenet.com/help?topic=patentfamily&amp;amp;method=handleHelpTopic&amp;amp;locale=en_ep&#34;&gt;espacenet database families&lt;/a&gt; are a little bit different to INPADOC families. In addition Thomson Reuters uses the Thomson system. For an important in depth discussion of patent families in relation to patent statistics see the excellent OECD STI Working Paper by Catalina Martinez (2010) &lt;a href=&#34;http://www.oecd.org/sti/inno/44604939.pdf&#34;&gt;Insight Into Different Types of Patent Families&lt;/a&gt;. This basically demonstrates that DOCDB families are a little smaller than INPADOC families. Thomson families tend to be ignored in patent statistics because they are limited to commercial users of the Thomson platforms. That doesn’t mean you shouldn’t use them, but that if developing work on patent trends that others can follow then DOCDB or INPADOC families makes much more sense. In the author’s work we tend to always use INPADOC family data where it is available.&lt;/p&gt;
&lt;p&gt;By now, this may sound rather complicated. In practice, it isn’t. A very simple way of understanding a patent family is as follows.
&lt;strong&gt;A patent family is a stack of documents with the parent (priority) at the bottom of the stack.&lt;/strong&gt; Those documents may have been published in multiple countries and in different languages but because they link back to the same parent (priority) they are members of its family.&lt;/p&gt;
&lt;p&gt;This simple approach to understanding a patent family is also very useful when thinking about what to count.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;When we develop counts based on a &lt;strong&gt;patent families&lt;/strong&gt; we are counting the first filings of a patent application and nothing else. That is, the document at the bottom of each stack.&lt;/li&gt;
&lt;li&gt;When we count &lt;strong&gt;patent family members&lt;/strong&gt; we are counting all documents that link to the patent family as its parent. That is, all documents in the stack.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The above approach will allow you to successfully count thousands or millions of patent documents in a way that makes sense to you and to others. However, bear in mind that in some cases a patent may have more than one priority parent. This appears to be particularly true for software patents. That is we are confronted by a “many to many” relationship rather than a simpler “one to many” relationship between the priority documents and family members. Additional measures would therefore be needed to develop counts to address this aspect of the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cited&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://worldwide.espacenet.com/publicationDetails/citedDocuments?CC=WO&amp;amp;NR=2008024129A2&amp;amp;KC=A2&amp;amp;FT=D&amp;amp;ND=5&amp;amp;date=20080228&amp;amp;DB=EPODOC&amp;amp;locale=en_EP&#34;&gt;Cited&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Patent applicants can be said to be “standing on the shoulders of giants” to borrow from the work of the economist &lt;a href=&#34;http://socrates.berkeley.edu/~scotch/giants.pdf&#34;&gt;Suzanne Scotchmer&lt;/a&gt; on patents. For our purposes, judgements about novelty and inventive step during examination are based on assessments of the existing patent literature (what others have previously claimed) and what is called the Non-Patent Literature (NPL), including scientific publications and other materials constituting “prior art”. The existence of prior art may mean that a patent application cannot proceed or that applicants will need to limit what they claim to the aspects of the invention that do not exist in the prior art. This information is recorded in the Cited documents field in databases such as espacenet.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/datafields/synthetic_genomes_WO_cited.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case the cited documents include a 2003 US grant &lt;a href=&#34;http://worldwide.espacenet.com/publicationDetails/biblio?DB=EPODOC&amp;amp;II=0&amp;amp;ND=7&amp;amp;adjacent=true&amp;amp;locale=en_EP&amp;amp;FT=D&amp;amp;date=20030218&amp;amp;CC=US&amp;amp;NR=6521427B1&amp;amp;KC=B1&#34;&gt;US6521427B1&lt;/a&gt; to Egea Biosciences with a priority filing in 1997 related to oligonucleotide synthesis for “the assembly of genes and genomes of completely synthetic artificial organisms” using computer directed gene synthesis. In addition, cited documents include cited literature (awarded an XP code in espacenet) that in two cases originate from the inventors of the synthetic genomes application.&lt;/p&gt;
&lt;p&gt;Cited patent and non-patent literature may have two sources.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Information provided by the applicants&lt;/li&gt;
&lt;li&gt;Documents identified by examiners during search and/or examination.
In some countries, applicants are required to provide detailed information on prior art relevant to the claimed invention. In other cases the requirement is weaker. As we might expect, applicants will be reluctant to disclose information that invalidates or greatly complicates their efforts to secure a patent. In addition, examiners will also perform searches to identify relevant art but requirements on examiners to actually disclose that information may also vary. For discussion see work by Colin Webb and colleagues at the OECD &lt;a href=&#34;http://www.oecd.org/sti/35520805.pdf&#34;&gt;here&lt;/a&gt; and the &lt;a href=&#34;http://www.oecd.org/sti/inno/oecdpatentstatisticsmanual.htm&#34;&gt;2009 OECD Patent Statistics Manual&lt;/a&gt;, notably Chapter 6. Citations added by examiners are generally speaking more important than those added by applicants and in some cases may be marked in patent databases. In this particular case we could find additional information on the origin of the citations by looking at the original document for the publication of the international search report (A3 document) mentioned above in &lt;a href=&#34;http://worldwide.espacenet.com/publicationDetails/originalDocument?FT=D&amp;amp;date=20081009&amp;amp;DB=EPODOC&amp;amp;locale=en_EP&amp;amp;CC=WO&amp;amp;NR=2008024129A3&amp;amp;KC=A3&amp;amp;ND=7&#34;&gt;WO2008024129A3&lt;/a&gt;. As we can see this contains the front page and then a set of citations accompanied by a category where the entry marked X for the patent grant to Egea Biosciences is judged to affect the claims to novelty and/or inventive step when taken on its own.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In practice, applicants may use citations to adjust their claims and as such citations are not necessarily an obstacle to obtaining a patent grant (as we have seen in the patent family data). However, depending on our purpose, citation data is very useful in patent analysis.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;It makes it possible to gather relevant patent data that might have been missed because of the limitations of a particular search query. It thus helps to complete the picture for a patent landscape analysis or searches of relevant prior art.&lt;/li&gt;
&lt;li&gt;For academic research it can display the activity that influenced the emergence of a particular field such as synthetic biology.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When reviewing cited patent and non-patent literature note that a cited patent document (which may be an application or a grant) may not fall directly into the field of invention of interest. For example, a particular feature of a claimed invention in one technology field (such as military optics) may affect developments in another field (such as medical optics) or be apparently completely unrelated except for a specific technical aspect.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;citing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://worldwide.espacenet.com/publicationDetails/citingDocuments?CC=WO&amp;amp;NR=2008024129A2&amp;amp;KC=A2&amp;amp;FT=D&amp;amp;ND=5&amp;amp;date=20080228&amp;amp;DB=EPODOC&amp;amp;locale=en_EP&#34;&gt;Citing&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Citing data is the opposite of cited data. One useful way to think of this is that cited data means back citations while citing data means forward citations. Citing data or forward citations are later patent applications that cite our reference document as follows.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/datafields/synthetic_genomes_WO_citing.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case there are two citing documents at the time of writing. One is from UK applicant Discuva for bacterial engineering &lt;a href=&#34;http://worldwide.espacenet.com/publicationDetails/biblio?DB=EPODOC&amp;amp;II=0&amp;amp;ND=8&amp;amp;adjacent=true&amp;amp;locale=en_EP&amp;amp;FT=D&amp;amp;date=20140515&amp;amp;CC=WO&amp;amp;NR=2014072697A1&amp;amp;KC=A1&#34;&gt;WO2014072697A1&lt;/a&gt;. A second is from Synthetic Genomics (a commercial arm of the J. Craig Venter Institute) for methods for cloning and manipulating genomes with some of the same inventors listed on the application &lt;a href=&#34;http://worldwide.espacenet.com/publicationDetails/biblio?DB=EPODOC&amp;amp;II=2&amp;amp;ND=8&amp;amp;adjacent=true&amp;amp;locale=en_EP&amp;amp;FT=D&amp;amp;date=20110909&amp;amp;CC=WO&amp;amp;NR=2011109031A1&amp;amp;KC=A1&#34;&gt;WO2011109031A1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Forward citations provide information on the applicants who are being affected by a particular patent application or grant or, on a larger scale, by sets of documents. This data can be used in a strategic way to identify others working in a particular field that is ‘close’ to a company or university’s area of interest. This information could inform decisions on building potential alliances or, in other circumstances, it may inform decisions on infringement proceedings.&lt;/p&gt;
&lt;p&gt;In broader terms, forward citing data can inform patent landscape analysis on the development of a particular field (such as synthetic biology), while bearing in mind that patent activity in one field may have spill over effects in other apparently unrelated areas of technology.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;legal-status&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;http://worldwide.espacenet.com/publicationDetails/inpadoc?CC=WO&amp;amp;NR=2008024129A2&amp;amp;KC=A2&amp;amp;FT=D&amp;amp;ND=5&amp;amp;date=20080228&amp;amp;DB=EPODOC&amp;amp;locale=en_EP&#34;&gt;Legal Status&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;As discussed above, patent kind codes at the end of publication numbers provide an indication of the publication level and type of patent document. In cases where this involves specific kind codes (e.g. B) this is often an indicator of a patent grant. However, to gain additional insights we need to review the legal status data as below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/datafields/synthetic_genomes_WO_legalstatus.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case the most obvious point about the data is that it informs us that the application is entering the national phase in a number of different countries. That is the applicants are pursuing the application in specific countries listed in the front page (above) under the Designated States field (all Contracting States to the PCT are listed by default). As such, the applicants are signalling their intention to pursue patent grants in these countries. In other case, the legal status data may indicate that an application has been rejected, that a granted patent has lapsed due to failure to pay fees or has expired. Additional information may be obtained through the interpretation of legal status codes with more information available by downloading the &lt;a href=&#34;http://www.epo.org/searching/data/data/tables/regular.html&#34;&gt;Categorisation of recently used legal status codes&lt;/a&gt; from the EPO website.&lt;/p&gt;
&lt;p&gt;When reviewing legal status data note that it may not be recent or complete. For this reason investigation at the national level (and consultation with a patent professional) will generally be necessary to determine what is happening with a particular application or grant.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;patent-registers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Patent Registers&lt;/h2&gt;
&lt;p&gt;Additional information on a patent document is typically available by consulting patent registers on the national or regional level. In the case of European level applications, more information is typically available through the EP Register button on the front page. If we select this for our WO document we will be taken to the EP Register entry for the European family member &lt;a href=&#34;https://register.epo.org/application?number=EP06851474&amp;amp;tab=main&#34;&gt;EP1968994&lt;/a&gt;. As below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/datafields/synthetic_genomes_register1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
From this information we can see that no opposition to the patent application was filed within the time limit. We then see that the most recent event is a lapse of a patent in Ireland (IE) along with the publication history. If we scroll down the page more information becomes available.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/datafields/synthetic_genomes_register2.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case we gain access to information on the written communication history between applicants and the EPO along with details of the payment of patent renewal fees. Out of sight on this image is citation data including the DOIs of cited literature that will link directly to the article concerned where available.&lt;/p&gt;
&lt;p&gt;On the menu to the left additional information is available. The federated register menu provides access to the national patent registers of designated contracting states under the European Patent Convention as can be seen &lt;a href=&#34;https://register.epo.org/application?number=EP06851474&amp;amp;lng=en&amp;amp;tab=federated&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Finally, the menu item &lt;a href=&#34;https://register.epo.org/application?number=EP06851474&amp;amp;lng=en&amp;amp;tab=doclist&#34;&gt;All Documents&lt;/a&gt; provides access to copies of available correspondence and other documents that can be downloaded as a Zip archive. It is also possible to submit a third party observation using the &lt;a href=&#34;https://register.epo.org/application?number=EP06851474&amp;amp;lng=en&amp;amp;tab=doclist&#34;&gt;submit observations button&lt;/a&gt; in the menu.&lt;/p&gt;
&lt;p&gt;Data within the register can be particularly useful for exploring the history and status of an application such as the modification of patent claims in light of search reports. It is also very useful for identifying and reviewing opposition to a particular application.&lt;/p&gt;
&lt;p&gt;Within Europe it is quite easy to consult register detail. To assist with accessing registry information in other countries WIPO has recently launched a &lt;a href=&#34;http://www.wipo.int/branddb/portal/portal.jsp&#34;&gt;Patent Register Portal&lt;/a&gt; to simplify the task of locating the patent register in countries of interest.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round Up&lt;/h2&gt;
&lt;p&gt;In this chapter we have walked through some of the most important patent data fields using a single example and the espacenet database. As can now be appreciated a basic understanding of patent data fields opens ups a lot of additional information about a single document of interest.&lt;/p&gt;
&lt;p&gt;These basic fields are also the building blocks for sophisticated patent analysis. In future articles we will focus on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Retrieving data with these fields&lt;/li&gt;
&lt;li&gt;Cleaning up the data in these fields&lt;/li&gt;
&lt;li&gt;Mapping Trends&lt;/li&gt;
&lt;li&gt;Network Mapping&lt;/li&gt;
&lt;li&gt;Geographic Mapping&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Cleaning Patent Data with Open Refine</title>
      <link>/openrefine-patent-cleaning/</link>
      <pubDate>Sat, 02 May 2015 00:00:00 +0000</pubDate>
      
      <guid>/openrefine-patent-cleaning/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Cleaning patent data is one of the most challenging and time consuming tasks involved in patent analysis. In this chapter we will cover.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Basic data cleaning using Open Refine&lt;/li&gt;
&lt;li&gt;Separating a patent dataset on applicant names and cleaning the names.&lt;/li&gt;
&lt;li&gt;Exporting a dataset from Open Refine at different stages in the cleaning process.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;http://openrefine.org&#34;&gt;Open Refine&lt;/a&gt; is an open source tool for working with all types of messy data. It started life as Google Refine but has since migrated to Open Refine. It is a programme that runs in a browser on your computer but does not require an internet connection. It is a key tool in the open source patent analysis tool kit and includes extensions and the use of custom code for particular custom tasks. In this article we will cover some of the basics that are most relevant to patent analysis and then move on to more detailed work to clean patent applicant names.&lt;/p&gt;
&lt;p&gt;The reason that patent analysts should use Open Refine is that it is the easiest to use and most efficient free tool for cleaning patent data without programming knowledge. It is far superior to attempting the same cleaning tasks in Excel or Open Office. The terminology that is used can take some getting used to, but it is possible to develop efficient workflows for cleaning and reshaping data using Open Refine and to create and reuse custom codes needed for specific tasks.&lt;/p&gt;
&lt;p&gt;In this chapter we use Open Refine to clean up a raw dataset from WIPO Patentscope containing nearly 10,000 raw records that make some kind of reference to the word &lt;code&gt;pizza&lt;/code&gt; in the whole text. To follow this chapter using one of our training sets, download it from the Github repository &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium/pizza_medium.csv?raw=true&#34;&gt;here&lt;/a&gt; or use your own dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;install-open-refine&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Install Open Refine&lt;/h2&gt;
&lt;p&gt;To install Open Refine visit the Open Refine &lt;a href=&#34;http://openrefine.org&#34;&gt;website&lt;/a&gt; and &lt;a href=&#34;http://openrefine.org/download.html&#34;&gt;download&lt;/a&gt; the software for your operating system:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/OpenRefine_front.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the download page select your operating system. Note the extensions towards the bottom of the page for future reference.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/OpenRefine-download.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At the time of writing, when you download Open Refine it actually downloads and installs as Google Refine (reflecting its history) and so that is the application you will need to look for and open.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-a-project&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Create a Project&lt;/h2&gt;
&lt;p&gt;We will use the Patentscope Pizza Medium file that can be downloaded from the repository &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium/pizza_medium.csv?raw=true&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/create_project.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The file will load and then attempt to guess the column separator. Choose &lt;code&gt;.csv.&lt;/code&gt; Note that a wide range of files can be imported and that there are additional options such as storing blank cells as nulls that are selected by default. In the dataset that you will load we have prefilled blank cells with &lt;code&gt;NA&lt;/code&gt; values to avoid potential problems using fill down in Open Refine discussed below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/create_project2.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Click &lt;code&gt;create project&lt;/code&gt; in the top right of the bar as the next step.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;open-refine-basics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Open Refine Basics&lt;/h2&gt;
&lt;p&gt;A few basic features of Open Refine will soon have you working smoothly. Here is a quick tour.&lt;/p&gt;
&lt;div id=&#34;open-refine-runs-in-a-browser&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Open Refine runs in a browser&lt;/h3&gt;
&lt;p&gt;Open Refine is an application that lives on your computer but runs in a browser. However, it does not require an internet connection and it does not lose your work if you close the browser.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;open-refine-works-on-columns.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Open Refine works on columns.&lt;/h3&gt;
&lt;p&gt;At the top of each column is a pull down menu. Be ready to use these menus quite a lot. In particular, you will often use the &lt;code&gt;Edit cells &amp;gt; Common tranforms&lt;/code&gt; as shown below for functions such as trimming white space.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/menu_basics.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Other important menus are &lt;code&gt;Edit column&lt;/code&gt;, immediately below &lt;code&gt;Edit cells&lt;/code&gt;, for copying or splitting columns into new columns.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;open-refine-works-with-facets.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Open Refine works with Facets.&lt;/h3&gt;
&lt;p&gt;The term &lt;code&gt;facet&lt;/code&gt; may initially be confusing but basically calls up a window that arranges the items in a column for inspection, sorting, and editing as we can see below. This is important because it becomes possible to identify problems and address them. It also becomes possible to apply a variety of clustering algorithms to clean up the data. Note that the size of the facet window can be adjusted by dragging the bottom of the window as we have done in this image.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/facet_menu.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hovering over an item inside the facet window brings up a small &lt;code&gt;edit&lt;/code&gt; button that allows editing, such as removing &lt;i&gt; and &lt;/i&gt; from the title.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;custom-facets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Custom Facets&lt;/h3&gt;
&lt;p&gt;The Facet menu below brings up a custom menu with a range of options. Selecting &lt;code&gt;Custom text facet&lt;/code&gt; (see below) brings up a pop up that allows for the use of code in &lt;a href=&#34;https://github.com/OpenRefine/OpenRefine/wiki/GREL-Functions&#34;&gt;Open Refine Expression Language (GREL)&lt;/a&gt; to perform tasks not covered by the main menu items. This language is quite simple and can range from short snippets for finding and replacing text to more complex functions that can be reused in future. We will demonstrate the use of this function below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/facet_menu.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reordering-columns&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reordering Columns&lt;/h3&gt;
&lt;p&gt;There are two options for reordering columns. The first is to select the column menu then &lt;code&gt;Edit column &amp;gt; Move column to beginning&lt;/code&gt;. The second option, shown below, is to select the &lt;code&gt;All&lt;/code&gt; drop down menu in the first column and then &lt;code&gt;Edit columns &amp;gt; Re-order/remove columns&lt;/code&gt;. In the pop up menu of fields drag the desired field to the top of the list. In this case we have dragged the &lt;code&gt;priority_date&lt;/code&gt; column to the top of the list. It will now appear as the first data column.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/facet_menu.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;undo-and-redo&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Undo and Redo&lt;/h3&gt;
&lt;p&gt;Open Refine keeps track of each action and allows you to go back several steps or to the beginning. This is particularly helpful when testing whether a particular approach to cleaning (e.g. splitting columns or using a snippet of code) will meet your needs. In particular it means you can explore and test approaches while not worrying about losing your previous work.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/undo_redo.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, it can be important to plan the steps in your clean up operation to avoid problems at later stages. It may help to use a notepad as a checklist (see below). The main issue that can arise is where cleanup moves forward several steps without being fully completed in an earlier step. In some cases this can require returning to that earlier step, restarting and repeating earlier steps. As you become more familiar with Open Refine it will be easier to work out an appropriate sequence for your workflow.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exporting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exporting&lt;/h3&gt;
&lt;p&gt;When a clean up exercise is completed a file can be exported in a variety for formats. When working with patent data expect to create more than one file (e.g. core, applicants, inventors, IPC) to allow for analysis of aspects of the data in other tools. In this chapter we will create two files.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A cleaned version of the original data&lt;/li&gt;
&lt;li&gt;An applicants file that separates the data by each applicant.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-cleaning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basic Cleaning&lt;/h2&gt;
&lt;p&gt;This is the first step in working with a dataset and it will make sense to perform some basic cleaning tasks before going any further. The Pizza Medium dataset that we are working with in this article is raw in the sense that the only cleaning so far has been to remove the two empty rows at the head of the data table and to fill blank cells with NA values. The reason that it makes sense to do some basic cleaning before working with applicant, inventor or IPC data is that new datasets will be generated by this process.&lt;/p&gt;
&lt;p&gt;Bear in mind that Open Refine is not the fastest programme and make sure that you allocate sufficient time for the clean up tasks and are prepared to be patient while the programme runs algorithms to process the data. Note that Open Refine will save your work and you can return to it later.&lt;/p&gt;
&lt;p&gt;When working with Open Refine we will typically be working on one column at a time. However, the key &lt;code&gt;checklist&lt;/code&gt; for cleaning steps is:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Make sure you have a back up of the original file. Creating a &lt;code&gt;.zip&lt;/code&gt; file and marking it with the name &lt;code&gt;raw&lt;/code&gt; can help to preserve the original.&lt;/li&gt;
&lt;li&gt;Open and save a text file as a &lt;code&gt;code book&lt;/code&gt; to write down the steps taken in cleaning the data (e.g. pizza_codebook.txt).&lt;/li&gt;
&lt;li&gt;Regularise characters (e.g. title, lowercase, uppercase).&lt;/li&gt;
&lt;li&gt;Remove leading and trailing white space.&lt;/li&gt;
&lt;li&gt;Address encoding and related problems.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Additional actions:&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Transform dates&lt;/li&gt;
&lt;li&gt;Access additional information and create new columns and/or rows.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will generally approach these tasks in each column and steps 6 and 7 will not always apply. The creation of a codebook will allow you to keep a note of all the steps taken to clean up a dataset. The codebook should be saved with the cleaned dataset (e.g. in the same folder) as a reference point if you need to do further work or if colleagues want to understand the transformation steps.&lt;/p&gt;
&lt;div id=&#34;changing-case&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Changing case&lt;/h3&gt;
&lt;p&gt;The first column in our Patentscope pizza dataset is the publication number. To inspect what is happening and needs to be cleaned in this column we will first select the column menu and choose &lt;code&gt;text facet&lt;/code&gt; from the dropdown. This will generate the side menu panel that we can see below containing the data. We can then inspect the column for problems.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/cleaning_pubno_createfacet.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When we scroll down the side panel we can see that some publication numbers have a lowercase country code (in this case &lt;code&gt;ea&lt;/code&gt; rather than &lt;code&gt;EA&lt;/code&gt; for Eurasian Patent Organization using the &lt;a href=&#34;http://www.wipo.int/pct/guide/en/gdvol1/annexes/annexk/ax_k.pdf&#34;&gt;WIPO standardized country codes&lt;/a&gt;). To address this we select the column menu &lt;code&gt;Edit cells &amp;gt; Common transforms &amp;gt; to Uppercase&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/cleaning_pubno_totitlecase.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we scroll down all the publication numbers will have been converted to upper case. This will make it easier to extract the publication country codes at a later stage.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regularise-case&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regularise case&lt;/h3&gt;
&lt;p&gt;For the other text columns it is sensible to repeat the common transformations step and select &lt;code&gt;to titlecase&lt;/code&gt;. Note that this will generally work well for the title field but may not always work as well on concatenated fields such as applicants and inventor names. Repeat this step following the separation of these concatenated fields (see below on applicants). If the abstract or claims were present we would not regularise those text fields.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;remove-leading-and-trailing-whitespace&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Remove leading and trailing whitespace&lt;/h3&gt;
&lt;p&gt;To remove leading and trailing white space in a column we select &lt;code&gt;Edit cells &amp;gt; Common transforms &amp;gt; Trim&lt;/code&gt; leading and trailing white space across the columns.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/title_trailingwhitespace.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that following the splitting of concatenated cells with multiple entries such as the applicants and the inventors fields it is a good idea to repeat the trim exercise when the process is complete to avoid potential leading white space at the start of the new name entries.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;add-columns&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Add Columns&lt;/h3&gt;
&lt;p&gt;We can also add columns by selecting the column menu and &lt;code&gt;Edit Column &amp;gt; Add column based on this column&lt;/code&gt;. In this case we have added a column called publication_date.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/add_column_publicationyear.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have a number of options with respect to dates (see below). In this case we want to separate out the date information into separate columns. To do that we can use &lt;code&gt;Edit column &amp;gt; Split into several columns&lt;/code&gt;. We can also choose the separator for the split, in this case &lt;code&gt;.&lt;/code&gt; and whether to keep or delete the source column. In this case we selected to retain the original column and created three new date related columns. We could as necessary then delete the date and month column if we only needed the year field.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/split_date.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can then rename these columns using the edit &lt;code&gt;Edit Column &amp;gt; Rename this column&lt;/code&gt;. Note that in this case the use of lowercase and underscores marks out columns we are creating or editing as a flag for internal use informing us that this is a column that we have created. At a later stage we will rename the original fields to mark them as original.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/rename_column.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;address-encoding-and-related-problems&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Address Encoding and related problems&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/OpenRefine/OpenRefine/wiki/Recipes&#34;&gt;Recipes&lt;/a&gt; section of the documentation provides helpful tips and example code for dealing with encoding and related problems that are reproduced here:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;corrupted characters. This arises from aggregation of data from different sources. In Patentscope the data is converted to UTF8. However, if problems are encountered select &lt;code&gt;Edit cells &amp;gt; Transform&lt;/code&gt; and try entering the following.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;value.reinterpret(&amp;quot;utf-8&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It may be necessary to explore and test other sets which can be identified &lt;a href=&#34;http://java.sun.com/j2se/1.5.0/docs/guide/intl/encoding.doc.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Escape html/XML characters e.g. &amp;amp;amp&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The most likely source of patent data is XML but to be on the safe side the following should escape (remove) html and XML code appearing in the text.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;value.unescape(&amp;quot;html&amp;quot;).unescape(&amp;quot;xml&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Question marks&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Question marks often show up for characters that cannot be represented are a sign of encoding problems. In addition non-breaking spaces may be represented as &lt;code&gt;&amp;amp;nbsp&lt;/code&gt; (Unicode(16)). To find a Unicode value go to &lt;code&gt;Edit cells &amp;gt; Transform&lt;/code&gt; and then enter unicode(value) which will transform all the characters to unicode numbers. From there you can look up the problem.&lt;/p&gt;
&lt;p&gt;A quick fix is proposed in the documentation that may work in some circumstances.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;split(escape(value,&amp;#39;xml&amp;#39;),&amp;quot;&amp;amp;#160;&amp;quot;)[0]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Within this particular dataset, we found that these quick tips did not work (presumably because the text had already been converted to UTF-8). However, if all else fails an alternative is to simply find and replace in Transform as in the example below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/replace_encoding.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is not a very satisfactory solution because it requires inspection of the dataset to identify the specific character problems and then replacing the value. That will be time consuming.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reformatting-dates&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reformatting dates&lt;/h3&gt;
&lt;p&gt;One problem we may encounter is that the standard date definition on patent documents (e.g. 21.08.2009) may not be recognised as a date field in our analysis software because dates can be ambiguous from the perspective of software code. For example, how should 08/12/2009 or 12/08/2009 be interpreted?&lt;/p&gt;
&lt;p&gt;Alternatively, as in this case, the decimal points may not be correctly interpreted as signifying a date in some software (e.g. R). We might anticipate this and transform the data into a more recognisable form such as 21/08/2009. A very simple way to do this is by using a replace function. In this case we select the menu for the &lt;code&gt;publication_date&lt;/code&gt; field and then &lt;code&gt;Edit cells &amp;gt; Transform&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/transform.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This produces a menu where we enter a simple GREL replace code.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/transform_replace_date.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;replace(value, &amp;#39;.&amp;#39;, &amp;#39;/&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This simple replace code is basically the same as find and replace in Excel or Open Office. In addition, we can see the consequences of the choice in the panel before we run the command. This is extremely useful for spotting problems. For example, if attempting to split a field on a comma we may discover that there are multiple commas in a cell (see the &lt;code&gt;Priority Data&lt;/code&gt; field for this). By testing the code in the panel we could then work to find a solution or edit the offending texts in the main facet panel.&lt;/p&gt;
&lt;p&gt;To find other simple codes visit the &lt;a href=&#34;https://github.com/OpenRefine/OpenRefine/wiki/Recipes&#34;&gt;Open Refine Recipes page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We will now focus on extracting information from some of the columns before saving the dataset and moving on.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;access-additional-information&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Access additional information&lt;/h3&gt;
&lt;p&gt;There are a range of pieces of information that are hidden in data inside columns. For example, Patentscope data does not contain a publication country field. In particular, note that Patentscope merges all publications for an application record into one dossier. So we are only seeing one record for a set of documents (in Patentscope the wider dossier for a record is accessible through the Documents section of the website). This is very helpful in reducing duplication but it is important to bear in mind that we are not seeing the wider family in our data table. However, we can work with the information at the front of the publication number in the Patentscope records using a very simple code and create a new column (as above) based on the values returned as below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/publication_country.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;substring(value, 0, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note here that the code begins counting from 0 (e.g. 0, 1 = U, 2 = S). The first part of the code looks in the value field. 0 tells the code to begin counting from 0 and the 2 tells it to read the two characters from 0. We could change these values, e.g to 1 and 4 to capture only a chunk of a number.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;fill-blank-cells&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fill blank cells&lt;/h2&gt;
&lt;p&gt;Filling blank cells with a value (NA for Not Available) to prevent calculation problems with analysis tools later can be performed by selecting each column, creating a text facet, scrolling down to the bottom of the facet choosing &lt;code&gt;(blank)&lt;/code&gt;, edit and then entering NA for the value.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/blank_na_facets.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that this is somewhat time consuming (until a more rapid method is found) but has the benefit of being accurate. It is generally faster to open the file in Excel (or Open Office) and use find and replace with the find box left blank and NA in the replace field across the data table. For that reason, blank cells should not appear in the dataset you are using in this article. However, in later steps below we will be generating blank cells by splitting the applicant and inventor field. It is therefore important to know this procedure.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;renaming-columns&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Renaming columns&lt;/h2&gt;
&lt;p&gt;At this stage we have a set of columns that are mixed between the original sentence case and the additions in lower case with no spaces such as &lt;code&gt;publication_number&lt;/code&gt;. This is a matter of personal preference but it is generally a good idea to regularise the case of all columns to make them easy to remember. In this case we will also add the word &lt;code&gt;original&lt;/code&gt; to the columns to distinguish between those created by cleaning the data and those we have created.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exporting-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exporting Data&lt;/h2&gt;
&lt;p&gt;When we are happy that we have worked through the core cleaning steps it is a good idea to export the new core dataset. It is important to do this before the steps described below because it preserves a copy of the core dataset that can be used for separation (or splitting activities) on applicants, inventors, IPC etc. during the next steps. It is important that this &lt;code&gt;clean&lt;/code&gt; dataset is as clean as is reasonably possible before moving on. The reason for this is that &lt;strong&gt;&lt;em&gt;any noise or problems will multiply&lt;/em&gt;&lt;/strong&gt; when we move on to the next steps. This may require a major rerun of the cleaning steps on later files created for applicants or inventors. Therefore make sure that you are happy that the data is as clean as is reasonably possible at this stage. Then choose export from the menu and the desired format (preferably &lt;code&gt;.csv&lt;/code&gt; or &lt;code&gt;.tab&lt;/code&gt; if using analytics tools later).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/export_core.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;splitting-applicants&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Splitting Applicants&lt;/h2&gt;
&lt;p&gt;This article was inspired by a &lt;a href=&#34;http://www.patinformatics.com/blog/patent-assignee-cleanup-using-google-refine-open-refine-text-facets-and-clustering/&#34;&gt;very useful tutorial&lt;/a&gt; on cleaning assignee names with Google Refine by Anthony Trippe. Anthony is also the author of the forthcoming &lt;a href=&#34;http://www.wipo.int/patentscope/en/programs/patent_landscapes/&#34;&gt;WIPO Guidelines for Preparing Patent Landscape Reports&lt;/a&gt; and the &lt;a href=&#34;http://www.patinformatics.com&#34;&gt;Patentinformatics LLC&lt;/a&gt; website has played a pioneering role in promoting patent analytics. We will take this example forward using the our sample pizza patent dataset so that it can be visualised in a range of tools. If you have not done so already, you can download the dataset &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium/pizza_medium.csv?raw=true&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We actually have two options here and we will go through them so that you can work out your needs in a particular situation. Note that you can use Undo in Google Refine to go back to the point immediately before you tested these approaches. However, if you have followed the data cleaning steps above, make sure that you have already exported a copy of the main dataset.&lt;/p&gt;
&lt;div id=&#34;situation-1---first-applicants&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Situation 1 - First Applicants&lt;/h3&gt;
&lt;p&gt;As discussed by Anthony Tripp we could split the applicant column into separate columns by choosing, &lt;code&gt;Applicants &amp;gt; Edit column &amp;gt; Split into several columns&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/split_applicants_columns.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We then need to select the separator. In this case (and normally with patent data), it is &lt;code&gt;;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/split_selectseparator.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This will produce a set of 18 columns.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/split_columns.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At this point, we could begin the clustering process to start cleaning the names that is discussed in situation 2. However, the disadvantage with this is that with this size of dataset we would need to do this 18 times in the absence of an easy way of combining the columns into a single column (applicants) with a name on each row. We might want to use this approach in circumstances where we are not focusing on the applicants and are happy to accept the first name in the list as the first applicant. In that case we would simply be reducing the applicant field to one applicant. Bear in mind that the first applicant listed in the series of names may not always be the first applicant as listed on an application and may not be an organisation name. Bearing these caveats in mind, we might also use this approach to reduce the concatenated inventors field to one inventor. For general purposes that would be clean and simple for visualisation purposes.&lt;/p&gt;
&lt;p&gt;However, if we wanted to perform detailed applicant analysis for an area of technology, we would need to adopt a different approach.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;situation-2---all-applicants&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Situation 2 - All Applicants&lt;/h3&gt;
&lt;p&gt;One of the real strengths of Open Refine is that it is very easy to separate applicant and inventor names into individual rows. Instead of choosing Edit column we now choose &lt;code&gt;Edit cells&lt;/code&gt; and then &lt;code&gt;split multi-valued cells&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/split_multivaluedcells.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the pop up menu choose &lt;code&gt;;&lt;/code&gt; as the separator rather than the default comma.&lt;/p&gt;
&lt;p&gt;We now have a dataset with 15,884 rows as we can see below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/split_multivaluedcells2.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The advantage of this is that all our individual applicant names are now in a single column. However, note that the rest of the data has not been copied to the new rows. We will come back to this but as a precaution it is sensible to fill down on the publication number column as the key that links the individual applicants to the record. So let’s do that for peace of mind by selecting &lt;code&gt;publication number &amp;gt; edit cells &amp;gt; fill down&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/split_filldown.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We should now have a column filled with the publication number values for each applicant as our key. Note here that there is a need for caution in using &lt;code&gt;fill down&lt;/code&gt; in Open Refine as discussed in detail &lt;a href=&#34;http://googlerefine.blogspot.co.uk/2012/03/fill-down-right-and-secure-way.html&#34;&gt;here&lt;/a&gt;. Basically, fill down is not performed by record, it simply fills down. That can mean that data becomes mixed up. This is another reason why it is important to fill blank values with NA either before starting work in Open Refine or as one of the initial clean up steps. Using NA early on will help prevent refine from filling down blank cells with the values of another record.&lt;/p&gt;
&lt;p&gt;If you have not already done so above to assist the clean up process, and as general good practice, transform the mixed case in the applicants field to a single case type. To do that select &lt;code&gt;Applicants &amp;gt; Edit Cells &amp;gt; Common Transformations &amp;gt; To titlecase&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/applicants_titlecase.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We now move back to the applicants and select &lt;code&gt;Facet &amp;gt; Text Facet&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/applicants_textfacet.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What we will see (with Applicants moved to the first column by selecting &lt;code&gt;Applicants &amp;gt; Edit Column &amp;gt; Move column to beginning&lt;/code&gt;) is a new side window with 9,368 choices.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/applicants1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The cluster button will trigger a set of six clean up algorithms with user choices along the way. It is well worth reading the &lt;a href=&#34;https://github.com/OpenRefine/OpenRefine/wiki/Clustering-In-Depth&#34;&gt;documentation&lt;/a&gt; on these steps to decide what will best fit your needs in future. These cleaning steps proceed from the strict to the lax in terms of matching criteria. The following is a brief summary of the details provided in the documentation page:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Fingerprinting. This method is the least likely in the set to produce false positives (and that is particularly important for East Asian names in patent data). It involves a series of steps including removing trailing white space, using all lowercase, removing punctuation and control characters, splitting into tokens on white space, splitting and joining and normalising to ASCII.&lt;/li&gt;
&lt;li&gt;N-Gram Fingerprint. This is similar but uses n-grams (a sequence of characters or multiple sequences of characters) that are chunked, sorted and rejoined and normalised to ASCII text. The documentation highlights that this can produce more false positives but is food for finding clusters missed by fingerprinting.&lt;/li&gt;
&lt;li&gt;Phonetic Fingerprint. This transforms tokens into the way they are pronounced and produces different fingerprints to methods 2 &amp;amp; 3.&lt;/li&gt;
&lt;li&gt;Nearest Neighbour Methods. This is a distance method but can be very very slow.&lt;/li&gt;
&lt;li&gt;Levenshtein Distance. This famous algorithm measures the minimal number of edits that are required to change one string into another (and for this reason is widely known as edit distance). Typically, this will spot typological and spelling errors not spotted by the earlier approaches.&lt;/li&gt;
&lt;li&gt;PPM a particular use of Kolmogorov complexity as described in this &lt;a href=&#34;http://arxiv.org/abs/cs/0111054&#34;&gt;article&lt;/a&gt; that is implemented in Open Refine as &lt;code&gt;Prediction by Partial Matching&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It is important to gain an insight into these methods because they may affect the results you receive. In particular, caution is required on East Asian names where cultural naming traditions produce a lot of false positive matches on the same name for persons who are actually distinct persons (synonyms or “lumping” in the literature). This can have very dramatic impacts on the results of patent analysis for inventor names because it will treat all persons sharing the name &lt;code&gt;Smith, John&lt;/code&gt; or &lt;code&gt;Wang, Wei&lt;/code&gt; as the same person, when in practice they are multiple individual people.&lt;/p&gt;
&lt;p&gt;We will now walk through each algorithm to view the results.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/cluster_step1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This identifies 1187 clusters dominated by hidden characters in the applicants field (typically appearing after the name). At this stage we need to make some decisions about whether or not to accept or reject the proposed merger by checking the &lt;code&gt;Merge?&lt;/code&gt; boxes.&lt;/p&gt;
&lt;p&gt;This step was particularly good at producing a match on variant names and name reversals as we can see here.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/applicants2.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It pays to manually inspect the data before accepting it. One important option here is to use the slider on &lt;code&gt;Choices in Cluster&lt;/code&gt; to move the range up or down and then make a decision about the appropriate cut off point. Then use select all in the bottom left for results you are happy with followed by &lt;code&gt;Merge Selected &amp;amp; Re-Cluster&lt;/code&gt;. In the next step we can change the keying function dropdown to Ngram-fingerprint.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/applicants-ngramfingerprint2.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This produces 98 clusters which inspection suggests are very accurate. The issues to watch out for here (and throughout) are very similar names for companies that may not be the same company (such as Ltd. and Inc.) or distinct divisions of the same company. It is also important, when working with inventor names, not to assume that the same name is the same inventor in the absence of other match criteria, or that apparently minor variations in initials (e.g. Smith, John A and Smith, John B) are the same person because they may well not be.&lt;/p&gt;
&lt;p&gt;To see these potential problems in action try reducing the N-gram size to 1. At this point we see the following.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/ngram1_problems.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This measure is too lax and is grouping together companies that should not be grouped. In contrast increasing the N-gram value to 3 or 4 will tighten the cluster. We will select all on N-gram 2 and proceed to the next step.&lt;/p&gt;
&lt;p&gt;At this point it is worth noting that the original 9,368 clusters have been reduced to 7,875 and if we sort on the count in the main window then Google is starting to emerge as the top applicant across our set of 10,000 records.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;phonetic-fingerprint-metaphone-3-clustering&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Phonetic Fingerprint (Metaphone 3) clustering&lt;/h3&gt;
&lt;p&gt;As we can see below, Metaphone 3 clustering produces 413 looser clusters with false positive matches on International Business Machines but positive matches on Cooperative Verkoop.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/applicants_metaphone3.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At this point we could either manually review the 413 clusters and select as appropriate or change the settings to reduce the number of clusters using the &lt;code&gt;Choices in Clusters&lt;/code&gt; slider until we see something manageable for manual review. At this stage we might also want to use the &lt;code&gt;browse this cluster&lt;/code&gt; function that appears on hovering over a particular selection, to review the data (see the second entry in the image below).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/applicants_browsecluster.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case we are attempting to ascertain whether the Korea Institute of Oriental Medicine should be grouped with the Korea Institute of Science and Technology (which appears unlikely). If we open the browser function we can review the entries for shared characteristics as possible match criteria.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/applicants_browsecluster1.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For example, if the applicants shared inventors and/or the same title we may want to record this record in the larger grouping (remembering that we have exported the original cleaned up data). Or, as is more likely, we might want to capture most members of the group and remove the Korea Institute of Oriental Medicine. However, how to do this is not at all obvious.&lt;/p&gt;
&lt;p&gt;In practice, selection of items at this stage feeds into the next stage of cleaning using the Cologne-phonetic algorithm. As we can see below, this algorithm identified 271 clusters that were almost entirely clustered on the names of individuals with a limited number of accurate hits.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/applicants_cologne.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;levenshtein-edit-distance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Levenshtein Edit Distance&lt;/h3&gt;
&lt;p&gt;The final steps in the process focus on Nearest Neighbour matches for our reduced number of clusters. Note that this may take some time to run (e.g. 10-15 minutes for the +7,000 clusters in this case). The results are displayed below&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/openrefine/applicants_levenshtein.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In some cases the default settings matched individual names on different initials, but in the majority of cases the clusters appeared valid and were accepted. In this case particular caution is required on the names of individuals and browsing the results to check the accuracy of matches.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ppm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;PPM&lt;/h3&gt;
&lt;p&gt;The PPM step is the final logarithm but took so long that we decided to abandon it relative to the likely gains.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;preparing-for-export&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Preparing for export&lt;/h3&gt;
&lt;p&gt;In practice, the cleaning process will generate a new data table for export that focuses on the characteristics of applicants. To prepare for export with the cleaned group of applicant names there will be a choice on whether to retain the publication number as the single key, or, whether to use the fill down process illustrated above across the columns of the dataset. It is important to note that caution is required in the blanket use of fill down.&lt;/p&gt;
&lt;p&gt;It is also important to bear in mind that the dataset that has been created contains many more rows than the original version. Prior to exporting we would therefore suggest two steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Rerun Common transforms &amp;gt; to title case, to regularise any names that may have been omitted in the first round and rerun trim whitespace for any spaces arising from the splitting of names.&lt;/li&gt;
&lt;li&gt;Rerun facets on each column, select blank at the end of the facet panel and fill with NA. Alternatively, do this immediately following export.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round Up&lt;/h2&gt;
&lt;p&gt;In this chapter we have covered the main features of basic data cleaning using Open Refine. As should now be clear, while it requires an investment in familiarisation it is a powerful tool for cleaning up small to medium sized patent datasets such as our 10,000 pizza patent records. However, a degree of patience, caution and forward planning is required to create an effective workflow using this tool. It is likely that further investments of time (such as the use of regular expressions in GREL) would improve cleaning tasks prior to analysis.&lt;/p&gt;
&lt;p&gt;Open Refine is also probably the easiest to use free tool for separating and cleaning applicant and inventor names without programming knowledge. For that reason alone, while noting the caveats highlighted above, Open Refine is a very valuable tool in the open source patent analytics toolbox.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;useful-resources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Useful Resources&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;http://openrefine.org&#34;&gt;Open Refine website&lt;/a&gt; has links to lots of useful resources including video walkthroughs&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/OpenRefine/OpenRefine/wiki/Recipes&#34;&gt;Open Refine Wiki Recipes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://googlerefine.blogspot.co.uk&#34;&gt;Open Refine Tips and Tricks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://stackoverflow.com/questions/tagged/openrefine&#34;&gt;Stack Overflow questions on Open Refine&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reading and Writing an Excel File in R</title>
      <link>/reading-writing-excel-files-r/</link>
      <pubDate>Thu, 30 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>/reading-writing-excel-files-r/</guid>
      <description>&lt;p&gt;This post was updated in 2018 and you can read it &lt;a href=&#34;http://www.pauloldham.net/importing-excel-data-into-r-updated/&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;http://cran.r-project.org/doc/manuals/r-release/R-data.html#Reading-Excel-spreadsheets&#34;&gt;CRAN Project&lt;/a&gt; has the following to say about importing Excel files into R.&lt;/p&gt;
&lt;p&gt;“The first piece of advice is to avoid doing so if possible! If you have access to Excel, export the data you want from Excel in tab-delimited or comma-separated form, and use read.delim or read.csv to import it into R. (You may need to use read.delim2 or read.csv2 in a locale that uses comma as the decimal point.).”&lt;/p&gt;
&lt;p&gt;This is very sound advice. The best option when dealing with Excel is generally to use &lt;code&gt;save as&lt;/code&gt; to save the file as a .csv and then import it into R. However, there are a number of ways of reading an Excel file into R. We will deal with two of them in this walk through focusing on the patent datasets in our &lt;a href=&#34;https://drive.google.com/open?id=0B4piiKOCkRPDNThTWU1QQVYyRnM&amp;amp;authuser=0&#34;&gt;open access patent datasets folder&lt;/a&gt;. Download the GitHub .zip file &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/datasets.zip?raw=true&#34;&gt;here&lt;/a&gt;. Feel free to use your own dataset.&lt;/p&gt;
&lt;p&gt;One challenge with R and Excel files is that no one package seems to do everything that you want. In particular, reading from URLs is a bit of a minefield particularly on secure connections (&lt;code&gt;https:&lt;/code&gt;). If this walk through doesn’t meet your needs then try this R-bloggers &lt;a href=&#34;http://www.r-bloggers.com/read-excel-files-from-r/&#34;&gt;overview&lt;/a&gt; on the range of available packages. The &lt;a href=&#34;http://www.r-bloggers.com/search/excel&#34;&gt;R-bloggers excel topic listing&lt;/a&gt; also has lots of useful articles covering working with Excel in more depth than this short article. To find additional help try &lt;a href=&#34;http://stackoverflow.com/questions/tagged/r&#34;&gt;stackoverflow&lt;/a&gt;.
We will focus on:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Using the &lt;a href=&#34;http://www.r-bloggers.com/importexport-data-to-and-from-xlsx-files/&#34;&gt;xlsx&lt;/a&gt; package&lt;/li&gt;
&lt;li&gt;Testing the new &lt;a href=&#34;http://blog.rstudio.org/2015/04/15/readxl-0-1-0/&#34;&gt;readxl&lt;/a&gt; package&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To read an Excel file into R first install the package or tick the box in the Packages list to load it or load the library.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;xlsx&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Load the library&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(xlsx)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: rJava&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: xlsxjars&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can use your own local excel file but we will use the file &lt;a href=&#34;https://drive.google.com/file/d/0B4piiKOCkRPDNWhrdGxXc0YwTk0/view?usp=sharing&#34;&gt;wipotrends&lt;/a&gt; in the &lt;a href=&#34;https://drive.google.com/open?id=0B4piiKOCkRPDNWhrdGxXc0YwTk0&amp;amp;authuser=0&#34;&gt;patent dataset folder&lt;/a&gt; for this example. Other test Excel datasets in the folder are &lt;a href=&#34;https://drive.google.com/open?id=0B4piiKOCkRPDZGZ4dlJsVEN4TEk&amp;amp;authuser=0&#34;&gt;ewaste&lt;/a&gt; and &lt;a href=&#34;https://drive.google.com/open?id=0B4piiKOCkRPDMUVSaFJtdXlOX28&amp;amp;authuser=0&#34;&gt;solarcooking&lt;/a&gt;. Download the file and save it to your computer. Then copy the local file path.&lt;/p&gt;
&lt;div id=&#34;reading-a-local-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reading a local file&lt;/h2&gt;
&lt;p&gt;We will use a file called &lt;a href=&#34;https://drive.google.com/open?id=0B4piiKOCkRPDNWhrdGxXc0YwTk0&amp;amp;authuser=0&#34;&gt;wipotrends&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Let’s open the file up to inspect it briefly. We will see that it contains one worksheet and that the column headings begin at row 5. To load it into R we will use the &lt;code&gt;read.xlxs&lt;/code&gt; function and specify arguments to tell R where to look for and handle the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wipotrends &amp;lt;- read.xlsx(&amp;quot;/Users/pauloldham17inch/Desktop/open_source_master/2_datasets/wipo/wipotrends.xlsx&amp;quot;, sheetIndex = 1, startRow = 5, endRow = 23, as.data.frame = TRUE, header=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;sheetIndex = n&lt;/code&gt; tells R to import the first worksheet (working numerically). &lt;code&gt;startRow = n&lt;/code&gt; tells R where to start reading the data (if not the first row).
&lt;code&gt;endRow = n&lt;/code&gt; tells R where to stop reading the data. Note that in this case the data stops at row 23 from the first row. You do not need to specify this value but in some cases R will read in NA values for extra rows below the actual data (try excluding &lt;code&gt;endRow =&lt;/code&gt; and reimport the data to test this)
&lt;code&gt;as.data.frame =&lt;/code&gt; tells R whether to convert the data into a data frame. Generally this is a good thing. The default will import the data as a list.
&lt;code&gt;header = TRUE&lt;/code&gt; tells R whether or not there are column headings in the start row.&lt;/p&gt;
&lt;p&gt;In general it is good practice in your work to create Excel workbooks with 1 sheet and headings in the first row. However, as we can see from the WIPO example, reality tends to be different. That means that it is important to inspect and clean the data before hand. Keep a copy of the original file for reference by creating a .zip file. Other things to consider are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Checking for corrupted characters and correcting them using find and replace in Excel or Open Office (see this &lt;a href=&#34;https://youtu.be/YYaMEbJW7Qw?list=PLsZOGmKUMi54n8R06U1HmxNywt0bAFays&#34;&gt;video&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Tidy up column names by removing characters such as ‘&#39; or brackets that could cause problems (for example R will generally import &lt;code&gt;inventor(s)&lt;/code&gt; as &lt;code&gt;inventor.s&lt;/code&gt;). Consider removing blank spaces in column titles or replacing with’_’ and regularising the case (e.g. all lower case ). This will make life easier later.&lt;/li&gt;
&lt;li&gt;Dealing with any leading or trailing spaces using TRIM() in Excel or Open Office.&lt;/li&gt;
&lt;li&gt;Filling blank cells with NA (see this quick &lt;a href=&#34;https://youtu.be/40isuia2w3w?list=PLsZOGmKUMi54n8R06U1HmxNywt0bAFays&#34;&gt;video&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Any formulas, such as column or row sum functions, may not be wanted and could cause confusion when you run your own calculations.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The above preparation steps will generally take a few minutes but can save a lot of work later on. Jeff Leek provides a very good guide to preparatory steps in &lt;a href=&#34;https://leanpub.com/datastyle&#34;&gt;The Elements of Data Analytic Style&lt;/a&gt; and we will be following these steps in our patent analysis work.&lt;/p&gt;
&lt;p&gt;Let’s take a look at the other available arguments by calling up the description.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?read.xlsx()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The range of arguments is below.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;read.xlsx(file, sheetIndex, sheetName=NULL, rowIndex=NULL,   startRow=NULL, endRow=NULL, colIndex=NULL,   as.data.frame=TRUE, header=TRUE, colClasses=NA,   keepFormulas=FALSE, encoding=&amp;quot;unknown&amp;quot;, ...)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Because Excel workbooks often contain more than one sheet, R needs to know where to find the right sheet. This is generally easy to do by number rather than name using &lt;code&gt;sheetName =&lt;/code&gt;. &lt;code&gt;Row index =&lt;/code&gt; will indicate the rows that you want to extract (if there are specific rows).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;startRow =&lt;/code&gt; will indicate whether to start reading into R from the first row or from a later row. Quite often there are spaces or explanatory text in the top row or rows. It pays to examine the dataset first and count the rows. As a matter of good practice use the first rows for column headings only and put other material elsewhere (a readme text file or a new worksheet called readme).&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;endRow =&lt;/code&gt; argument specifies where to stop reading the data.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;colIndex&lt;/code&gt; - indicates the columns that you want to extract. NULL is the default and will import all columns.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;as.data.frame = TRUE&lt;/code&gt; helpfully tells R to create a data frame. If not then a List will be created.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;header = TRUE or FALSE&lt;/code&gt; specifies whether the columns have names. In this case if we had not started at &lt;code&gt;startRow = 5&lt;/code&gt;, the header would have appeared as “Figure.A.1.1.1.Trend.in.patent.applications.worldwide” followed by more text. To try this for yourself change the startRow to 1 and reimport the data giving wipotrends a different name.&lt;/p&gt;
&lt;p&gt;Let’s take a look at &lt;code&gt;wipotrends&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wipotrends&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Year Applications Growth.rate....
## 1  1995      1047700              NA
## 2  1996      1088800             3.9
## 3  1997      1163400             6.9
## 4  1998      1214900             4.4
## 5  1999      1269000             4.5
## 6  2000      1377800             8.6
## 7  2001      1456500             5.7
## 8  2002      1443300            -0.9
## 9  2003      1485800             2.9
## 10 2004      1570100             5.7
## 11 2005      1703600             8.5
## 12 2006      1794300             5.3
## 13 2007      1866000             4.0
## 14 2008      1914800             2.6
## 15 2009      1846800            -3.6
## 16 2010      1987600             7.6
## 17 2011      2149000             8.1
## 18 2012      2347700             9.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In reviewing &lt;code&gt;wipotrends&lt;/code&gt; note that the row numbers refer to data rows (we have excluded the padding in rows 1 -4). If we were spending time with this data we might also want to turn the columns to lowercase and &lt;code&gt;growth rate&lt;/code&gt; to &lt;code&gt;growth_rate&lt;/code&gt; (but see below on &lt;code&gt;readxl&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;writing-excel-files&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Writing Excel Files&lt;/h2&gt;
&lt;p&gt;It is generally better to write a .csv file rather than an Excel file because the results can be used in a wider range of tools (including Excel) and will be cleaner (see below). However, to write an Excel file with the new data frame use the &lt;code&gt;write.xlsx()&lt;/code&gt; function. Before running the command it is generally a good idea to use the command &lt;code&gt;getwd()&lt;/code&gt; to display the working directory you are in so that you know where the file will be saved. To change the directory to a new location use &lt;code&gt;setwd(&amp;quot;yourpathtofile&amp;quot;)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write.xlsx(wipotrends, &amp;quot;yourfilenamepath_new.xlsx&amp;quot;, sheetName=&amp;quot;Sheet1&amp;quot;, col.names = TRUE, row.names = TRUE, append = FALSE, showNA = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will create a new file called wipotrends_new. Note three points here:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Give your file a &lt;strong&gt;new name&lt;/strong&gt; if writing into the same directory. Otherwise R will overwrite your existing file. Assuming you don’t want to overwrite the original give the new file a sensible name.&lt;/li&gt;
&lt;li&gt;If you select &lt;code&gt;row.names = FALSE&lt;/code&gt; R will write a new column with row numbers (in this case)&lt;/li&gt;
&lt;li&gt;Selecting &lt;code&gt;showNA = TRUE&lt;/code&gt; will fill any blank cells with NA. That is useful when coming back into R to tidy up and select data. Blank cells are the enemy of calculations and it is better to fill the cells with a value where possible.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;writing-excel-to-csv&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Writing Excel to CSV&lt;/h2&gt;
&lt;p&gt;While Excel is popular in reality it is better to use .csv when using or sharing data across a range of software tools. To write results into .csv use &lt;code&gt;write.csv()&lt;/code&gt;. Call up the description for write.csv with ?write.csv in console. See the .csv walk through for further details.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write.csv(wipotrends, file = &amp;quot;yourfilenamepath_new.csv&amp;quot;, row.names = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-readxl-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using the Readxl package&lt;/h2&gt;
&lt;p&gt;readxl is a new package from RStudio and is still a work in progress. We will cover it here because as the package develops it will become more popular and you are more likely to use it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;readxl&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readxl)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At the moment readxl version 0.1.0 has two functions.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;excel_sheets(path)&lt;/code&gt; where path is the path to the xls/xlsx file. This function will list all the sheets in an excel spreadsheet to help you select the sheet that you want to import.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, if we add a couple of random sheets to wipotrends and then use &lt;code&gt;excel_sheets(&amp;quot;myfilenamepath&amp;quot;)&lt;/code&gt; will provide names that look something like this:&lt;/p&gt;
&lt;p&gt;[1] “Sheet1” “my sheet” “another sheet”&lt;/p&gt;
&lt;p&gt;This is very helpful if you don’t know how many sheets are in a workbook or you want to call them by name.
2. &lt;code&gt;read_excel()&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readxl)
read_wipo &amp;lt;- read_excel(&amp;quot;/Users/pauloldham17inch/Desktop/WIPO_Training/training_datasets/wipo/wipotrends.xlsx&amp;quot;, col_names = TRUE, na = &amp;quot;&amp;quot;,  skip = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we read in this file and print it to the console we will notice something unusual.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read_wipo&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 17 x 3
##    `1995` `1047700`   X__1
##     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1  1996.  1088800.  3.90 
##  2  1997.  1163400.  6.90 
##  3  1998.  1214900.  4.40 
##  4  1999.  1269000.  4.50 
##  5  2000.  1377800.  8.60 
##  6  2001.  1456500.  5.70 
##  7  2002.  1443300. -0.900
##  8  2003.  1485800.  2.90 
##  9  2004.  1570100.  5.70 
## 10  2005.  1703600.  8.50 
## 11  2006.  1794300.  5.30 
## 12  2007.  1866000.  4.00 
## 13  2008.  1914800.  2.60 
## 14  2009.  1846800. -3.60 
## 15  2010.  1987600.  7.60 
## 16  2011.  2149000.  8.10 
## 17  2012.  2347700.  9.20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That is while we have specified that &lt;code&gt;col_names = TRUE&lt;/code&gt; and &lt;code&gt;skip = 5&lt;/code&gt; the function has not returned the column names in the dataset. While this is a bit puzzling ( the package was released less than a month ago), it suggests that it is still a work in progress. Unless this is a glitch with our data then one option would be to specify &lt;code&gt;col_names = FALSE&lt;/code&gt; and then rename the &lt;code&gt;X0   X1   X2&lt;/code&gt; column names that are generated. readr is under active development and you can follow its progress &lt;a href=&#34;%22https://github.com/hadley/readxl%22&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This is a useful reminder of one of the important principles of clean and tidy data. The first row should contain the column names.&lt;/p&gt;
&lt;p&gt;Bear in mind that readxl may struggle with reading dates correctly, but expect that to also change in the future.&lt;/p&gt;
&lt;p&gt;At the time of writing there is no &lt;code&gt;write_excel&lt;/code&gt; function but expect that to change.&lt;/p&gt;
&lt;p&gt;The main advantage of &lt;code&gt;read_excel&lt;/code&gt; (as with &lt;code&gt;read_csv&lt;/code&gt; in the &lt;code&gt;readr&lt;/code&gt; package) is that the data imports into an easy to print object with three attributes a &lt;code&gt;tbl_df&lt;/code&gt; a &lt;code&gt;tbl&lt;/code&gt; and a &lt;code&gt;data.frame&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you are using &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;tidyr&lt;/code&gt; (and we assume that you will be) then the creation of a tbl_df makes life much easier.&lt;/p&gt;
&lt;p&gt;In summary, readxl is a welcome development for those who prefer using Excel (or are forced too), but it is very recent. It’s main strength is the ability to easily see what worksheets are in a workbook and also the automatic creation of a data frame or data frame table at the time of import. The absence of a write function will hopefully encourage hardened Excel uses to adopt comma separated or tab delimited files as their standard and to take advantage of the fuller functionality of the &lt;code&gt;readr&lt;/code&gt; package. You know it makes sense.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reading-excel-files-from-url-locations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reading Excel files from URL locations&lt;/h2&gt;
&lt;p&gt;It is faster to simply download the file to your drive, or swim the Atlantic ocean, than to successfully download an excel file on http: or, in particular https:. So maybe ask yourself what is the path of least resistance and run with that.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-help-and-further-resources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting Help and Further Resources&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;For additional functionality experiment with the very useful &lt;strong&gt;XLConnect&lt;/strong&gt; package in Packages. Read the documentation on &lt;a href=&#34;http://cran.r-project.org/web/packages/XLConnect/index.html&#34;&gt;CRAN&lt;/a&gt;. This adds a lot of functionality in working with Excel files in R.&lt;/li&gt;
&lt;li&gt;See the R-bloggers &lt;a href=&#34;http://www.r-bloggers.com/read-excel-files-from-r/&#34;&gt;overview&lt;/a&gt; article on the range of packages for working with Excel files.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Paul Oldham&lt;/li&gt;
&lt;li&gt;Updated 13/05/2015&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Importing CSV files into R</title>
      <link>/importing-csv-files-into-r/</link>
      <pubDate>Wed, 29 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>/importing-csv-files-into-r/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#a-quick-2018-update&#34;&gt;A Quick 2018 Update&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reading-in-a-file-using-read.table-utils-package&#34;&gt;Reading in a file using read.table (&lt;code&gt;utils&lt;/code&gt; package)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reading-a-.csv-from-the-web&#34;&gt;Reading a .csv from the web&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#downloading-from-google-drive&#34;&gt;Downloading From Google Drive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#downloading-from-github&#34;&gt;Downloading from GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#writing-a-.csv-file&#34;&gt;Writing a .csv file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reading-in-multiple-.csv-files&#34;&gt;Reading in multiple .csv files&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#using-the-new-readr-package.&#34;&gt;Using the new &lt;code&gt;readr&lt;/code&gt; package.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#writing-a-.csv-file-using-write_csv&#34;&gt;Writing a .csv file using &lt;code&gt;write_csv()&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#round-up&#34;&gt;Round Up&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;a-quick-2018-update&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Quick 2018 Update&lt;/h2&gt;
&lt;p&gt;This post is now showing its age and was the first thing I wrote about R. Everything still works but readr, which was brand new at the time, has made a big difference. I now suggest the importing local csv files into RStudio using &lt;code&gt;File &amp;gt; Import &amp;gt; From Text (readr)&lt;/code&gt; as it is by far the easiest way to import files.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If that fails consult the &lt;code&gt;readr&lt;/code&gt; documentation and review the arguments.&lt;/li&gt;
&lt;li&gt;If that fails try the base R &lt;code&gt;read.csv()&lt;/code&gt; making sure to set &lt;code&gt;stringsAsFactors = FALSE&lt;/code&gt; unless you really want factors.&lt;/li&gt;
&lt;li&gt;When working with very large datasets use &lt;code&gt;fread()&lt;/code&gt; from the &lt;code&gt;data.table&lt;/code&gt; package… it rocks… but study the arguments.&lt;/li&gt;
&lt;li&gt;If looking at importing from online sources such as GitHub or Google Drive the approach below normally works fine.&lt;/li&gt;
&lt;li&gt;If you are working with Googlesheets the go to package is &lt;code&gt;googlesheets&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If looking to import multiple files at the same time the approach below will work well (but the steps could be joined together)… e.g. if you are doing the Coursera R course.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will update the post properly when time permits.&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;As part of the &lt;a href=&#34;https://github.com/wipo-analytics&#34;&gt;WIPO Manual on Open Source Patent Analytics project&lt;/a&gt; we will be working with patent data in R using &lt;a href=&#34;http://www.rstudio.com&#34;&gt;RStudio&lt;/a&gt;. If you do not have a copy of RStudio follow the simple instructions for installing on your platform &lt;a href=&#34;http://www.rstudio.com/products/rstudio/download/&#34;&gt;here&lt;/a&gt;. There are lots of resources on the site to help you get started including &lt;a href=&#34;http://www.rstudio.com/resources/training/online-learning/&#34;&gt;online learning&lt;/a&gt;, &lt;a href=&#34;http://www.rstudio.com/resources/webinars/&#34;&gt;videos&lt;/a&gt;, and &lt;a href=&#34;http://www.rstudio.com/resources/cheatsheets/&#34;&gt;cheatsheets&lt;/a&gt;. The excellent &lt;a href=&#34;http://www.r-bloggers.com&#34;&gt;R-Bloggers site&lt;/a&gt; will demonstrate why it is worth investing time in R when working with patent data.&lt;/p&gt;
&lt;p&gt;Comma separated value files (or .csv) files are one of the most common and useful ways for sharing data. This includes patent data.&lt;/p&gt;
&lt;p&gt;This walk through covers the basics of importing .csv files into R and writing .csv files. We will use the freely available &lt;a href=&#34;https://drive.google.com/open?id=0B4piiKOCkRPDRlBlcGpxR0tMTms&amp;amp;authuser=0&#34;&gt;ritonavir&lt;/a&gt; patent dataset as the example. You can grab the datasets by either forking or downloading the &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics&#34;&gt;GitHub repository&lt;/a&gt; or downloading the &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/datasets.zip?raw=true&#34;&gt;zip file&lt;/a&gt;. While we use patent data as the example, this will work for other types of .csv data.&lt;/p&gt;
&lt;p&gt;We will cover the following approaches to importing and writing .csv files here:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Importing .csv files from local directories using the standard read.table in the utils package.&lt;/li&gt;
&lt;li&gt;Writing .csv files using &lt;code&gt;write.csv()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Importing multiple .csv files using &lt;code&gt;lapply()&lt;/code&gt; and &lt;code&gt;ldply()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Importing and writing .csv files using the new &lt;a href=&#34;http://blog.rstudio.org/2015/04/09/readr-0-1-0/&#34;&gt;readr&lt;/a&gt; package.&lt;/li&gt;
&lt;li&gt;Downloading a .csv file from a URL, focusing on https: connections, using &lt;code&gt;Rcurl&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That should cover most needs. If you find that you are stuck with a function try calling the description for a particular function (?read.csv) or try &lt;a href=&#34;http://stackoverflow.com/questions/tagged/r&#34;&gt;stackoverflow.com&lt;/a&gt; results tagged R or from a Google search (often the quickest route).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;reading-in-a-file-using-read.table-utils-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reading in a file using read.table (&lt;code&gt;utils&lt;/code&gt; package)&lt;/h2&gt;
&lt;p&gt;Reading in a .csv file is easy and is part of &lt;code&gt;read.table&lt;/code&gt; in the R &lt;code&gt;utils&lt;/code&gt; package (installed by default). We can simply read in a .csv by creating an object linked to the function &lt;code&gt;read.csv()&lt;/code&gt; followed by the path to the local file as follows. You will need to download the file from the link above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ritonavir &amp;lt;- read.csv(&amp;quot;yourfilenamepath.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In some European countries the delimiter in a .csv is a semicolon “;” and not a comma. In the unlikely event you come across these files use &lt;code&gt;read.csv2()&lt;/code&gt; as above instead of &lt;code&gt;read.csv&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You now have a dataset called ritonavir in R. That is how easy it is. You can take a look at the data by simply typing ritonavir into the console. What you will see is a mass of data.
We can improve on that by using &lt;code&gt;head(ritonavir)&lt;/code&gt; but it is still a little difficult to view. We will come back to this in turning the data into a table data frame (&lt;code&gt;tbl_df()&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;First, let’s look at the function read.csv. R functions have settings called arguments that can be used to control what is going on.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;`?`(read.csv)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;read.csv(file, header = TRUE, sep = &amp;quot;,&amp;quot;, quote = &amp;quot;\&amp;quot;&amp;quot;, dec = &amp;quot;.&amp;quot;, fill = TRUE, comment.char = &amp;quot;&amp;quot;, ...)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The arguments for this function can be very useful, for example,&lt;/p&gt;
&lt;p&gt;&lt;code&gt;header = TRUE or FALSE&lt;/code&gt;. Determines whether or not to import column headings from the first row.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sep = &amp;quot;,&amp;quot;&lt;/code&gt; . The separator for the values in each row.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;...&lt;/code&gt; refers to additional arguments that might be applied. Among the most important of these using this function are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;stringsAsFactors = FALSE&lt;/code&gt;. To prevent character columns being converted to factors. This is actually a lot more important than it sounds, and will generally be your default.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;na.strings = &amp;quot;NA&amp;quot;&lt;/code&gt;. NA refers to not available. In some cases this needs to be expanded to cover blank cells in the source data. for example &lt;code&gt;c(&amp;quot;NA&amp;quot;, &amp;quot; &amp;quot;, &amp;quot;&amp;quot;)&lt;/code&gt; captures cells containing “NA”, cells with only a space &amp;quot; &amp;quot; or empty cells &amp;quot;&amp;quot;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;strip.white = TRUE&lt;/code&gt;. This will strip leading and trailing white space. Note that this will only work if you have specified sep = in the arguments.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;skip = n&lt;/code&gt;. Specify the number of lines to skip before the data starts. Very useful for data tables with blank rows or text padding at the top of files.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This means that we would often want a &lt;code&gt;read.csv()&lt;/code&gt; function with the following additional arguments for our file. In this case we are reading the data in directly from the datasets repository &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/tree/master/2_datasets&#34;&gt;here&lt;/a&gt;. Where the file is located on your computer you will need to enter the file path. If you are a Windows user your file path will contain back slashes. These need to be changed to forward slashes as back slashes are an escape character on every other system (yes it really is a pain).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ritonavir &amp;lt;- read.csv(&amp;quot;https://github.com/poldham/opensource-patent-analytics/raw/master/2_datasets/ritonavir/ritonavir.csv&amp;quot;, 
    sep = &amp;quot;,&amp;quot;, na.strings = &amp;quot;NA&amp;quot;, strip.white = TRUE, stringsAsFactors = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note here that the use of &lt;code&gt;sep = &amp;quot;,&amp;quot;&lt;/code&gt; is the condition for stripping leading and trailing white space on import using &lt;code&gt;strip.white = TRUE&lt;/code&gt;. &lt;code&gt;strip.white&lt;/code&gt; won’t work without the &lt;code&gt;sep&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you intend to split the inventor and applicant data following import you may want to wait because the process will generate white space. It is always possible to write a .csv file after the cleaning process and reimport it with &lt;code&gt;strip.white&lt;/code&gt; set to TRUE along with &lt;code&gt;sep= &amp;quot;,&amp;quot;&lt;/code&gt;. We will write a .csv file below.&lt;/p&gt;
&lt;p&gt;We have not specified &lt;code&gt;skip = n&lt;/code&gt; in the above as the column headers are in the first row in the original data. But, there are lots of occasions when skip can be useful.&lt;/p&gt;
&lt;p&gt;Lets look at the type or class of object that has been created from our latest import.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(ritonavir)  ##class is &amp;#39;data.frame&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we print the ritonavir R object we will get the first 500 rows of data. Try entering ritonavir in your console.&lt;/p&gt;
&lt;p&gt;That is not terribly helpful because we are overwhelmed with information and can’t see everything as a snap shot. The solution to this is to install and load &lt;code&gt;dplyr&lt;/code&gt; package. We will be using this package a lot in the patent analysis tutorials so, if you don’t have it already, now is a good time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;dplyr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;load the package&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now use the &lt;code&gt;tbl_df()&lt;/code&gt; function to create an easy to read dataframe table (&lt;code&gt;tbl_df()&lt;/code&gt;) using our ritonavir dataset that lists columns as characters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ritonavirtbl &amp;lt;- tbl_df(ritonavir)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates an easy to read table dataframe.&lt;/p&gt;
&lt;p&gt;If we print the frame we will now have readable content.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ritonavirtbl&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 804 x 20
##    row.no publication.numb… kind.code title   priority.docume… priority.country
##     &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;           
##  1      1 AU2007202956      B2        Polymo… AU19990050037    Australia       
##  2      2 CA2425495         A1        ANTI-R… CA2425495        Canada          
##  3      3 CN101440091       A         Ritona… CN20081072486    China           
##  4      4 CN101613325       A         Proces… CN20081031608    China           
##  5      5 CN1247554         C         Proces… CN20031121091    China           
##  6      6 DE10131036        A1        Medici… DE20011031036    Germany         
##  7      7 DE10145361        A1        Steril… DE20011045361    Germany         
##  8      8 DE102005012681    A1        New 1,… DE200510012681   Germany         
##  9      9 DE102007030695    A1        Co-cry… DE200710063623   Germany         
## 10     10 EP0490667         B1        HIV pr… JP19900409673    Japan           
## # ... with 794 more rows, and 14 more variables: earliest.priority.date &amp;lt;chr&amp;gt;,
## #   earliest.priority.year &amp;lt;int&amp;gt;, application.date &amp;lt;chr&amp;gt;,
## #   publication.date &amp;lt;chr&amp;gt;, publication.year &amp;lt;int&amp;gt;, applicant &amp;lt;chr&amp;gt;,
## #   inventors &amp;lt;chr&amp;gt;, ipc.classes &amp;lt;chr&amp;gt;, family.members &amp;lt;chr&amp;gt;,
## #   family.size &amp;lt;int&amp;gt;, family.size.1 &amp;lt;int&amp;gt;, wo.in.family &amp;lt;chr&amp;gt;,
## #   grant.or.application.type.families &amp;lt;chr&amp;gt;, granted.patent &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That is a lot easier to read than our original (try typing ritonavir into the console to take a look, then try ritonavirtbl).&lt;/p&gt;
&lt;p&gt;There are two points to note here.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Spaces in column names such as publication number are filled with full stops. Use the new &lt;code&gt;janitor&lt;/code&gt; package &lt;code&gt;clean_names()&lt;/code&gt; function to deal with this.&lt;/li&gt;
&lt;li&gt;More importantly, by default character vectors are converted to factors (characters backed by a hidden number). It therefore makes sense to always use &lt;code&gt;StringsAsFactors = FALSE&lt;/code&gt; unless you actually want columns to import as factors.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;reading-a-.csv-from-the-web&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reading a .csv from the web&lt;/h2&gt;
&lt;p&gt;Reading a .csv file from the web is also easy as we have seen above but it can involve some complications. We will cover a couple of cases here. If the URL begins with &lt;code&gt;http:&lt;/code&gt; then it is as simple as entering the URL inside quotes. However, if it is the secure &lt;code&gt;https:&lt;/code&gt; then it is a little bit more of a challenge.&lt;/p&gt;
&lt;div id=&#34;downloading-from-google-drive&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Downloading From Google Drive&lt;/h3&gt;
&lt;p&gt;For example if we try the following it will generally work with &lt;code&gt;http:&lt;/code&gt; but not with &lt;code&gt;https:&lt;/code&gt;. This will simply return an empty object with nothing in it and throw an error &lt;code&gt;Error in read.table(file = file, header = header, sep = sep, quote = quote, : more columns than column names&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ritonavir_url &amp;lt;- read.csv(&amp;quot;https://drive.google.com/drive/folders/0B4piiKOCkRPDTGdSQmRMa1BOUEE&amp;quot;, 
    sep = &amp;quot;,&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To deal with this we need to install and load the package RCurl.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;RCurl&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;load the library&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(RCurl)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s try again and add a couple of arguments to make it work.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ritonavir_url &amp;lt;- download.file(&amp;quot;https://drive.google.com/drive/folders/0B4piiKOCkRPDTGdSQmRMa1BOUEE&amp;quot;, 
    &amp;quot;ritonavir_url.csv&amp;quot;, method = &amp;quot;curl&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case we use &lt;code&gt;download.file&lt;/code&gt; and the URL in quotes, followed by the destination filename (which will download into the current working directory). For this to work without an error we need finally to specify &lt;code&gt;method = &amp;quot;curl&amp;quot;&lt;/code&gt; or an error reading &lt;code&gt;unsupported URL scheme&lt;/code&gt; will appear.&lt;/p&gt;
&lt;p&gt;You now need to navigate to where the file is and import it&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;downloading-from-github&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Downloading from GitHub&lt;/h3&gt;
&lt;p&gt;In downloading from GitHub (where the project Google Drive datasets are also located), we have to go a step further. The URL that you see on the page in Github is basically a marker for the data location… not the actual dataset location. To access the actual dataset navigate to the relevant page &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics/blob/master/datasets/ritonavir/ritonavir.csv&#34;&gt;here&lt;/a&gt;. However, then select the Raw button and copy that &lt;em&gt;URL&lt;/em&gt;. The URL should begin with &lt;a href=&#34;https:raw&#34; class=&#34;uri&#34;&gt;https:raw&lt;/a&gt;. as below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ritonavir_urlg &amp;lt;- download.file(&amp;quot;https://raw.githubusercontent.com/poldham/opensource-patent-analytics/master/datasets/ritonavir/ritonavir.csv&amp;quot;, 
    &amp;quot;ritonavir_urlg.csv&amp;quot;, method = &amp;quot;curl&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next you need to navigate to the file location and import as before.&lt;/p&gt;
&lt;p&gt;As an alternative to this approach in Github it can be easier to simply navigate to the repository (such as &lt;a href=&#34;https://github.com/poldham/opensource-patent-analytics&#34; class=&#34;uri&#34;&gt;https://github.com/poldham/opensource-patent-analytics&lt;/a&gt;) and then select &lt;code&gt;Clone in Desktop&lt;/code&gt; (if you are using GitHub on your local machine) or ‘Download ZIP’. That will download the repository including the relevant datasets.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;writing-a-.csv-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Writing a .csv file&lt;/h2&gt;
&lt;p&gt;If we wanted to write this data table to a new .csv file we would use the ‘write.csv()’ command.&lt;/p&gt;
&lt;p&gt;Before we do that a critical point to remember is to give the file a new filename or it will overwrite your original data. It is also worth checking your working directory so that you know where it will be saved.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;getwd()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If this is the wrong directory, locate and copy the path for the directory you want and then use &lt;code&gt;setwd()&lt;/code&gt; to set the directory to save the file in.&lt;/p&gt;
&lt;p&gt;To write the file with a new file name we will use &lt;code&gt;write.csv()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write.csv(ritonavirtbl, &amp;quot;ritonavirtbl.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will write a file called ritonavirtbl.csv to your working directory. If you take a look at the file note that an extra column will have been added at the beginning (we will come back to that) and column names will now contain full stops instead of spaces.&lt;/p&gt;
&lt;p&gt;Let’s take a look at the options for writing .csv files by calling help.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;`?`(write.csv)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;write.csv is a function in &lt;code&gt;write.table&lt;/code&gt;. Let’s take a look at the arguments.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;write.table(x, file = &amp;quot;&amp;quot;, append = FALSE, quote = TRUE, sep = &amp;quot; &amp;quot;,             eol = &amp;quot;\n&amp;quot;, na = &amp;quot;NA&amp;quot;, dec = &amp;quot;.&amp;quot;, row.names = TRUE,             col.names = TRUE, qmethod = c(&amp;quot;escape&amp;quot;, &amp;quot;double&amp;quot;),             fileEncoding = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;A couple of these settings may be useful.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;append = TRUE or FALSE&lt;/code&gt;. Do we want to append the data to an existing file of that name or not? If false and the same filename is used then it will overwrite the existing file. If TRUE it will append the data to the file of the same name.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;na = &amp;quot;NA&amp;quot;&lt;/code&gt; the string that you want to use for missing data. This may need further definition depending on your data (e.g. &lt;code&gt;na = c(&amp;quot;NA&amp;quot;, &amp;quot; &amp;quot;, &amp;quot;&amp;quot;)&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;row.names&lt;/code&gt; and &lt;code&gt;col.names&lt;/code&gt; may be useful depending on your dataset or needs. Note that the default is TRUE. This is generally correct for columns with patent data but not for rows.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;code&gt;row.names = FALSE&lt;/code&gt; argument is more important than it appears. As Garrett Grolemund points out in &lt;a href=&#34;http://shop.oreilly.com/product/0636920028574.do&#34;&gt;Hands-On Programming with R&lt;/a&gt; every time that you write a .csv file in R it will add a row names column and keep on writing a new column each time you write the file. No one seems to quite understand why that happens but you will not be wanting that to happen. So the suggested default for &lt;code&gt;write.csv()&lt;/code&gt; is&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write.csv(ritonavirtbl, &amp;quot;ritonavirtbl.csv&amp;quot;, row.names = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will prevent R from writing an additional row names column.&lt;/p&gt;
&lt;p&gt;We will now take a look at a somewhat unusual import case. That is importing multiple files. I have left this until after we have worked on writing .csv files because realising that we can import, export and then reimport files into R is an important part of creating effective workflows in future work. Otherwise, we may spend hours in R to achieve something that can be done easily outside R. This particular example provides a good illustration of this point.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reading-in-multiple-.csv-files&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reading in multiple .csv files&lt;/h2&gt;
&lt;p&gt;On some occasions we might want to read in multiple .csv files at the same time. Typically this will be where a patent dataset has been split into multiple files. If you would like to follow this discussion then download the &lt;a href=&#34;https://drive.google.com/folderview?id=0B4piiKOCkRPDVUxhVVhkbGtQLVU&amp;amp;usp=sharing&#34;&gt;pizza_sliced dataset&lt;/a&gt; which contains five .csv files plus a ReadMe file.&lt;/p&gt;
&lt;p&gt;Reading in multiple files is a task that is a little trickier than it should be. However, the approach below will work (assuming you have the files in a local folder). My suggestion would be to remove the Read Me file from the downloaded set for this exercise.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pizzasliced &amp;lt;- list.files(&amp;quot;/Users/pauloldham17inch/opensource-patent-analytics/2_datasets/pizza_sliced&amp;quot;, 
    full.names = TRUE)  ##create a vector of file names to read in
pizzasliced&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;/Users/pauloldham17inch/opensource-patent-analytics/2_datasets/pizza_sliced/pizza_sliced_20002002_319.csv&amp;quot;
## [2] &amp;quot;/Users/pauloldham17inch/opensource-patent-analytics/2_datasets/pizza_sliced/pizza_sliced_20032005_366.csv&amp;quot;
## [3] &amp;quot;/Users/pauloldham17inch/opensource-patent-analytics/2_datasets/pizza_sliced/pizza_sliced_20062008_439.csv&amp;quot;
## [4] &amp;quot;/Users/pauloldham17inch/opensource-patent-analytics/2_datasets/pizza_sliced/pizza_sliced_20092012_428.csv&amp;quot;
## [5] &amp;quot;/Users/pauloldham17inch/opensource-patent-analytics/2_datasets/pizza_sliced/pizza_sliced_20132014_274.csv&amp;quot;
## [6] &amp;quot;/Users/pauloldham17inch/opensource-patent-analytics/2_datasets/pizza_sliced/pizza_sliced_codebook.txt&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we print pizza sliced we will now see a list of the full name of files including the full path.&lt;/p&gt;
&lt;p&gt;If we check the class of this object using &lt;code&gt;class(pizzasliced)&lt;/code&gt; it will be a character type&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(pizzasliced)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What we now need to do is to transform this into a list. To do that we will use the function &lt;code&gt;lapply()&lt;/code&gt; for list apply. In this case we are saying “apply list apply to the R character object pizzasliced and then apply the function &lt;code&gt;read.csv&lt;/code&gt; ensuring that headings are included from column names” as below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pizzasliced1 &amp;lt;- lapply(pizzasliced, read.csv, header = TRUE, stringsAsFactors = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;lapply will then iterate through and read in the five files to create a new object that we call pizzasliced1. Note that if doing this normally you would probably retain the same name and allow R to overwrite the object as you go along. Here we are showing the steps. If we now investigate the class of R object the answer will be a list.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(pizzasliced1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;list&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A list is an object that groups together R objects (in this case the data from our files). To demonstrate this if we now try and print pizzasliced1 then we will see an overwhelming amount of information rush by as R prints the five objects in the list.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(pizzasliced1)  # try this in your console&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What we now need to do is to convert the list into a data.frame. To do that however we need to turn to the &lt;code&gt;dplyr&lt;/code&gt; package and the &lt;code&gt;bind_rows()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;dplyr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Load the library&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now simply apply &lt;code&gt;bind_rows()&lt;/code&gt; to pizzasliced1 to turn the list of objects into a data.frame. We will call that pizzasliced2.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pizzasliced2 &amp;lt;- bind_rows(pizzasliced1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we print pizzasliced2 into the console we will see another set of data rushing by. However, if we check the class we will see that we now have a data.frame object. A data.frame is actually a list of the class data.frame. To test that try typing &lt;code&gt;typeof(pizzasliced2)&lt;/code&gt; into the console and the result will be “list”).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(pizzasliced2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can take a look at the top of the dataset using &lt;code&gt;head()&lt;/code&gt;. This will show rather crunched data and column headings&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(pizzasliced2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                                                     Title
## 1                                                              PIZZA TRAY
## 2                                 COOKING METHOD OF GREEN-TEA ADDED PIZZA
## 3                                              METHOD FOR COOKING A PIZZA
## 4                          Pizza preparation and delivery method and unit
## 5                                  Method of making laminated pizza crust
## 6 Container for transporting heated food, particularly pizza and the like
##                  Publication.number Publication.date
## 1                       CA93181 (S)       24/08/2001
## 2                 KR20010107868 (A)       07/12/2001
## 3     CA2731260 (A1); CA2731260 (C)       14/09/2000
## 4 US2002048624 (A1); US6858243 (B2)       25/04/2002
## 5                     US6126977 (A)       03/10/2000
## 6 US2002040862 (A1); US6601758 (B2)       11/04/2002
##                                                                                                                           Inventor.s.
## 1                                                                                                                                    
## 2                                                                                                         YOU YEN SIL\x89\xdb\xe2[KR]
## 3                                                       HEDRINGTON JAMES ALAN\x89\xdb\xe2[US];  DRESSEL BRENT WILLIAM\x89\xdb\xe2[US]
## 4 BLANCHET JEAN\x89\xdb\xe2[FR];  CATHELIN HERVE\x89\xdb\xe2[FR];  HEBERT CHRISTIAN\x89\xdb\xe2[FR];  NOUYRIT OLIVIER\x89\xdb\xe2[FR]
## 5                                                                                                      BUBAR RONALD O\x89\xdb\xe2[US]
## 6                                                                                                      LIZZIO FILIPPO\x89\xdb\xe2[IT]
##                            Applicant.s.
## 1 SCHWAN S FOOD MFG INC\x89\xdb\xe2[US]
## 2           YOU YEN SIL\x89\xdb\xe2[KR]
## 3        NAT PRESTO IND\x89\xdb\xe2[US]
## 4             NESTEC SA\x89\xdb\xe2[US]
## 5       PAULUCCI JENO F\x89\xdb\xe2[US]
## 6        TERMOPIZZA SRL\x89\xdb\xe2[US]
##                                                                   International.classification
## 1                                                                                             
## 2                                                                                    A21D13/00
## 3                                                     A23L1/01; A21B1/00; A47J37/04; A47J37/06
## 4 A23L1/48; A21B1/00; A21D8/00; A21D13/00; A21D15/02; B60P3/025; B60P3/14; A23L1/00; A21D13/00
## 5                     A21C3/02; A21C11/00; A21D8/00; A21D8/02; A21D13/00; A21D13/08; A21D13/00
## 6                                         B65D77/04; B65D81/26; B65D85/36; B65D85/36; B65D5/50
##                                                        Cooperative.Patent.Classification
## 1                                                                                       
## 2                                                                                       
## 3                                                                A47J37/0611; A47J37/043
## 4                        A21D13/007; A21B1/00; A21D8/00; A21D15/02; B60P3/0257; B60P3/14
## 5                                A21D13/0061; A21C3/02; A21C11/004; A21D8/02; A21D13/007
## 6 B65D77/0433; B65D81/262; B65D81/263; B65D85/36; B65D2585/366; Y10S229/906; Y10S229/902
##   Application.number Date.of.application
## 1     CANDAT0093181F                   0
## 2      KR20010069326            20011105
## 3      CA20002731260            20000310
## 4      US20010982377            20011018
## 5      US19970968900            19971106
## 6      US20010963393            20010927
##                                Priority.number.s.
## 1                         CANDAT0093181F 00000000
## 2                          KR20010069326 20011105
## 3  US19990267981 19990312; CA20002363329 20000310
## 4                          EP20000122736 20001018
## 5  US19970968900 19971106; US19950496894 19950630
## 6 IT2000TO00900 20000928; IT2001TO00008U 20010119
##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Patents.cited.in.the.search.report
## 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  
## 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  
## 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  
## 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        US3879564 (A); US4164591 (A); US4919477 (A); US5256432 (A)
## 5 US628449 (A); US969173 (A); US1174826 (A); US1179294 (A); US1646921 (A); US2089396 (A); US2509035 (A); US2668767 (A); US3143424 (A); US3235390 (A); US3677769 (A); US3845219 (A); US3880069 (A); US4020184 (A); US4205091 (A); US4283424 (A); US4283431 (A); US4308286 (A); US4313961 (A); US4416910 (A); US4463020 (A); US4551337 (A); US4574090 (A); US4626188 (A); US4645673 (A); US4661361 (A); US4696823 (A); US4753813 (A); US4842882 (A); US4907501 (A); US5104669 (A); US5180603 (A); US5182123 (A); US5194273 (A); US5196223 (A); US5268188 (A); US5348751 (A); US5405626 (A); US5417150 (A); US5417996 (A); US5529799 (A); US5560946 (A); DE3704192 (A1); GB2241863 (A)
## 6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               US3026209 (A); US3515331 (A); US4197940 (A); US4441626 (A); US4883195 (A); US5052559 (A); US5385292 (A); US5445286 (A); US5482724 (A); US5588587 (A); US5895698 (A); EP0989067 (A1)
##   Literature.cited.in.the.search.report Patents.cited.during.examination
## 1                                                                       
## 2                                                                       
## 3                                                                       
## 4                                                                       
## 5                                                                       
## 6                                                                       
##   Literature.cited.during.examination Other.patent.citations
## 1                                  NA                     NA
## 2                                  NA                     NA
## 3                                  NA                     NA
## 4                                  NA                     NA
## 5                                  NA                     NA
## 6                                  NA                     NA
##   Other.literature.citations Patents.used.in.opposition
## 1                         NA                       &amp;lt;NA&amp;gt;
## 2                         NA                       &amp;lt;NA&amp;gt;
## 3                         NA                       &amp;lt;NA&amp;gt;
## 4                         NA                       &amp;lt;NA&amp;gt;
## 5                         NA                       &amp;lt;NA&amp;gt;
## 6                         NA                       &amp;lt;NA&amp;gt;
##   Literature.used.in.opposition                Patents.cited.by.the.applicant
## 1                            NA                                              
## 2                            NA                                              
## 3                            NA                                              
## 4                            NA US4361227 (A); US4791861 (A); JPH09299017 (A)
## 5                            NA                                              
## 6                            NA                                              
##   Literature.cited.by.the.applicant International.search.citation
## 1                                NA                              
## 2                                NA                              
## 3                                NA                              
## 4                                NA                              
## 5                                NA                              
## 6                                NA                              
##   International.search.NPL.citation Supplementary.international.search.citation
## 1                                                                            NA
## 2                                                                            NA
## 3                                                                            NA
## 4                                                                            NA
## 5                                                                            NA
## 6                                                                            NA
##   Supplementary.international.search.NPL.citation espacenet.search.20.03.2015
## 1                                              NA                          NA
## 2                                              NA                          NA
## 3                                              NA                          NA
## 4                                              NA                          NA
## 5                                              NA                          NA
## 6                                              NA                          NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could also take a look at the bottom of the dataset&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tail(pizzasliced2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we use &lt;code&gt;summary()&lt;/code&gt; we will gain a slightly cleaner view that tries to sum up all of the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(pizzasliced2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Title           Publication.number Publication.date   Inventor.s.       
##  Length:1716        Length:1716        Length:1716        Length:1716       
##  Class :character   Class :character   Class :character   Class :character  
##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  
##                                                                             
##                                                                             
##                                                                             
##                                                                             
##  Applicant.s.       International.classification
##  Length:1716        Length:1716                 
##  Class :character   Class :character            
##  Mode  :character   Mode  :character            
##                                                 
##                                                 
##                                                 
##                                                 
##  Cooperative.Patent.Classification Application.number Date.of.application
##  Length:1716                       Length:1716        Min.   :       0   
##  Class :character                  Class :character   1st Qu.:20020816   
##  Mode  :character                  Mode  :character   Median :20060413   
##                                                       Mean   :20046941   
##                                                       3rd Qu.:20100407   
##                                                       Max.   :20140724   
##                                                       NA&amp;#39;s   :9          
##  Priority.number.s. Patents.cited.in.the.search.report
##  Length:1716        Length:1716                       
##  Class :character   Class :character                  
##  Mode  :character   Mode  :character                  
##                                                       
##                                                       
##                                                       
##                                                       
##  Literature.cited.in.the.search.report Patents.cited.during.examination
##  Length:1716                           Length:1716                     
##  Class :character                      Class :character                
##  Mode  :character                      Mode  :character                
##                                                                        
##                                                                        
##                                                                        
##                                                                        
##  Literature.cited.during.examination Other.patent.citations
##  Mode:logical                        Mode:logical          
##  NA&amp;#39;s:1716                           NA&amp;#39;s:1716             
##                                                            
##                                                            
##                                                            
##                                                            
##                                                            
##  Other.literature.citations Patents.used.in.opposition
##  Mode:logical               Length:1716               
##  NA&amp;#39;s:1716                  Class :character          
##                             Mode  :character          
##                                                       
##                                                       
##                                                       
##                                                       
##  Literature.used.in.opposition Patents.cited.by.the.applicant
##  Mode:logical                  Length:1716                   
##  NA&amp;#39;s:1716                     Class :character              
##                                Mode  :character              
##                                                              
##                                                              
##                                                              
##                                                              
##  Literature.cited.by.the.applicant International.search.citation
##  Mode:logical                      Length:1716                  
##  NA&amp;#39;s:1716                         Class :character             
##                                    Mode  :character             
##                                                                 
##                                                                 
##                                                                 
##                                                                 
##  International.search.NPL.citation Supplementary.international.search.citation
##  Length:1716                       Mode:logical                               
##  Class :character                  NA&amp;#39;s:1716                                  
##  Mode  :character                                                             
##                                                                               
##                                                                               
##                                                                               
##                                                                               
##  Supplementary.international.search.NPL.citation espacenet.search.20.03.2015
##  Mode:logical                                    Min.   :238                
##  NA&amp;#39;s:1716                                       1st Qu.:238                
##                                                  Median :238                
##                                                  Mean   :238                
##                                                  3rd Qu.:238                
##                                                  Max.   :238                
##                                                  NA&amp;#39;s   :1715&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is needed is an easier to read form and that is where we can use &lt;code&gt;tbl_df&lt;/code&gt; again from the &lt;code&gt;dplyr&lt;/code&gt; package that we used earlier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pizzasliced3 &amp;lt;- tbl_df(pizzasliced2)&lt;/code&gt;&lt;/pre&gt;
&lt;!---Houston, we have a problem. That problem is that somewhere in the esp@cenet data that makes up the pizza_sliced dataset there is an issue with the character encoding that is preventing the `dplyr` function from working. Typically this will mean that the character encoding has become messed up in the underlying data. This of course is one of the reasons it is always worth checking the data first (as is suggested in this video [here](https://youtu.be/YYaMEbJW7Qw?list=PLsZOGmKUMi54n8R06U1HmxNywt0bAFays)). Clearly that is OK for small datasets but for large or multi-part datasets is a problem. So, another solution is needed. The problem as it turns out is in the inventor and the applicant fields in the space between the name and the country code entry which looks like this in Excel `BIANCHI MARCO‰Ûâ[IT]` and merely throws a `???` in Open Office Calc. 

Working out how to solve this problem in R could take a while (although string replacement is a good bet). The issue therefore is how to get to a solution as quickly as possible. That solution is to take what we learned above to write the new data.frame to a .csv file, then open the file in Open Office or Excel and use find and replace on the corrupted strings (as is suggested in this video [here](https://youtu.be/YYaMEbJW7Qw?list=PLsZOGmKUMi54n8R06U1HmxNywt0bAFays)). 

First check your working directory `getwd()` so you know where the file will go. Then `setwd(pathtoyourdirectory)` if that needs to change.---&gt;
&lt;!---Open the file in either Open Office or Excel. Copy the corrupted characters in the Inventors or Applicants columns, select those columns and then find and replace with a space as the replacement (see the video link above). Then save the file writing over the original download. Now let&#39;s reimport it. ---&gt;
&lt;!---It can be worth printing the top of the file to check that the clean up has worked.---&gt;
&lt;!---The inventors and applicant fields are now free of the encoding problem. We are now in a position to try again with the table data frame using `tbl_df()`. ---&gt;
&lt;p&gt;Let’s print pizzasliced4.&lt;/p&gt;
&lt;!---We can now see the first ten rows of a data frame that contains 1,707 rows and 24 columns. It will still be a little crunched but is much more readable. Also note that most of the columns are character fields (not factors).

Using `dplyr` we can call up a View of the dataset in a new pop up window. This can be very useful for inspecting datasets.---&gt;
&lt;p&gt;Let’s finish off here by writing pizzasliced2 out to a .csv.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write.csv(pizzasliced2, &amp;quot;pizzasliced2&amp;quot;, row.names = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While we would probably choose more informative filenames it is good practice to output work before moving on to other tasks or at some point it will be lost.&lt;/p&gt;
&lt;p&gt;We have now covered the basics of importing a .csv file into R and then writing a file. We have also covered importing multiple .csv files and a quick work around character encoding problems before exporting a data frame.&lt;/p&gt;
&lt;p&gt;It is useful to know these steps when you get into trouble, but in day to day practice you are most likely to use the &lt;code&gt;readr&lt;/code&gt; package (or &lt;code&gt;data.table&#39;s&lt;/code&gt; &lt;code&gt;fread()&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-new-readr-package.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using the new &lt;code&gt;readr&lt;/code&gt; package.&lt;/h2&gt;
&lt;p&gt;If you don’t have &lt;code&gt;readr&lt;/code&gt; use the following to install it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;readr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you do, or to check it has loaded, use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you have not done so already, let’s install and load &lt;code&gt;dplyr&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;dplyr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s try loading our dataset again using the function &lt;code&gt;read_csv()&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
ritonavir3 &amp;lt;- read_csv(&amp;quot;/Users/pauloldham17inch/opensource-patent-analytics/2_datasets/ritonavir/ritonavir.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will create a data frame and then display problems in red. The problems can be investigated by typing &lt;code&gt;problems()&lt;/code&gt; in the console. We will ignore these in this case. As with &lt;code&gt;read.csv2()&lt;/code&gt;, the &lt;code&gt;readr&lt;/code&gt; function &lt;code&gt;read_csv2()&lt;/code&gt; will read files with the &lt;code&gt;&amp;quot;;&amp;quot;&lt;/code&gt; as the separator.&lt;/p&gt;
&lt;p&gt;To see the read_csv arguments let’s call help&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;`?`(read_csv)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;read_csv(file, col_names = TRUE, col_types = NULL, na = &amp;quot;NA&amp;quot;, skip = 0,   n_max = -1, progress = interactive())&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This tells us that the function will assume that there are column names in the first row. &lt;code&gt;col_types = NULL&lt;/code&gt; tells us that the function will attempt to calculate the column type from the first thirty rows of data. You can however specify the column types as character, double, integer, logical etc.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;skip&lt;/code&gt; will specify the number of rows to skip as before. &lt;code&gt;n_max&lt;/code&gt; will specify the maximum number of records to read. That can be helpful if the dataset is large and you just want to take a look at some of it to get a sense of the data.&lt;/p&gt;
&lt;p&gt;The main advantages of &lt;code&gt;read_csv&lt;/code&gt; over &lt;code&gt;read.csv&lt;/code&gt; are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;read_csv&lt;/code&gt; does not automatically read in character vectors as factors. This means there is &lt;strong&gt;no need&lt;/strong&gt; to specify &lt;code&gt;stringsAsFactors = FALSE&lt;/code&gt; as part of the function’s arguments. This will be a very great relief to many people as it is one less thing to remember!&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;problems()&lt;/code&gt; prompt advises you that problems may exist with reading the file. You might be able to fix or ignore them.&lt;/li&gt;
&lt;li&gt;For larger files a progress indicator will display on loading (in interactive mode) where the load is over 5 seconds.&lt;/li&gt;
&lt;li&gt;Column names are left as is. That means that publication number stays as publication number rather than becoming publication.number and requiring renaming.&lt;/li&gt;
&lt;li&gt;By default, &lt;code&gt;readr&lt;/code&gt; turns imported data into a &lt;code&gt;data.frame&lt;/code&gt;, and a &lt;code&gt;table (tbl)&lt;/code&gt; and a &lt;code&gt;table dataframe (tbl_df)&lt;/code&gt;. You can test this by typing &lt;code&gt;class(ritonavir3)&lt;/code&gt; into the console. That means if you are running &lt;code&gt;dplyr&lt;/code&gt; then it will automatically show the first ten rows and the column name. That may not sound exciting but it is a lot better than masses of data rushing past.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, as &lt;code&gt;readr&lt;/code&gt; is a new package that is being actively developed there are also some issues. You might want to check out the latest development version is available &lt;a href=&#34;https://github.com/hadley/readr/blob/master/README.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s take a look now at ritonavir3 but using the &lt;code&gt;View()&lt;/code&gt; function to call up a dataset window.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;View(ritonavir3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we scroll across then we can see that the date columns in the dataset have been transformed to &lt;code&gt;NA&lt;/code&gt;. In some circumstances this is not a problem (remember that we still have the original dataset, what we see here is a data table). In other cases this could be a problem (if we wanted to use this data).&lt;/p&gt;
&lt;p&gt;At the time of writing, there does not seem to be a clear way to deal with this issue (but see the development page read.me on precisely this issue). This reflects the difficulty of dealing with dates because they can be ambiguous. We will discuss this elsewhere.&lt;/p&gt;
&lt;p&gt;For the moment, let’s call ritonavir3 into the console.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ritonavir3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What we see here is the data with column names left as is. We can also see that most of the columns (vectors) are character vectors and have not been transformed into factors meaning &lt;strong&gt;no more&lt;/strong&gt; &lt;code&gt;stringsAsFactors = FALSE&lt;/code&gt;. The date fields have been recognised as dates, but as we have seen have been transformed to NA (not available) because of the lack of clarity on the kind of date.&lt;/p&gt;
&lt;p&gt;We will update this part of the walkthrough as clarity on dealing with dates becomes available with &lt;code&gt;readr&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;writing-a-.csv-file-using-write_csv&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Writing a .csv file using &lt;code&gt;write_csv()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;We can easily write a .csv file using the &lt;code&gt;write_csv()&lt;/code&gt; function as follows&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(ritonavir3, &amp;quot;ritonavir3.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output from this has the advantage of preserving the column names as is so that “publication number” stays as is and does not become “publication.number”.&lt;/p&gt;
&lt;p&gt;The full list of arguments for &lt;code&gt;write_csv()&lt;/code&gt; at the moment is&lt;/p&gt;
&lt;p&gt;&lt;code&gt;write_csv(x, path, append = FALSE, col_names = !append)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;append = TRUE&lt;/code&gt; will append the table to the existing file. &lt;code&gt;col_names = TRUE&lt;/code&gt; will write column names at the top of the file. Expect more arguments to be added as &lt;code&gt;readr&lt;/code&gt; develops.&lt;/p&gt;
&lt;p&gt;Bear in mind that &lt;code&gt;readr&lt;/code&gt; does not possess the functionality of &lt;code&gt;read.csv&lt;/code&gt; or &lt;code&gt;write.csv&lt;/code&gt; in &lt;code&gt;read.table&lt;/code&gt;. Part of the aim of &lt;code&gt;readr&lt;/code&gt; is simplification based on the idea of doing a limited number of things well. Therefore, it is unlikely that &lt;code&gt;readr&lt;/code&gt; will ever be as comprehensive as the &lt;code&gt;read.table&lt;/code&gt; equivalents in the future. However, &lt;code&gt;readr&lt;/code&gt; is likely to become the go to package because of its simplicity for most needs and because it links with the wider family of &lt;code&gt;tidyr&lt;/code&gt;, &lt;code&gt;plyr&lt;/code&gt; and &lt;code&gt;dplyr&lt;/code&gt; packages under development at RStudio to make data wrangling and analysis easier.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round Up&lt;/h2&gt;
&lt;p&gt;In this walkthrough we have covered the fundamentals of reading and writing .csv files in R. This is pretty much the easiest file format to work with for patent data and considerably better than Excel which we will cover next. I, like almost everyone else, would encourage you to start working with .csv files wherever possible for the straightforward reason that they are cleaner and better to share across platforms and programmes than Excel or other proprietary format files.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
